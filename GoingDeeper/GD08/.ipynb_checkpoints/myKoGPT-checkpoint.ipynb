{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f41963e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version:1.12.1\n",
      "Cuda version: 11.3\n",
      "transformers version: 4.28.0\n",
      "GPU ì‚¬ìš© ê°€ëŠ¥ì—¬ë¶€: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "print(\"Torch version:{}\".format(torch.__version__)) # Torch version:1.12.1\n",
    "print(\"Cuda version: {}\".format(torch.version.cuda)) # Cuda version: 11.3\n",
    "print(\"transformers version: {}\".format(transformers.__version__)) # transformers 4.28.0\n",
    "print(\"GPU ì‚¬ìš© ê°€ëŠ¥ì—¬ë¶€: {}\".format(torch.cuda.is_available()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f647a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: evaluate in /opt/conda/lib/python3.9/site-packages (0.4.3)\n",
      "Requirement already satisfied: rouge_score in /opt/conda/lib/python3.9/site-packages (0.1.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from evaluate) (1.21.4)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.9/site-packages (from evaluate) (21.3)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.9/site-packages (from evaluate) (0.70.12.2)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.9/site-packages (from evaluate) (2.0.2)\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.9/site-packages (from evaluate) (0.3.4)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from evaluate) (2.13.1)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (from evaluate) (1.3.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.9/site-packages (from evaluate) (2.26.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.9/site-packages (from evaluate) (2021.11.1)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.9/site-packages (from evaluate) (4.62.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.9/site-packages (from evaluate) (0.15.1)\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.9/site-packages (from rouge_score) (3.6.5)\n",
      "Requirement already satisfied: absl-py in /opt/conda/lib/python3.9/site-packages (from rouge_score) (0.12.0)\n",
      "Requirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.9/site-packages (from rouge_score) (1.16.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (6.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (3.8.4)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (12.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.7.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from huggingface-hub>=0.7.0->evaluate) (3.12.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging->evaluate) (3.0.6)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (2.0.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (2023.5.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (2.10)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.9/site-packages (from nltk->rouge_score) (8.0.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.9/site-packages (from nltk->rouge_score) (2021.11.10)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.9/site-packages (from nltk->rouge_score) (1.1.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.9/site-packages (from pandas->evaluate) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.9/site-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (21.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.7.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.2.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (5.2.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install evaluate rouge_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4064eb9b",
   "metadata": {},
   "source": [
    "### ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸° - skt/kogpt2-base-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7e56409",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pandas as pd\n",
    "import numpy\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name = \"skt/kogpt2-base-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05af1d75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gpt2': 1024,\n",
       " 'gpt2-medium': 1024,\n",
       " 'gpt2-large': 1024,\n",
       " 'gpt2-xl': 1024,\n",
       " 'distilgpt2': 1024}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.max_model_input_sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e72911",
   "metadata": {},
   "source": [
    "### SFT ë°ì´í„° - ê¸°ë³¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7811dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'prompt': 'ë¶ˆê³ ê¸°ìš© ê³ ê¸° í•œìš°ì—ìš”?',\n",
       "  'completion': \"'ì €ëŠ” ì¸ê³µì§€ëŠ¥ ì±—ë´‡ì´ë©°, ì§ì ‘ì ìœ¼ë¡œ ì‹í’ˆì— ê´€í•œ ì •ë³´ë¥¼ ê°€ì§€ê³  ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì¼ë°˜ì ìœ¼ë¡œ ë¶ˆê³ ê¸°ìš© ê³ ê¸°ëŠ” í•œìš°, ì‡ ê³ ê¸°, ë¼ì§€ê³ ê¸° ë“± ë‹¤ì–‘í•œ ì¢…ë¥˜ì˜ ê³ ê¸°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. í•˜ì§€ë§Œ í•œìš°ëŠ” ëŒ€í‘œì ì¸ ê³ ê¸‰ ìœ¡ë¥˜ë¡œ ì•Œë ¤ì ¸ ìˆê¸° ë•Œë¬¸ì—, í•œìš°ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš°ë„ ë§ìŠµë‹ˆë‹¤. ì•ŒëŸ¬ì§€ë‚˜ ê°œë³„ ê±´ê°• ìƒíƒœì— ë”°ë¼ ë‹¤ë¥¼ ìˆ˜ ìˆìœ¼ë‹ˆ ì¶©ë¶„í•œ ì •ë³´ ìˆ˜ì§‘ í›„ì— ì„ íƒí•´ ì£¼ì‹œê¸° ë°”ëë‹ˆë‹¤.\",\n",
       "  'tokens': 193},\n",
       " {'prompt': 'ì“°ë˜ ì•±ì´ ìœ ë£Œë¡œ ì „í™˜ëì–´',\n",
       "  'completion': \"'ì–´ë–¤ ì•±ì¸ì§€ ëª¨ë¥´ê¸° ë•Œë¬¸ì— ì •í™•í•œ ë‹µë³€ì„ ë“œë¦¬ê¸° ì–´ë µìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ, ì¼ë°˜ì ìœ¼ë¡œ ìœ ë£Œ ì „í™˜ëœ ì•±ì€ ê¸°ì¡´ ë¬´ë£Œ ì•±ì—ì„œ ì¶”ê°€ì ì¸ ê¸°ëŠ¥ì´ë‚˜ ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•˜ëŠ” ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤. ì´ ê²½ìš°, ìœ ë£Œ ì „í™˜ ì´ì „ ê°€ê²©ì´ ë§¤ìš° ì €ë ´í•˜ê±°ë‚˜ ë¬´ë£Œì¸ ê²½ìš°ê°€ ë§ìœ¼ë¯€ë¡œ, ì•±ì„ ê³„ì† ì‚¬ìš©í•˜ë ¤ë©´ ì¶”ê°€ì ì¸ ë¹„ìš©ì„ ì§€ë¶ˆí•´ì•¼ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ, ìœ ë£Œ ì „í™˜ ì´í›„ì—ëŠ” ê¸°ì¡´ ì‚¬ìš©ì ë˜ëŠ” ìƒˆë¡œìš´ ì‚¬ìš©ì ëª¨ë‘ê°€ ì¶”ê°€ ê¸°ëŠ¥ê³¼ ë†’ì€ í’ˆì§ˆì˜ ì„œë¹„ìŠ¤ë¥¼ ì´ìš©í•  ìˆ˜ ìˆê²Œ ë©ë‹ˆë‹¤. ë”°ë¼ì„œ, ì•± ê°œë°œìëŠ” ìœ ë£Œ ëª¨ë¸ë¡œ ì „í™˜í•¨ìœ¼ë¡œì¨ ì•±ì˜ ìˆ˜ìµì„ ì¦ê°€ì‹œí‚¤ê³  ë” ë‚˜ì€ ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\",\n",
       "  'tokens': 288},\n",
       " {'prompt': 'ì—¬ì¹œì´ë‘ ë‹¤íˆ¼',\n",
       "  'completion': \"'ì €ëŠ” ì¸ê³µì§€ëŠ¥ì´ê¸° ë•Œë¬¸ì— ì—¬ì¹œê³¼ì˜ ë‹¤íˆ¼ì— ëŒ€í•´ì„œëŠ” ì¡°ì–¸ì„ ë“œë¦´ ìˆ˜ëŠ” ì—†ìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ë‹¤íˆ¬ê²Œ ë˜ë©´ ìƒëŒ€ë°©ì˜ ë§ì„ ê²½ì²­í•˜ëŠ” íƒœë„ì™€ ì„œë¡œì˜ ê°ì •ì„ ì¡´ì¤‘í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤. ë…¼ë¦¬ì ì´ê³  ì°¨ë¶„í•œ ëŒ€í™”ë¥¼ í†µí•´ ë¬¸ì œë¥¼ í•´ê²°í•´ë³´ì„¸ìš”. ê·¸ë¦¬ê³  ì„œë¡œë¥¼ ì´í•´í•˜ê³  ì„œë¡œì˜ ì˜ê²¬ì„ ìˆ˜ìš©í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.\",\n",
       "  'tokens': 153}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json \n",
    "data_path_1_SFT = '/aiffel/KoChatGPT/data_kochatgpt/kochatgpt_1_SFT.jsonl' \n",
    "with open(data_path_1_SFT, \"r\", encoding='utf-8-sig') as json_file:\n",
    "    list_data_dict = json.load(json_file)\n",
    "\n",
    "print(len(list_data_dict))\n",
    "list_data_dict[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c59f0e5",
   "metadata": {},
   "source": [
    "### RM ë°ì´í„° - ê¸°ë³¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad2ce794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10220\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'prompt': 'ë²ˆë””ëŠ” ìì‹ ì´ íƒì •ì¡ì§€, ë²”ì£„ì†Œì„¤ ê·¸ë¦¬ê³  ì„±ë²”ì£„ ê´€ë ¨ ì‹¤ì œ ë²”ì£„ ë‹¤íë©˜í„°ë¦¬ë“¤ì„ íƒë…í–ˆë‹¤ê³  ëˆ„êµ¬ì—ê²Œ ë§í–ˆë‚˜?',\n",
       "  'completion_0': 'Allow me to answer your question. I know that you are curious about me.',\n",
       "  'completion_1': 'ë²ˆë””ëŠ” ë‹¤ì–‘í•œ ì¸í„°ë·°ìë“¤ê³¼ ë‰´ìŠ¤í™ë³´ ë‹´ë‹¹ìë“¤ê³¼ì˜ ë©´ë‹´ ë•Œ ë°í˜”ë‹¤.',\n",
       "  'completion_2': 'ë¼ì´ì–¸ì—ê²Œ ë§í–ˆë‹¤.',\n",
       "  'ranking': [2, 1, 0]},\n",
       " {'prompt': 'ê°œí¬ì£¼ê³µì•„íŒŒíŠ¸ëŠ” ëª‡ ë‹¨ì§€ë¡œ ì´ë£¨ì–´ì ¸ ìˆë‚˜?',\n",
       "  'completion_0': 'ê°œí¬ì£¼ê³µì•„íŒŒíŠ¸ëŠ” ë‹¤ì„¯ ë‹¨ì§€ë¡œ ì´ë£¨ì–´ì ¸ ìˆìŠµë‹ˆë‹¤.',\n",
       "  'completion_1': 'ì´ë‚  ëª©ì†¡ì—ì„œ êµ¬ê¸€ìƒìœ„ë…¸',\n",
       "  'completion_2': 'ê°œí¬ì£¼ê³µì•„íŒŒíŠ¸ëŠ” ì´ 27ê°œ ë‹¨ì§€ë¡œ ì´ë£¨ì–´ì ¸ ìˆìŠµë‹ˆë‹¤.',\n",
       "  'ranking': [2, 0, 1]},\n",
       " {'prompt': 'ê¹€ì˜ì‚¼ì˜ í›„ë³´ ì‹œì ˆ ì§€ì—­í‘œì‹¬ì„ ê²¨ëƒ¥í•œ ë°œì–¸ì„ ë¬¸ì œì‚¼ì€ í›„ë³´ëŠ”?',\n",
       "  'completion_0': 'The diameter of the Metallic domain is bigger than the Hyperonic domain.',\n",
       "  'completion_1': 'ì´ ì§ˆë¬¸ì€ ì¡°ê¸ˆ ë¶ˆë¶„ëª…í•©ë‹ˆë‹¤. ê¹€ì˜ì‚¼ ëŒ€í†µë ¹ì´ í›„ë³´ ì‹œì ˆì— ì–´ë–¤ ë°œì–¸ì„ í–ˆê³ , ëˆ„ê°€ ê·¸ ë°œì–¸ì„ ë¬¸ì œì‚¼ì•˜ëŠ”ì§€ì— ë”°ë¼ ë‹µì´ ë‹¤ë¥¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\\\n\\\\në§Œì•½ ê¹€ì˜ì‚¼ ëŒ€í†µë ¹ì´ í›„ë³´ ì‹œì ˆì— ì§€ì—­í‘œì‹¬ì„ ê²¨ëƒ¥í•œ ë°œì–¸ì„ í–ˆë‹¤ëŠ” ê°€ì •í•˜ì—, ê·¸ ë°œì–¸ì„ ë¬¸ì œì‚¼ì€ í›„ë³´ê°€ ëˆ„êµ¬ì˜€ëŠ”ì§€ë¥¼ ëŒ€ë‹µí•˜ìë©´, ê·¸ ë‹µì€ ì´í™”ì„  ë‹¹ì‹œ ë¯¼ì£¼ë‹¹ ëŒ€í†µë ¹ í›„ë³´ê°€ ë  ê²ƒì…ë‹ˆë‹¤. 1992ë…„ ì´ì„  ë•Œ, ê¹€ì˜ì‚¼ ëŒ€ì„ í›„ë³´ëŠ” \"ì§‘ê°’ì´ ì˜¤ë¥¸ ë…¸ëŸ‰ì§„ì—­ ë¶€ê·¼ì˜ ë¶€ë™ì‚° ê°€ê²©ì€ ì„¸ì›”í˜¸ í­ì¹¨ í›„ \\\\\\'ê°•ë‚¨ ë„ì‹œì¬ìƒ\\\\\\' ì¼í™˜ìœ¼ë¡œ ìƒìŠ¹í–ˆë‹¤\"ëŠ” ë°œì–¸ì„ í–ˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì´í™”ì„  í›„ë³´ëŠ” ì´ ë°œì–¸ì„ \"ì „êµ­ì ìœ¼ë¡œ ê²½ì œì  ë°œì „ì´ ì´ë£¨ì–´ì§€ì§€ ì•Šì€ ì§€ë°©ë¯¼ì˜ ë§ˆìŒì„ ë©€ë¦¬í•´ì§€ë ¤ëŠ” ë¬´ë¡€í•œ ë°œì–¸\"ì´ë¼ê³  ë¹„íŒí•˜ë©° ë¬¸ì œì‚¼ì•˜ìŠµë‹ˆë‹¤.\\\\n\\\\ní•˜ì§€ë§Œ, ì´ ì§ˆë¬¸ì„ ë‹µë³€í•˜ëŠ” ë° ìˆì–´ì„œ ë³´ë‹¤ ëª…í™•í•œ ì •ë³´ê°€ ìˆìœ¼ë©´ ë‹µë³€ì„ ë³´ì™„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.',\n",
       "  'completion_2': 'ê¹€ì˜ì‚¼ì˜ í›„ë³´ ì‹œì ˆì— ì§€ì—­í‘œì‹¬ì„ ê²¨ëƒ¥í•œ ë°œì–¸ì€ ëŒ€í†µë ¹ ë‹¹ì„  ì „ê¹Œì§€ ëŒ€í•œë¯¼êµ­ ì •ë¶€ê°€ ì¶”êµ¬í•˜ê³  ìˆëŠ” ë¯¼ì£¼ì£¼ì˜ ê´‘ë²”ìœ„í•˜ê²Œ í™•ë¦½ê³¼ ë³´ìˆ˜ì˜ ì‚¬ìƒì„ ì´ì–´ê°€ëŠ” ë° ìˆì–´ ì§€ì—­ê²½ì œ ë°œì „ê³¼ ê³µê³µì„œë¹„ìŠ¤ ì‹ ì† ê°œì„ ì„ ìœ„í•´ í•©ë¦¬ì ì¸ êµ­ê°€ ì •ì±…ì— ë”°ë¥´ëŠ” ë°©í–¥ì„±ì„ ì œì‹œí•˜ê³  ìˆìŠµë‹ˆë‹¤.',\n",
       "  'ranking': [1, 2, 0]}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path_2_RM = '/aiffel/KoChatGPT/data_kochatgpt/kochatgpt_2_RM.jsonl'\n",
    "with open(data_path_2_RM, \"r\", encoding='utf-8-sig') as json_file:\n",
    "    list_data_dict = json.load(json_file)\n",
    "\n",
    "print(len(list_data_dict))\n",
    "list_data_dict[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59909a3",
   "metadata": {},
   "source": [
    "### PPO ë°ì´í„° - ê¸°ë³¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c72a602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'prompt': 'ë²ˆë””ëŠ” ìì‹ ì´ íƒì •ì¡ì§€, ë²”ì£„ì†Œì„¤ ê·¸ë¦¬ê³  ì„±ë²”ì£„ ê´€ë ¨ ì‹¤ì œ ë²”ì£„ ë‹¤íë©˜í„°ë¦¬ë“¤ì„ íƒë…í–ˆë‹¤ê³  ëˆ„êµ¬ì—ê²Œ ë§í–ˆë‚˜?'},\n",
       " {'prompt': 'ê°œí¬ì£¼ê³µì•„íŒŒíŠ¸ëŠ” ëª‡ ë‹¨ì§€ë¡œ ì´ë£¨ì–´ì ¸ ìˆë‚˜?'},\n",
       " {'prompt': 'ê¹€ì˜ì‚¼ì˜ í›„ë³´ ì‹œì ˆ ì§€ì—­í‘œì‹¬ì„ ê²¨ëƒ¥í•œ ë°œì–¸ì„ ë¬¸ì œì‚¼ì€ í›„ë³´ëŠ”?'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path_3_PPO = '/aiffel/KoChatGPT/data_kochatgpt/kochatgpt_3_PPO.jsonl'\n",
    "with open(data_path_3_PPO, \"r\", encoding='utf-8-sig') as json_file:\n",
    "    list_data_dict = json.load(json_file)\n",
    "\n",
    "print(len(list_data_dict))\n",
    "list_data_dict[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8bef8248",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.optim import Adam\n",
    "from datasets import load_dataset\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from copy import deepcopy\n",
    "import copy\n",
    "import logging\n",
    "import json\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6574c2aa",
   "metadata": {},
   "source": [
    "### SFT ëª¨ë¸ í† í¬ë‚˜ì´ì € ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b156cf3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2TokenizerFast(name_or_path='skt/kogpt2-base-v2', vocab_size=51200, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '</s>', 'eos_token': '</s>', 'unk_token': '</s>', 'pad_token': '</s>'}, clean_up_tokenization_spaces=True)\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained('skt/kogpt2-base-v2')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    'skt/kogpt2-base-v2', bos_token='</s>', eos_token='</s>', unk_token='</s>', pad_token='</s>',\n",
    "    padding_side=\"right\",\n",
    "    model_max_length=512,\n",
    ")\n",
    "\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58cfd40",
   "metadata": {},
   "source": [
    "### SFT ë°ì´í„°ì…‹ í´ë˜ìŠ¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a2059f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Dict, Sequence\n",
    "\n",
    "class SFT_dataset(Dataset):\n",
    "\n",
    "    def __init__(self, data_path_1_SFT: str, tokenizer: transformers.PreTrainedTokenizer, verbose=False):\n",
    "        super(SFT_dataset, self).__init__()\n",
    "        logging.warning(\"Loading data...\")\n",
    "\n",
    "        pattern_instruction = 'prompt'  # instruction\n",
    "        pattern_output = 'completion'  # response\n",
    "\n",
    "        with open(data_path_1_SFT, \"r\", encoding='utf-8-sig') as json_file:\n",
    "            list_data_dict = json.load(json_file)\n",
    "\n",
    "        PROMPT_DICT = {\n",
    "            \"prompt_input\": (\n",
    "                \"### Instruction(ëª…ë ¹ì–´):\\n{prompt}\\n\\n### Response(ì‘ë‹µ):\"\n",
    "            )\n",
    "        }\n",
    "\n",
    "        prompt_input = PROMPT_DICT[\"prompt_input\"]\n",
    "\n",
    "        sources = []\n",
    "        for example in list_data_dict:\n",
    "            tmp = prompt_input.format_map(example)\n",
    "            sources.append(tmp)\n",
    "\n",
    "        targets = []\n",
    "        for example in list_data_dict:\n",
    "            targets.append(f\"{example[pattern_output]}{tokenizer.eos_token}\")\n",
    "        examples = [s + t for s, t in zip(sources, targets)]\n",
    "\n",
    "        sources_tokenized = self._tokenize_fn(sources, tokenizer)  # source\n",
    "        examples_tokenized = self._tokenize_fn(examples, tokenizer)  # source + target\n",
    "\n",
    "        input_ids = examples_tokenized[\"input_ids\"]\n",
    "        labels = copy.deepcopy(input_ids)\n",
    "        for label, source_len in zip(labels, sources_tokenized[\"input_ids_lens\"]):\n",
    "            label[:source_len] = -100\n",
    "\n",
    "        data_dict = dict(input_ids=input_ids, labels=labels)\n",
    "\n",
    "        self.input_ids = data_dict[\"input_ids\"]\n",
    "        self.labels = data_dict[\"labels\"]\n",
    "        logging.warning(\"Loading data done!!: %d\"%(len(self.labels)))\n",
    "\n",
    "\n",
    "    def _tokenize_fn(self, strings: Sequence[str], tokenizer: transformers.PreTrainedTokenizer) -> Dict:\n",
    "        tokenized_list = [\n",
    "            tokenizer(\n",
    "                text,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=\"longest\",\n",
    "                max_length=tokenizer.model_max_length,\n",
    "                truncation=True,\n",
    "            )\n",
    "            for text in strings\n",
    "        ]\n",
    "        input_ids = labels = [tokenized.input_ids[0] for tokenized in tokenized_list]\n",
    "        input_ids_lens = labels_lens = [\n",
    "            tokenized.input_ids.ne(tokenizer.pad_token_id).sum().item() for tokenized in tokenized_list\n",
    "        ]\n",
    "        return dict(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "            input_ids_lens=input_ids_lens,\n",
    "            labels_lens=labels_lens,\n",
    "        )\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "\n",
    "    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
    "        return dict(input_ids=self.input_ids[i], labels=self.labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d5f0e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorForSupervisedDataset(object): \n",
    "\n",
    "    tokenizer: transformers.PreTrainedTokenizer\n",
    "\n",
    "    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
    "        input_ids, labels = tuple([instance[key] for instance in instances] for key in (\"input_ids\", \"labels\"))\n",
    "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "            input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
    "        )\n",
    "        labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value= -100)\n",
    "        return dict(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8322ce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Loading data...\n",
      "WARNING:root:Loading data done!!: 12000\n"
     ]
    }
   ],
   "source": [
    "train_dataset = SFT_dataset(data_path_1_SFT='/aiffel/KoChatGPT/data_kochatgpt/kochatgpt_1_SFT.jsonl', tokenizer=tokenizer)\n",
    "data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "69faa304",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/aiffel/KoChatGPT/test\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    warmup_steps=5,\n",
    "    prediction_loss_only=True,\n",
    "    fp16 = True\n",
    "    )\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "21875b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.empty_cache()      # ìºì‹œ ë¹„ì›€ (ì‹¤ì œ ë©”ëª¨ë¦¬ í•´ì œëŠ” ì•„ë‹˜)\n",
    "torch.cuda.ipc_collect()      # inter-process communication ìºì‹œ ì •ë¦¬ (í•„ìš”í•œ ê²½ìš°)\n",
    "\n",
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:32\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f97e847",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3000' max='3000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3000/3000 08:33, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.149500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.947900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.759300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>2.742400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>2.656700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()\n",
    "model.save_pretrained('/aiffel/KoChatGPT/output_1_SFT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "739d342e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer # rouge_score ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
    "\n",
    "def calculate_and_print_rouge_scores(generated_texts: list,\n",
    "                                     reference_texts: list,\n",
    "                                     use_stemmer: bool = False,\n",
    "                                     print_individual_scores: bool = True):\n",
    "    \"\"\"\n",
    "    ìƒì„±ëœ í…ìŠ¤íŠ¸ì™€ ì°¸ì¡° í…ìŠ¤íŠ¸ ê°„ì˜ ROUGE ì ìˆ˜ë¥¼ ê³„ì‚°í•˜ê³  ê²°ê³¼ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.\n",
    "\n",
    "    Args:\n",
    "        generated_texts (list): ëª¨ë¸ì´ ìƒì„±í•œ í…ìŠ¤íŠ¸(ì‘ë‹µ)ì˜ ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤.\n",
    "        reference_texts (list): ì°¸ì¡° (ì •ë‹µ) í…ìŠ¤íŠ¸ì˜ ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤.\n",
    "        use_stemmer (bool): ROUGE ê³„ì‚° ì‹œ í˜•íƒœì†Œ ë¶„ì„ê¸°(stemmer) ì‚¬ìš© ì—¬ë¶€ì…ë‹ˆë‹¤.\n",
    "                            Trueë¡œ ì„¤ì • ì‹œ, NLTK ë“±ì˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ í•„ìš”í•  ìˆ˜ ìˆìœ¼ë©°,\n",
    "                            í•œêµ­ì–´ì˜ ê²½ìš° ì ì ˆí•œ í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ê¸° ì„¤ì •ì´ í•„ìš”í•©ë‹ˆë‹¤.\n",
    "                            ê¸°ë³¸ê°’ì€ Falseì…ë‹ˆë‹¤.\n",
    "        print_individual_scores (bool): ê° ìƒ˜í”Œë³„ ROUGE ì ìˆ˜ ì¶œë ¥ ì—¬ë¶€ì…ë‹ˆë‹¤. ê¸°ë³¸ê°’ì€ Trueì…ë‹ˆë‹¤.\n",
    "\n",
    "    Returns:\n",
    "        dict: í‰ê·  ROUGE Precision, Recall, F1-scoreë¥¼ ë‹´ì€ ë”•ì…”ë„ˆë¦¬ì…ë‹ˆë‹¤.\n",
    "              (ì˜ˆ: {'rouge1_precision': 0.XX, 'rouge1_recall': 0.YY, 'rouge1_f1': 0.ZZ, ...})\n",
    "              í…ìŠ¤íŠ¸ ëª©ë¡ì´ ë¹„ì–´ ìˆê±°ë‚˜ ê¸¸ì´ê°€ ë‹¤ë¥¼ ê²½ìš° Noneì„ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    if not generated_texts or not reference_texts:\n",
    "        print(\"ğŸš« ìƒì„±ëœ í…ìŠ¤íŠ¸ ë˜ëŠ” ì°¸ì¡° í…ìŠ¤íŠ¸ ëª©ë¡ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ROUGE ì ìˆ˜ë¥¼ ê³„ì‚°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return None\n",
    "\n",
    "    if len(generated_texts) != len(reference_texts):\n",
    "        print(f\"ğŸš« ìƒì„±ëœ í…ìŠ¤íŠ¸ì˜ ìˆ˜({len(generated_texts)})ì™€ ì°¸ì¡° í…ìŠ¤íŠ¸ì˜ ìˆ˜({len(reference_texts)})ê°€ ì¼ì¹˜í•˜ì§€ ì•Šì•„ ROUGE ì ìˆ˜ë¥¼ ê³„ì‚°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        # rouge_scorer ì´ˆê¸°í™”\n",
    "        # ì§€ì›ë˜ëŠ” ë©”íŠ¸ë¦­: 'rouge1', 'rouge2', ..., 'rougeL', 'rougeLsum'\n",
    "        scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=use_stemmer)\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ RougeScorer ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "        print(\"ROUGE ì ìˆ˜ ê³„ì‚°ì„ ì§„í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return None\n",
    "\n",
    "    # ëª¨ë“  ìƒ˜í”Œì˜ ROUGE ì ìˆ˜ë¥¼ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬\n",
    "    all_scores_p = {'rouge1': [], 'rouge2': [], 'rougeL': []} # Precision\n",
    "    all_scores_r = {'rouge1': [], 'rouge2': [], 'rougeL': []} # Recall\n",
    "    all_scores_f = {'rouge1': [], 'rouge2': [], 'rougeL': []} # F1-score\n",
    "\n",
    "    if print_individual_scores:\n",
    "        print(\"\\n\\n--- ğŸ’¯ ê°œë³„ ìƒ˜í”Œ ROUGE ìŠ¤ì½”ì–´ ---\")\n",
    "\n",
    "    for i in range(len(generated_texts)):\n",
    "        candidate = str(generated_texts[i])  # ìƒì„±ëœ í…ìŠ¤íŠ¸\n",
    "        reference = str(reference_texts[i])  # ì°¸ì¡° í…ìŠ¤íŠ¸\n",
    "\n",
    "        # ROUGE ì ìˆ˜ ê³„ì‚°\n",
    "        # scoresëŠ” ê° rouge íƒ€ì… (ì˜ˆ: 'rouge1')ì— ëŒ€í•´ Score(precision=..., recall=..., fmeasure=...) ê°ì²´ë¥¼ í¬í•¨í•˜ëŠ” ë”•ì…”ë„ˆë¦¬\n",
    "        scores = scorer.score(reference, candidate)\n",
    "\n",
    "        if print_individual_scores:\n",
    "            print(f\"\\nğŸ“œ ìƒ˜í”Œ {i+1}:\")\n",
    "            print(f\"  ROUGE ì ìˆ˜:\")\n",
    "            for rouge_type, score_obj in scores.items():\n",
    "                print(f\"    {rouge_type.upper()}: P={score_obj.precision:.4f}, R={score_obj.recall:.4f}, F1={score_obj.fmeasure:.4f}\")\n",
    "\n",
    "        # ê° íƒ€ì…ë³„ ì ìˆ˜ ì €ì¥\n",
    "        for rouge_type in ['rouge1', 'rouge2', 'rougeL']:\n",
    "            all_scores_p[rouge_type].append(scores[rouge_type].precision)\n",
    "            all_scores_r[rouge_type].append(scores[rouge_type].recall)\n",
    "            all_scores_f[rouge_type].append(scores[rouge_type].fmeasure)\n",
    "\n",
    "    # í‰ê·  ROUGE ì ìˆ˜ ê³„ì‚°\n",
    "    average_results = {}\n",
    "    if all_scores_f['rouge1']:  # ì ìˆ˜ê°€ í•˜ë‚˜ë¼ë„ ê³„ì‚°ë˜ì—ˆë‹¤ë©´\n",
    "        print(\"\\n\\n--- ğŸ“‰ ì „ì²´ ìƒ˜í”Œ í‰ê·  ROUGE ìŠ¤ì½”ì–´ ---\")\n",
    "        for rouge_type in ['rouge1', 'rouge2', 'rougeL']:\n",
    "            avg_p = sum(all_scores_p[rouge_type]) / len(all_scores_p[rouge_type])\n",
    "            avg_r = sum(all_scores_r[rouge_type]) / len(all_scores_r[rouge_type])\n",
    "            avg_f = sum(all_scores_f[rouge_type]) / len(all_scores_f[rouge_type])\n",
    "\n",
    "            average_results[f'{rouge_type}_precision'] = avg_p\n",
    "            average_results[f'{rouge_type}_recall'] = avg_r\n",
    "            average_results[f'{rouge_type}_f1'] = avg_f\n",
    "            \n",
    "            print(f\"  í‰ê·  {rouge_type.upper()}: P={avg_p:.4f}, R={avg_r:.4f}, F1={avg_f:.4f}\")\n",
    "        \n",
    "        return average_results\n",
    "    else:\n",
    "        print(\"âš ï¸ ê³„ì‚°ëœ ROUGE ì ìˆ˜ê°€ ì—†ì–´ í‰ê· ì„ ë°˜í™˜í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f584a89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "eos_token_id = tokenizer.eos_token_id\n",
    "pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bba9fd7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Instruction(ëª…ë ¹ì–´):\n",
      "ë¶ˆê³ ê¸°ìš© ê³ ê¸° í•œìš°ì—ìš”?\n",
      "\n",
      "### Response(ì‘ë‹µ):'ì €ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì´ê¸° ë•Œë¬¸ì— ê³ ê¸°ë¥¼ ë¨¹ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì¼ë°˜ì ìœ¼ë¡œ ë¶ˆê³ ê¸°ëŠ” ê±´ê°•í•˜ê³  ë§›ìˆëŠ” ìŒì‹ ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤.\n",
      "\n",
      "### Instruction(ëª…ë ¹ì–´):\n",
      "ë¦¬ì²˜ë“œ ë‹‰ìŠ¨ì´ 43ëŒ€ ë¶€í†µë ¹ì§ì„ ìˆ˜í–‰í•œ ë…„ë„ëŠ”?\n",
      "\n",
      "### Response(ì‘ë‹µ):'ë¦¬ì²˜ë“œ ë‹‰ìŠ¨ì€ 46ëŒ€ ë¶€í†µë ¹ì§ì„ ìˆ˜í–‰í–ˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "### Instruction(ëª…ë ¹ì–´):\n",
      "ì‹œì¹´ê³  ì˜¤í—¤ì–´ êµ­ì œê³µí•­ì€ ì–´ë””ì— ìˆì–´?\n",
      "\n",
      "### Response(ì‘ë‹µ):'ì €ëŠ” ì¸ê³µì§€ëŠ¥ ì–´ì‹œìŠ¤í„´íŠ¸ì´ê¸° ë•Œë¬¸ì— ì‹œì¹´ê³ ì— ëŒ€í•œ ì •ë³´ë¥¼ ì•Œ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì¼ë°˜ì ìœ¼ë¡œ ì‹œì¹´ê³ ëŠ” ë¯¸êµ­ ë‚´ì— ìœ„ì¹˜í•´ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "### Instruction(ëª…ë ¹ì–´):\n",
      "ì˜¤ëŠ˜ ë¯¸ì„¸ë¨¼ì§€ ì–´ë•Œ?\n",
      "\n",
      "### Response(ì‘ë‹µ):'ì €ëŠ” ì¸ê³µì§€ëŠ¥ ì±—ë´‡ì´ê¸° ë•Œë¬¸ì— ë¯¸ì„¸ë¨¼ì§€ ì •ë³´ë¥¼ ì•Œ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì¼ë°˜ì ìœ¼ë¡œ ë¯¸ì„¸ë¨¼ì§€ëŠ” ê±´ê°•ì— ë§¤ìš° ì¤‘ìš”í•˜ê¸° ë•Œë¬¸ì— ì™¸ì¶œ ì‹œ ë§ˆìŠ¤í¬ ì°©ìš©ê³¼ ì˜ˆë°©ìˆ˜ì¹™ì„ ì¤€ìˆ˜í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "generator = pipeline('text-generation', model='/aiffel/KoChatGPT/output_1_SFT', tokenizer=tokenizer)\n",
    "\n",
    "generation_args = dict(   \n",
    "    num_beams=4,\n",
    "    repetition_penalty=2.0,\n",
    "    no_repeat_ngram_size=4,\n",
    "    eos_token_id=eos_token_id,\n",
    "    pad_token_id=eos_token_id, \n",
    "    max_new_tokens=64,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"### Instruction(ëª…ë ¹ì–´):\\n{prompt}\\n\\n### Response(ì‘ë‹µ):\"\n",
    "    )\n",
    "}\n",
    "\n",
    "list_prompt = ['ë¶ˆê³ ê¸°ìš© ê³ ê¸° í•œìš°ì—ìš”?',\n",
    "               'ë¦¬ì²˜ë“œ ë‹‰ìŠ¨ì´ 43ëŒ€ ë¶€í†µë ¹ì§ì„ ìˆ˜í–‰í•œ ë…„ë„ëŠ”?',\n",
    "               'ì‹œì¹´ê³  ì˜¤í—¤ì–´ êµ­ì œê³µí•­ì€ ì–´ë””ì— ìˆì–´?',\n",
    "               'ì˜¤ëŠ˜ ë¯¸ì„¸ë¨¼ì§€ ì–´ë•Œ?']\n",
    "\n",
    "list_prompt = [PROMPT_DICT['prompt_input'].format_map({'prompt' : tmp}) for tmp in list_prompt]\n",
    "\n",
    "list_result = generator(list_prompt, **generation_args)   \n",
    "for prompt, result in zip(list_prompt, list_result):\n",
    "    print()\n",
    "    print((result[0]['generated_text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca1fa14",
   "metadata": {},
   "source": [
    "## ì •ì„±ì  í‰ê°€\n",
    "\n",
    "ì „ì²´ì ìœ¼ë¡œ ì¸ê³µì§€ëŠ¥ ì±—ë´‡ì— ì•Œë§ì€ ëŒ€ë‹µì„ í•˜ê³  ìˆë‹¤ëŠ” ìƒê°ì´ ë“ ë‹¤.\n",
    "\n",
    "ë‚´ìš©ì„ ë¶„ì„í•´ë³´ìë©´\n",
    "\n",
    "1. ë¶ˆê³ ê¸°ì— ëŒ€í•œ ë‚´ìš©ì€ ê³ ê¸°ë¥¼ ë¨¹ì„ ìˆ˜ ìˆëŠ”ì§€ì— ëŒ€í•œ ë‹µë³€ì´ ë‚˜ì˜¤ëŠ” ê²ƒì€ ìì—°ìŠ¤ëŸ½ì§€ ì•Šë‹¤ê³  ìƒê°í•œë‹¤.\n",
    "2. ë…„ë„ë¥¼ ë¬»ëŠ” ì§ˆë¬¸ì— ì ì ˆí•˜ì§€ ì•Šì€ ëŒ€ë‹µì´ë‹¤.\n",
    "3. í¬ê´„ì ì´ì§€ë§Œ í‹€ë¦° ë‚´ìš©ì€ ì•„ë‹ˆë‹¤.\n",
    "4. ë¯¸ì„¸ë¨¼ì§€ ì •ë³´ë¥¼ ëª¨ë¥´ëŠ” ìƒí™©ì—ì„œ ì ì ˆí•œ ëŒ€ë‹µì´ë‹¤.\n",
    "\n",
    "eos tokenê³¼ pad token, early stoppingì„ ì¶”ê°€í•˜ì—¬ ì“¸ëª¨ì—†ëŠ” ë‚´ìš©ì´ ë‚˜ì˜¤ëŠ” ë¶€ë¶„ì„ ë‚˜ì˜¤ì§€ ì•Šë„ë¡ ìˆ˜ì •í•˜ì˜€ë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bae9880",
   "metadata": {},
   "source": [
    "## ChatGPT 4oê°€ ìƒì„±í•œ referenceì™€ ROUGE ìŠ¤ì½”ì–´ ë¹„êµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e689edfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Instruction(ëª…ë ¹ì–´):\n",
      "ë¶ˆê³ ê¸°ìš© ê³ ê¸° í•œìš°ì—ìš”?\n",
      "\n",
      "### Response(ì‘ë‹µ):'ì €ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì´ê¸° ë•Œë¬¸ì— ê³ ê¸°ë¥¼ ë¨¹ì„ ìˆ˜ëŠ” ì—†ìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì¼ë°˜ì ìœ¼ë¡œ ë¶ˆê³ ê¸°ëŠ” ê³ ê¸°ì™€ í•¨ê»˜ ë¨¹ëŠ” ê²½ìš°ê°€ ë§ìœ¼ë‹ˆê¹Œ ë§›ìˆê²Œ ë“œì„¸ìš”!\n",
      "\n",
      "### Instruction(ëª…ë ¹ì–´):\n",
      "ë¦¬ì²˜ë“œ ë‹‰ìŠ¨ì´ 43ëŒ€ ë¶€í†µë ¹ì§ì„ ìˆ˜í–‰í•œ ë…„ë„ëŠ”?\n",
      "\n",
      "### Response(ì‘ë‹µ):'ë¦¬ì²˜ë“œ ë‹‰ìŠ¨ì€ 41ëŒ€ ë¶€í†µë ¹ì§ì„ ìˆ˜í–‰í–ˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "### Instruction(ëª…ë ¹ì–´):\n",
      "ì‹œì¹´ê³  ì˜¤í—¤ì–´ êµ­ì œê³µí•­ì€ ì–´ë””ì— ìˆì–´?\n",
      "\n",
      "### Response(ì‘ë‹µ):'ì‹œì¹´ê³  ì˜¤ í—¤ì–´ êµ­ì œê³µí•­ì€ ë¯¸êµ­ ìº˜ë¦¬í¬ë‹ˆì•„ì£¼ ìƒŒí”„ë€ì‹œìŠ¤ì½”ì— ìœ„ì¹˜í•´ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "### Instruction(ëª…ë ¹ì–´):\n",
      "ì˜¤ëŠ˜ ë¯¸ì„¸ë¨¼ì§€ ì–´ë•Œ?\n",
      "\n",
      "### Response(ì‘ë‹µ):'ì €ëŠ” ì¸ê³µì§€ëŠ¥ ì±—ë´‡ì´ê¸° ë•Œë¬¸ì— ë¯¸ì„¸ë¨¼ì§€ ì •ë³´ë¥¼ ì•Œ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì¼ë°˜ì ìœ¼ë¡œ ë¯¸ì„¸ë¨¼ì§€ëŠ” í˜¸í¡ê¸° ê±´ê°•ì— ë§¤ìš° ì¤‘ìš”í•œ ì—­í• ì„ í•˜ê¸° ë•Œë¬¸ì—, ì™¸ì¶œ ì‹œ ë°˜ë“œì‹œ ë§ˆìŠ¤í¬ë¥¼ ì°©ìš©í•˜ê³  ì‹¤ë‚´ì—ì„œ ëŒ€ê¸°ì˜¤ì—¼ì„ ì¤„ì´ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.\n",
      "\n",
      "Extracting generated responses...\n",
      "\n",
      "âœ¨ ROUGE ìŠ¤ì½”ì–´ í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ì—¬ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...\n",
      "\n",
      "\n",
      "--- ğŸ’¯ ê°œë³„ ìƒ˜í”Œ ROUGE ìŠ¤ì½”ì–´ ---\n",
      "\n",
      "ğŸ“œ ìƒ˜í”Œ 1:\n",
      "  ROUGE ì ìˆ˜:\n",
      "    ROUGE1: P=0.0000, R=0.0000, F1=0.0000\n",
      "    ROUGE2: P=0.0000, R=0.0000, F1=0.0000\n",
      "    ROUGEL: P=0.0000, R=0.0000, F1=0.0000\n",
      "\n",
      "ğŸ“œ ìƒ˜í”Œ 2:\n",
      "  ROUGE ì ìˆ˜:\n",
      "    ROUGE1: P=0.0000, R=0.0000, F1=0.0000\n",
      "    ROUGE2: P=0.0000, R=0.0000, F1=0.0000\n",
      "    ROUGEL: P=0.0000, R=0.0000, F1=0.0000\n",
      "\n",
      "ğŸ“œ ìƒ˜í”Œ 3:\n",
      "  ROUGE ì ìˆ˜:\n",
      "    ROUGE1: P=0.0000, R=0.0000, F1=0.0000\n",
      "    ROUGE2: P=0.0000, R=0.0000, F1=0.0000\n",
      "    ROUGEL: P=0.0000, R=0.0000, F1=0.0000\n",
      "\n",
      "ğŸ“œ ìƒ˜í”Œ 4:\n",
      "  ROUGE ì ìˆ˜:\n",
      "    ROUGE1: P=0.0000, R=0.0000, F1=0.0000\n",
      "    ROUGE2: P=0.0000, R=0.0000, F1=0.0000\n",
      "    ROUGEL: P=0.0000, R=0.0000, F1=0.0000\n",
      "\n",
      "\n",
      "--- ğŸ“‰ ì „ì²´ ìƒ˜í”Œ í‰ê·  ROUGE ìŠ¤ì½”ì–´ ---\n",
      "  í‰ê·  ROUGE1: P=0.0000, R=0.0000, F1=0.0000\n",
      "  í‰ê·  ROUGE2: P=0.0000, R=0.0000, F1=0.0000\n",
      "  í‰ê·  ROUGEL: P=0.0000, R=0.0000, F1=0.0000\n",
      "\n",
      "âœ… ROUGE ì ìˆ˜ ê³„ì‚°ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!\n"
     ]
    }
   ],
   "source": [
    "generator = pipeline('text-generation', model='/aiffel/KoChatGPT/output_1_SFT', tokenizer=tokenizer)\n",
    "\n",
    "generation_args = dict(   \n",
    "    num_beams=4,\n",
    "    repetition_penalty=2.0,\n",
    "    no_repeat_ngram_size=4,\n",
    "    eos_token_id=eos_token_id,\n",
    "    pad_token_id=eos_token_id, \n",
    "    max_new_tokens=64,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"### Instruction(ëª…ë ¹ì–´):\\n{prompt}\\n\\n### Response(ì‘ë‹µ):\"\n",
    "    )\n",
    "}\n",
    "\n",
    "list_prompt = ['ë¶ˆê³ ê¸°ìš© ê³ ê¸° í•œìš°ì—ìš”?',\n",
    "               'ë¦¬ì²˜ë“œ ë‹‰ìŠ¨ì´ 43ëŒ€ ë¶€í†µë ¹ì§ì„ ìˆ˜í–‰í•œ ë…„ë„ëŠ”?',\n",
    "               'ì‹œì¹´ê³  ì˜¤í—¤ì–´ êµ­ì œê³µí•­ì€ ì–´ë””ì— ìˆì–´?',\n",
    "               'ì˜¤ëŠ˜ ë¯¸ì„¸ë¨¼ì§€ ì–´ë•Œ?']\n",
    "\n",
    "list_prompt = [PROMPT_DICT['prompt_input'].format_map({'prompt' : tmp}) for tmp in list_prompt]\n",
    "\n",
    "list_result = generator(list_prompt, **generation_args)   \n",
    "for prompt, result in zip(list_prompt, list_result):\n",
    "    print()\n",
    "    print((result[0]['generated_text']))\n",
    "\n",
    "# --- ğŸ‘‡ ROUGE í•¨ìˆ˜ í˜¸ì¶œì„ ìœ„í•œ ì¶”ê°€ ì½”ë“œ ---\n",
    "\n",
    "# 1. ëª¨ë¸ì´ ìƒì„±í•œ \"ì‘ë‹µ\" ë¶€ë¶„ë§Œ ì¶”ì¶œí•˜ê¸°\n",
    "list_generated_responses = []\n",
    "response_marker = PROMPT_DICT[\"prompt_input\"].split(\"{prompt}\")[1].split(\"### Response(ì‘ë‹µ):\")[0] + \"### Response(ì‘ë‹µ):\" # \"### Response(ì‘ë‹µ):\" ë§ˆì»¤\n",
    "\n",
    "# ì›ë³¸ ì§ˆë¬¸ (ì°¸ì¡° í…ìŠ¤íŠ¸ì™€ ë§¤ì¹­í•˜ê¸° ìœ„í•¨)\n",
    "list_prompt_original = ['ë¶ˆê³ ê¸°ìš© ê³ ê¸° í•œìš°ì—ìš”?',\n",
    "                       'ë¦¬ì²˜ë“œ ë‹‰ìŠ¨ì´ 43ëŒ€ ë¶€í†µë ¹ì§ì„ ìˆ˜í–‰í•œ ë…„ë„ëŠ”?',\n",
    "                       'ì‹œì¹´ê³  ì˜¤í—¤ì–´ êµ­ì œê³µí•­ì€ ì–´ë””ì— ìˆì–´?',\n",
    "                       'ì˜¤ëŠ˜ ë¯¸ì„¸ë¨¼ì§€ ì–´ë•Œ?']\n",
    "\n",
    "print(\"\\nExtracting generated responses...\")\n",
    "for i, single_result_list in enumerate(list_result):\n",
    "    full_generated_text = single_result_list[0]['generated_text']\n",
    "    \n",
    "    # \"### Response(ì‘ë‹µ):\" ë§ˆì»¤ ì´í›„ì˜ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œ\n",
    "    # rfindë¥¼ ì‚¬ìš©í•˜ì—¬ í”„ë¡¬í”„íŠ¸ì™€ ì‘ë‹µ ì‚¬ì´ì— ë§ˆì»¤ê°€ ì—¬ëŸ¬ ë²ˆ ë‚˜ì˜¤ëŠ” ê²½ìš°, ë§ˆì§€ë§‰ ë§ˆì»¤ë¥¼ ê¸°ì¤€ìœ¼ë¡œ í•¨\n",
    "    marker_position = full_generated_text.rfind(response_marker)\n",
    "    \n",
    "    if marker_position != -1:\n",
    "        # ë§ˆì»¤ ë‹¤ìŒë¶€í„° í…ìŠ¤íŠ¸ ì¶”ì¶œ\n",
    "        response_only = full_generated_text[marker_position + len(response_marker):].strip()\n",
    "    else:\n",
    "        # ë§ˆì»¤ë¥¼ ì°¾ì§€ ëª»í•œ ê²½ìš°, í¬ë§·íŒ…ëœ í”„ë¡¬í”„íŠ¸(list_prompt[i]) ì´í›„ì˜ í…ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì˜¤ë ¤ëŠ” ì‹œë„\n",
    "        # ì´ ë°©ë²•ì€ ëª¨ë¸ì´ í”„ë¡¬í”„íŠ¸ë¥¼ ê·¸ëŒ€ë¡œ ë°˜ë³µí•˜ì§€ ì•Šìœ¼ë©´ ì •í™•í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "        try:\n",
    "            response_only = full_generated_text.split(list_prompt[i])[1].strip()\n",
    "        except IndexError:\n",
    "            print(f\"âš ï¸ Warning: ì‘ë‹µ ë§ˆì»¤ ë° í”„ë¡¬í”„íŠ¸ ë¶„ë¦¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ìƒ˜í”Œ {i}ëŠ” ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\")\n",
    "            response_only = full_generated_text # ìµœí›„ì˜ ìˆ˜ë‹¨ (ê°œì„  í•„ìš”)\n",
    "\n",
    "    # eos_token (ì˜ˆ: '\\n' ë˜ëŠ” tokenizer.eos_token) ì •ë¦¬\n",
    "    # generation_argsì˜ eos_token_idê°€ 375 ('\\n')ë¡œ ë˜ì–´ ìˆìœ¼ë¯€ë¡œ, ì´ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì²˜ë¦¬\n",
    "    # tokenizer.eos_token (ì˜ˆ: '</s>')ë„ í•¨ê»˜ ê³ ë ¤í•˜ë©´ ì¢‹ìŠµë‹ˆë‹¤.\n",
    "    if tokenizer.eos_token: # tokenizer ë³€ìˆ˜ê°€ ì‚¬ìš© ê°€ëŠ¥í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "         response_only = response_only.replace(tokenizer.eos_token, \"\")\n",
    "    \n",
    "    # eos_token_id=375ê°€ '\\n'ì´ë¼ê³  ê°€ì •í•˜ê³ , ì²« ì¤„ë°”ê¿ˆ ì´ì „ í…ìŠ¤íŠ¸ë§Œ ì‚¬ìš©í•˜ê±°ë‚˜ ë¶ˆí•„ìš”í•œ ì¤„ë°”ê¿ˆ ì œê±°\n",
    "    response_only = response_only.split('\\n')[0].strip()\n",
    "    \n",
    "    list_generated_responses.append(response_only)\n",
    "    # print(f\"  Q: {list_prompt_original[i]}\") # ë””ë²„ê¹…ìš©\n",
    "    # print(f\"  Extracted A: {response_only}\") # ë””ë²„ê¹…ìš©\n",
    "\n",
    "# 2. ì°¸ì¡° (ì •ë‹µ) í…ìŠ¤íŠ¸ ì¤€ë¹„\n",
    "#    list_prompt_originalì˜ ê° ì§ˆë¬¸ì— ëŒ€í•œ ì´ìƒì ì¸ ë‹µë³€ì„ ì‘ì„±í•©ë‹ˆë‹¤.\n",
    "#    ì´ ëª©ë¡ì€ ëª¨ë¸ì˜ ë‹µë³€ê³¼ ë¹„êµë  \"ì •ë‹µì§€\" ì—­í• ì„ í•©ë‹ˆë‹¤.\n",
    "list_references = [\n",
    "    \"ì •í™•íˆ ì•Œ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. íŒë§¤ìì—ê²Œ ê³ ê¸°ì˜ ì›ì‚°ì§€ ë° í’ˆì¢…(ì˜ˆ: í•œìš°, ìˆ˜ì…ìœ¡ ë“±)ì„ í™•ì¸í•˜ì„¸ìš”. ëŒ€ë¶€ë¶„ í¬ì¥ì§€ì— í‘œì‹œë¼ ìˆìŠµë‹ˆë‹¤.\",\n",
    "    \"ë¦¬ì²˜ë“œ ë‹‰ìŠ¨ì€ 1953ë…„ë¶€í„° 1961ë…„ê¹Œì§€ ì œ43ëŒ€ ë¯¸êµ­ ë¶€í†µë ¹ì´ì—ˆìŠµë‹ˆë‹¤.\",\n",
    "    \"ë¯¸êµ­ ì¼ë¦¬ë…¸ì´ì£¼ ì‹œì¹´ê³  ì‹œ ë¶ì„œìª½, ì‹œ ì™¸ê³½ì˜ ì¿¡ ì¹´ìš´í‹°ì— ìœ„ì¹˜í•´ ìˆìŠµë‹ˆë‹¤.\",\n",
    "    \"ì‚¬ìš©ìì˜ ì§€ì—­ ì •ë³´ê°€ ì—†ìœ¼ë¯€ë¡œ ì •í™•í•œ ìˆ˜ì¹˜ë¥¼ ì•Œ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í¬í„¸ ì‚¬ì´íŠ¸ë‚˜ í™˜ê²½ë¶€ â€˜ì—ì–´ì½”ë¦¬ì•„â€™ì—ì„œ ì‹¤ì‹œê°„ í™•ì¸ ê°€ëŠ¥í•©ë‹ˆë‹¤.\"\n",
    "]\n",
    "\n",
    "# ìƒì„±ëœ ì‘ë‹µê³¼ ì°¸ì¡° í…ìŠ¤íŠ¸ì˜ ê°œìˆ˜ê°€ ê°™ì€ì§€ í™•ì¸\n",
    "if len(list_generated_responses) != len(list_references):\n",
    "    print(f\"âš ï¸ ê²½ê³ : ìƒì„±ëœ ì‘ë‹µì˜ ìˆ˜({len(list_generated_responses)})ì™€ ì°¸ì¡° í…ìŠ¤íŠ¸ì˜ ìˆ˜({len(list_references)})ê°€ ë‹¤ë¦…ë‹ˆë‹¤.\")\n",
    "    print(\"    ROUGE ìŠ¤ì½”ì–´ ê³„ì‚°ì„ ìœ„í•´ ì°¸ì¡° í…ìŠ¤íŠ¸ ëª©ë¡ì„ í™•ì¸í•˜ê³  ì¡°ì •í•´ì£¼ì„¸ìš”.\")\n",
    "else:\n",
    "    # 3. ì´ì „ì— ì •ì˜í•œ ROUGE ìŠ¤ì½”ì–´ í•¨ìˆ˜ í˜¸ì¶œ\n",
    "    #    (calculate_and_print_rouge_scores í•¨ìˆ˜ê°€ ì´ì „ì— ë…¸íŠ¸ë¶ì— ì •ì˜ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤)\n",
    "    print(\"\\nâœ¨ ROUGE ìŠ¤ì½”ì–´ í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ì—¬ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...\")\n",
    "    \n",
    "    # calculate_and_print_rouge_scores í•¨ìˆ˜ê°€ ì •ì˜ë˜ì–´ ìˆë‹¤ê³  ê°€ì •í•˜ê³  í˜¸ì¶œ\n",
    "    # from rouge_score import rouge_scorer # í•¨ìˆ˜ ë‚´ì— ìˆìœ¼ë¯€ë¡œ ì—¬ê¸°ì„œ ë‹¤ì‹œ ì„í¬íŠ¸í•  í•„ìš”ëŠ” ì—†ìŒ\n",
    "    \n",
    "    average_rouge_scores = calculate_and_print_rouge_scores(\n",
    "        generated_texts=list_generated_responses,\n",
    "        reference_texts=list_references,\n",
    "        use_stemmer=False,  # í•œêµ­ì–´ì˜ ê²½ìš° Trueë¡œ ì„¤ì •í•˜ê³  ì ì ˆí•œ í™˜ê²½ êµ¬ì„± í•„ìš”\n",
    "        print_individual_scores=True # ê° ìƒ˜í”Œë³„ ì ìˆ˜ ì¶œë ¥ ì—¬ë¶€\n",
    "    )\n",
    "\n",
    "    if average_rouge_scores:\n",
    "        print(\"\\nâœ… ROUGE ì ìˆ˜ ê³„ì‚°ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!\")\n",
    "        # print(\"\\në°˜í™˜ëœ í‰ê·  ì ìˆ˜ ë”•ì…”ë„ˆë¦¬ (F1 ê¸°ì¤€):\", average_rouge_scores) # ìƒì„¸ ê²°ê³¼ í™•ì¸ìš©\n",
    "    else:\n",
    "        print(\"âŒ ROUGE ì ìˆ˜ ê³„ì‚°ì— ì‹¤íŒ¨í–ˆê±°ë‚˜ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b953d1e6",
   "metadata": {},
   "source": [
    "## ROUGE ìŠ¤ì½”ì–´ì™€ ë¹„êµ í‰ê°€\n",
    "ê° ì§ˆë¬¸ë“¤ì— ëŒ€í•´ ChatGPT 4oê°€ ì˜ˆì¸¡í•œ ë¬¸ìì—´ê³¼ ë¹„êµí•˜ì—¬ ROUGE í‰ê°€ë¥¼ ì§„í–‰í•˜ì˜€ë‹¤.\n",
    "\n",
    "ChatGPT 4oê°€ ìƒì„±í•´ ë‚¸ ë‹µë³€ì€ ë¶€í†µë ¹ ì •ë³´ê°€ ì•½ê°„ í‹€ë¦¬ë‹¤ëŠ” ì ì„ ì œì™¸í•˜ë©´ ì˜¬ë°”ë¥¸ ëŒ€ë‹µì´ì˜€ë‹¤.\n",
    "\n",
    "ROUGE ì ìˆ˜ëŠ” ëª¨ë‘ 0ìœ¼ë¡œ, ì ìˆ˜ ê°’ì„ ì „í˜€ í™•ì¸í•  ìˆ˜ ì—†ì—ˆë‹¤.\n",
    "\n",
    "ROUGE ì ìˆ˜ë¡œëŠ” ë‘ ëŒ€ë‹µ ê°„ì˜ ì˜ë¯¸ì  ìœ ì‚¬ë„ë¥¼ ì¸¡ì •í•˜ì§€ ëª»í•˜ê³ , ë‹¨ìˆœíˆ ê°™ì€ n-gramì´ ì¡´ì¬í•˜ëŠ”ì§€ë¥¼ ìœ„ì£¼ë¡œ í™•ì¸í•˜ê¸° ë•Œë¬¸ì´ë‹¤.\n",
    "\n",
    "ë”°ë¼ì„œ BERTScoreë¥¼ ì´ìš©í•˜ì—¬ ë‹¤ì‹œ í‰ê°€í•˜ê² ë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d65cae2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bert-score\n",
      "  Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
      "     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 61 kB 4.2 MB/s             \n",
      "\u001b[?25hRequirement already satisfied: transformers>=3.0.0 in /opt/conda/lib/python3.9/site-packages (from bert-score) (4.28.0)\n",
      "Requirement already satisfied: pandas>=1.0.1 in /opt/conda/lib/python3.9/site-packages (from bert-score) (1.3.3)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.9/site-packages (from bert-score) (3.4.3)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.9/site-packages (from bert-score) (21.3)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from bert-score) (2.26.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from bert-score) (1.21.4)\n",
      "Requirement already satisfied: torch>=1.0.0 in /opt/conda/lib/python3.9/site-packages (from bert-score) (1.12.1)\n",
      "Requirement already satisfied: tqdm>=4.31.1 in /opt/conda/lib/python3.9/site-packages (from bert-score) (4.62.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging>=20.9->bert-score) (3.0.6)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.9/site-packages (from pandas>=1.0.1->bert-score) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.9/site-packages (from pandas>=1.0.1->bert-score) (2.8.2)\n",
      "Requirement already satisfied: typing_extensions in /opt/conda/lib/python3.9/site-packages (from torch>=1.0.0->bert-score) (4.7.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from transformers>=3.0.0->bert-score) (6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/conda/lib/python3.9/site-packages (from transformers>=3.0.0->bert-score) (0.15.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from transformers>=3.0.0->bert-score) (3.12.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.9/site-packages (from transformers>=3.0.0->bert-score) (2021.11.10)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.9/site-packages (from transformers>=3.0.0->bert-score) (0.13.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib->bert-score) (1.3.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.9/site-packages (from matplotlib->bert-score) (8.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.9/site-packages (from matplotlib->bert-score) (0.11.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->bert-score) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->bert-score) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->bert-score) (2023.5.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests->bert-score) (2.0.8)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers>=3.0.0->bert-score) (2021.11.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas>=1.0.1->bert-score) (1.16.0)\n",
      "Installing collected packages: bert-score\n",
      "Successfully installed bert-score-0.3.13\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install bert-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "24c40536",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "563b1bc5fc574dccbb4a81b4281d62b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67db75f144eb488d81fc3bb7f5cdf00d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.09 seconds, 44.25 sentences/sec\n",
      "\n",
      "ğŸ”¹ ì˜ˆì œ 1\n",
      "  ğŸ“ ì§ˆë¬¸: ë¶ˆê³ ê¸°ìš© ê³ ê¸° í•œìš°ì—ìš”?\n",
      "  ğŸ¤– ì‘ë‹µ: 'ì €ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì´ê¸° ë•Œë¬¸ì— ê³ ê¸°ë¥¼ ë¨¹ì„ ìˆ˜ëŠ” ì—†ìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì¼ë°˜ì ìœ¼ë¡œ ë¶ˆê³ ê¸°ëŠ” ê³ ê¸°ì™€ í•¨ê»˜ ë¨¹ëŠ” ê²½ìš°ê°€ ë§ìœ¼ë‹ˆê¹Œ ë§›ìˆê²Œ ë“œì„¸ìš”!\n",
      "  âœ… ì •ë‹µ: ì •í™•íˆ ì•Œ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. íŒë§¤ìì—ê²Œ ê³ ê¸°ì˜ ì›ì‚°ì§€ ë° í’ˆì¢…(ì˜ˆ: í•œìš°, ìˆ˜ì…ìœ¡ ë“±)ì„ í™•ì¸í•˜ì„¸ìš”. ëŒ€ë¶€ë¶„ í¬ì¥ì§€ì— í‘œì‹œë¼ ìˆìŠµë‹ˆë‹¤.\n",
      "  ğŸ“Œ BERTScore - Precision: 0.5595, Recall: 0.5766, F1: 0.5679\n",
      "\n",
      "ğŸ”¹ ì˜ˆì œ 2\n",
      "  ğŸ“ ì§ˆë¬¸: ë¦¬ì²˜ë“œ ë‹‰ìŠ¨ì´ 43ëŒ€ ë¶€í†µë ¹ì§ì„ ìˆ˜í–‰í•œ ë…„ë„ëŠ”?\n",
      "  ğŸ¤– ì‘ë‹µ: 'ë¦¬ì²˜ë“œ ë‹‰ìŠ¨ì€ 41ëŒ€ ë¶€í†µë ¹ì§ì„ ìˆ˜í–‰í–ˆìŠµë‹ˆë‹¤.\n",
      "  âœ… ì •ë‹µ: ë¦¬ì²˜ë“œ ë‹‰ìŠ¨ì€ 1953ë…„ë¶€í„° 1961ë…„ê¹Œì§€ ì œ43ëŒ€ ë¯¸êµ­ ë¶€í†µë ¹ì´ì—ˆìŠµë‹ˆë‹¤.\n",
      "  ğŸ“Œ BERTScore - Precision: 0.7447, Recall: 0.6455, F1: 0.6916\n",
      "\n",
      "ğŸ”¹ ì˜ˆì œ 3\n",
      "  ğŸ“ ì§ˆë¬¸: ì‹œì¹´ê³  ì˜¤í—¤ì–´ êµ­ì œê³µí•­ì€ ì–´ë””ì— ìˆì–´?\n",
      "  ğŸ¤– ì‘ë‹µ: 'ì‹œì¹´ê³  ì˜¤ í—¤ì–´ êµ­ì œê³µí•­ì€ ë¯¸êµ­ ìº˜ë¦¬í¬ë‹ˆì•„ì£¼ ìƒŒí”„ë€ì‹œìŠ¤ì½”ì— ìœ„ì¹˜í•´ ìˆìŠµë‹ˆë‹¤.\n",
      "  âœ… ì •ë‹µ: ë¯¸êµ­ ì¼ë¦¬ë…¸ì´ì£¼ ì‹œì¹´ê³  ì‹œ ë¶ì„œìª½, ì‹œ ì™¸ê³½ì˜ ì¿¡ ì¹´ìš´í‹°ì— ìœ„ì¹˜í•´ ìˆìŠµë‹ˆë‹¤.\n",
      "  ğŸ“Œ BERTScore - Precision: 0.7028, Recall: 0.6267, F1: 0.6626\n",
      "\n",
      "ğŸ”¹ ì˜ˆì œ 4\n",
      "  ğŸ“ ì§ˆë¬¸: ì˜¤ëŠ˜ ë¯¸ì„¸ë¨¼ì§€ ì–´ë•Œ?\n",
      "  ğŸ¤– ì‘ë‹µ: 'ì €ëŠ” ì¸ê³µì§€ëŠ¥ ì±—ë´‡ì´ê¸° ë•Œë¬¸ì— ë¯¸ì„¸ë¨¼ì§€ ì •ë³´ë¥¼ ì•Œ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì¼ë°˜ì ìœ¼ë¡œ ë¯¸ì„¸ë¨¼ì§€ëŠ” í˜¸í¡ê¸° ê±´ê°•ì— ë§¤ìš° ì¤‘ìš”í•œ ì—­í• ì„ í•˜ê¸° ë•Œë¬¸ì—, ì™¸ì¶œ ì‹œ ë°˜ë“œì‹œ ë§ˆìŠ¤í¬ë¥¼ ì°©ìš©í•˜ê³  ì‹¤ë‚´ì—ì„œ ëŒ€ê¸°ì˜¤ì—¼ì„ ì¤„ì´ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.\n",
      "  âœ… ì •ë‹µ: ì‚¬ìš©ìì˜ ì§€ì—­ ì •ë³´ê°€ ì—†ìœ¼ë¯€ë¡œ ì •í™•í•œ ìˆ˜ì¹˜ë¥¼ ì•Œ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í¬í„¸ ì‚¬ì´íŠ¸ë‚˜ í™˜ê²½ë¶€ â€˜ì—ì–´ì½”ë¦¬ì•„â€™ì—ì„œ ì‹¤ì‹œê°„ í™•ì¸ ê°€ëŠ¥í•©ë‹ˆë‹¤.\n",
      "  ğŸ“Œ BERTScore - Precision: 0.5723, Recall: 0.6261, F1: 0.5980\n",
      "\n",
      "ğŸ“Š BERTScore F1 í‰ê· : 0.6300\n"
     ]
    }
   ],
   "source": [
    "from bert_score import score\n",
    "\n",
    "# BERTScore ê³„ì‚° (ê¸°ë³¸ ëª¨ë¸: roberta-large, í•œêµ­ì–´ëŠ” ì•„ë˜ ì°¸ê³ )\n",
    "P, R, F1 = score(list_generated_responses, list_references, model_type=\"klue/roberta-base\", num_layers=12, verbose=True)\n",
    "\n",
    "# ê° ë¬¸ì¥ë³„ ì ìˆ˜ ì¶œë ¥\n",
    "for i, (p, r, f1) in enumerate(zip(P, R, F1)):\n",
    "    print(f\"\\nğŸ”¹ ì˜ˆì œ {i+1}\")\n",
    "    print(f\"  ğŸ“ ì§ˆë¬¸: {list_prompt_original[i]}\")\n",
    "    print(f\"  ğŸ¤– ì‘ë‹µ: {list_generated_responses[i]}\")\n",
    "    print(f\"  âœ… ì •ë‹µ: {list_references[i]}\")\n",
    "    print(f\"  ğŸ“Œ BERTScore - Precision: {p:.4f}, Recall: {r:.4f}, F1: {f1:.4f}\")\n",
    "    \n",
    "# í‰ê·  F1 ì¶œë ¥\n",
    "print(f\"\\nğŸ“Š BERTScore F1 í‰ê· : {F1.mean().item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac18e1c7",
   "metadata": {},
   "source": [
    "## BERTScore í‰ê°€\n",
    "ê° ì§ˆë¬¸ë“¤ì— ëŒ€í•´ ChatGPT 4oê°€ ì˜ˆì¸¡í•œ ë¬¸ìì—´ê³¼ ë¹„êµí•˜ì—¬ BERTScore í‰ê°€ë¥¼ ì§„í–‰í•˜ì˜€ë‹¤.\n",
    "\n",
    "í™•ì‹¤íˆ ROUGE ì ìˆ˜ì— ë¹„í•´ ì˜ë¯¸ìˆëŠ” ê°’ì„ í™•ì¸í•  ìˆ˜ ìˆì—ˆë‹¤.\n",
    "\n",
    "ê° ì ìˆ˜ê°€ ë†’ì€ ì ìˆ˜ì¸ì§€ëŠ” ì•„ë˜ ëª¨ë¸ë“¤ê³¼ ë¹„êµí•˜ì—¬ ë¶„ì„í•˜ê² ë‹¤.\n",
    "\n",
    "íŠ¹ì§•ì ìœ¼ë¡œ 2, 3ë²ˆ ì§ˆë¬¸ì— ëŒ€í•œ ëŒ€ë‹µì´ ì ìˆ˜ê°€ ì¢‹ê²Œ ë‚˜ì™”ëŠ”ë°, 1, 4ë²ˆì˜ ê²½ìš° ì •í™•í•œ ì •ë³´ë¥¼ ì „ë‹¬í•˜ì§€ ëª»í•˜ë‹¤ ë³´ë‹ˆ ë‚´ìš©ì´ ë‹¤ë¥¸ ë¶€ë¶„ì´ ë§ì•„ì¡Œê³ , 2, 3ë²ˆì˜ ê²½ìš° ë§ë“  í‹€ë¦¬ë“  ë¹„ìŠ·í•œ ì •ë³´ë¥¼ ì „ë‹¬í•˜ë ¤ëŠ” ì‹œë„ê°€ ìˆì—ˆê¸°ì— ì ìˆ˜ê°€ ë†’ê²Œ ë‚˜ì˜¤ì§€ ì•Šì•˜ë‚˜ í•˜ëŠ” ìƒê°ì´ ë“¤ì—ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e94f2e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ebb8f6",
   "metadata": {},
   "source": [
    "## RM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "db8802ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# colossalai_ChatGPT_230319 ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ ë£¨íŠ¸ ê²½ë¡œë¥¼ sys.pathì— ì¶”ê°€\n",
    "library_root_path = '/aiffel/KoChatGPT/colossalai_ChatGPT_230319'\n",
    "\n",
    "if library_root_path not in sys.path:\n",
    "    sys.path.insert(0, library_root_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "60349b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import Optional\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from chatgpt.dataset import RewardDataset\n",
    "from chatgpt.models.base import RewardModel\n",
    "from chatgpt.trainer import RewardModelTrainer\n",
    "from chatgpt.trainer.strategies import NaiveStrategy\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel, AutoConfig\n",
    "from transformers.models.gpt2.configuration_gpt2 import GPT2Config\n",
    "from transformers.models.gpt2.modeling_gpt2 import GPT2Model\n",
    "import loralib as lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b4119fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTRM_custom(RewardModel):\n",
    "\n",
    "    def __init__(self,\n",
    "                 pretrained: Optional[str] = None,\n",
    "                 config: Optional[GPT2Config] = None,\n",
    "                 checkpoint: bool = False,\n",
    "                 lora_rank: int = 0,\n",
    "                 lora_train_bias: str = 'none',\n",
    "                 tokenizer=None) -> None:\n",
    "        if pretrained is not None:\n",
    "            model = GPT2Model.from_pretrained(pretrained)\n",
    "            model.resize_token_embeddings(len(tokenizer))\n",
    "        elif config is not None:\n",
    "            model = GPT2Model(config)\n",
    "        else:\n",
    "            model = GPT2Model(GPT2Config())\n",
    "        if checkpoint:\n",
    "            model.gradient_checkpointing_enable()\n",
    "\n",
    "        value_head = nn.Linear(model.config.n_embd, 1)\n",
    "        super().__init__(model, value_head, lora_rank, lora_train_bias)\n",
    "\n",
    "        if pretrained is not None:\n",
    "            self.model = model\n",
    "            self.pretrained = pretrained\n",
    "\n",
    "\n",
    "    def save_pretrained(self, dir):\n",
    "        if self.pretrained is not None:\n",
    "            self.model.save_pretrained(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "51fd5efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at skt/kogpt2-base-v2 were not used when initializing GPT2Model: ['lm_head.weight']\n",
      "- This IS expected if you are initializing GPT2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPT2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained('skt/kogpt2-base-v2')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    'skt/kogpt2-base-v2', bos_token='</s>', eos_token='</s>', unk_token='</s>', pad_token='</s>',\n",
    "    padding_side=\"right\",\n",
    "    model_max_length=512,\n",
    ")\n",
    "\n",
    "with NaiveStrategy().model_init_context():\n",
    "        model = GPTRM_custom(pretrained='skt/kogpt2-base-v2', lora_rank=0, tokenizer=tokenizer).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fe83fc49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before data num: 10220\n",
      "after  data num: 30660\n",
      "data example: \n",
      "{'prompt': 'ì• í”Œì€ ë¦¬ì‚¬ë¥¼ ì–´ë–»ê²Œ ì²˜ë¦¬í–ˆì–´', 'chosen': 'ì• í”Œì´ ëˆ„êµ¬ì¸ì§€ ëª…í™•íˆ ì•Œ ìˆ˜ ì—†ì–´ì„œ, ë¦¬ì‚¬ê°€ ëˆ„êµ¬ì¸ì§€ì™€ ì–´ë–¤ ìƒí™©ì—ì„œ ì²˜ë¦¬ë˜ì—ˆëŠ”ì§€ì— ëŒ€í•œ ì¶”ê°€ì ì¸ ì •ë³´ê°€ í•„ìš”í•©ë‹ˆë‹¤. ë”°ë¼ì„œ, ë³´ë‹¤ ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.', 'rejected': 'ì• í”Œì€ ë¦¬ì‚¬ë¥¼ ìœ„í•´ ê³ ê° ì„œë¹„ìŠ¤ ë¶€ì„œì—ì„œ ê³ ê° ë‹¤ì–‘í•œ ì»´í“¨í„° ê´€ë ¨ ë¬¸ì œì— ëŒ€í•´ ì‘ë‹µí•˜ëŠ” ë° í•„ìš”í•œ ëª¨ë“  ì§€ì›ì„ ì œê³µí–ˆìŠµë‹ˆë‹¤. ì‚¬ìš©ìê°€ í•˜ë“œì›¨ì–´ ë¬¸ì œë¥¼ ê²½í—˜í•  ë•Œ, ì „ë¬¸ê°€ë“¤ì€ í•„ìš”í•œ ìˆ˜ë¦¬(ìˆ˜ë¦¬, ì¶”ê°€ ë¶€í’ˆ ì œê³µ, ì†Œí”„íŠ¸ì›¨ì–´ ì—…ê·¸ë ˆì´ë“œ ë“±)ì„ ì œê³µí•´ ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë˜í•œ, ì‚¬ìš©ìê°€ ì‚¬ìš© ë°©ë²• ë¬¸ì œë‚˜ ê¸°íƒ€ ë¬¸ì œë¥¼ ê²½í—˜í•  ë•Œ, ëŒ€í™” ìƒëŒ€ë¡œ ì‚¬ìš©ìë¥¼ ì§€ì›í•  ìˆ˜ ìˆëŠ” ì „ë¬¸ ê³ ê° ì„œë¹„ìŠ¤ ì§ì›ë“¤ì´ ì‚¬ìš©ìì—ê²Œ ìƒë‹´í•˜ê³  ë„ì›€ì„ ì£¼ëŠ” ë° ë„ì›€ì´ ë  ìˆ˜ ìˆëŠ” ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ë˜í•œ, ì¸í„°ë„·ì—ì„œ ì œê³µë˜ëŠ” ì •ë³´ë¥¼ í†µí•´ ë¬¸ì œë¥¼ í•´ê²°í•˜ê±°ë‚˜ ê³ ê° ì„œë¹„ìŠ¤ ì›¹ ì‚¬ì´íŠ¸ë¥¼ í†µí•´ ìì‹ ì˜ ë¬¸ì œë¥¼ ì§„ë‹¨í•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” ë“± ë‹¤ì–‘í•œ ë°©ë²•ìœ¼ë¡œ ë¦¬ì‚¬ë¥¼ ì²˜ë¦¬í•´ ì™”ìŠµë‹ˆë‹¤.'}\n"
     ]
    }
   ],
   "source": [
    "with open('/aiffel/KoChatGPT/data_kochatgpt/kochatgpt_2_RM.jsonl', \"r\", encoding='utf-8-sig') as json_file:\n",
    "    list_data_dict = json.load(json_file)\n",
    "\n",
    "total_data_ranking2chosen = []\n",
    "for tmp in list_data_dict:\n",
    "    one_data_ranking2chosen = []\n",
    "\n",
    "    data = {}\n",
    "    data['prompt'] = tmp['prompt']\n",
    "    if tmp['ranking'][0] < tmp['ranking'][1]:\n",
    "        data['chosen'] = tmp['completion_0']\n",
    "        data['rejected'] = tmp['completion_1']\n",
    "    else:\n",
    "        data['chosen'] = tmp['completion_1']\n",
    "        data['rejected'] = tmp['completion_0']\n",
    "    one_data_ranking2chosen.append(data)\n",
    "\n",
    "    data = {}\n",
    "    data['prompt'] = tmp['prompt']\n",
    "    if tmp['ranking'][0] < tmp['ranking'][2]:\n",
    "        data['chosen'] = tmp['completion_0']\n",
    "        data['rejected'] = tmp['completion_2']\n",
    "    else:\n",
    "        data['chosen'] = tmp['completion_2']\n",
    "        data['rejected'] = tmp['completion_0']\n",
    "    one_data_ranking2chosen.append(data)\n",
    "\n",
    "    data = {}\n",
    "    data['prompt'] = tmp['prompt']\n",
    "    if tmp['ranking'][1] < tmp['ranking'][2]:\n",
    "        data['chosen'] = tmp['completion_1']\n",
    "        data['rejected'] = tmp['completion_2']\n",
    "    else:\n",
    "        data['chosen'] = tmp['completion_2']\n",
    "        data['rejected'] = tmp['completion_1']\n",
    "    one_data_ranking2chosen.append(data)\n",
    "\n",
    "\n",
    "\n",
    "    total_data_ranking2chosen.extend(one_data_ranking2chosen)\n",
    "\n",
    "print('before data num: %d'%(len(list_data_dict)))\n",
    "print('after  data num: %d'%(len(total_data_ranking2chosen)))\n",
    "print('data example: \\n%s'%total_data_ranking2chosen[45])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ffc04c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': 'ìœ ì•„ì¸ì´ ë¥˜ìŠ¹ì™„ ê°ë…ì„ ë§Œë‚˜ ì˜í™” ë² í…Œë‘ì˜ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ë°›ì•˜ë˜ ê³³ì€?', 'chosen': 'ìœ ì•„ì¸ì´ ë¥˜ìŠ¹ì™„ ê°ë…ì„ ë§Œë‚˜ ì˜í™” ë² í…Œë‘ì˜ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ë°›ì•˜ë˜ ê³³ì€ ë¥˜ìŠ¹ì™„ì˜ ì‚¬ë¬´ì‹¤ì…ë‹ˆë‹¤.', 'rejected': 'ëŒ€êµ¬ ì˜í™”ì‚¬ì˜¥'}\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(230319)\n",
    "random.shuffle(total_data_ranking2chosen)\n",
    "print(total_data_ranking2chosen[45])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a9719c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 1407.22it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:00<00:00, 1405.81it/s]\n"
     ]
    }
   ],
   "source": [
    "train_data = total_data_ranking2chosen[:1000] \n",
    "eval_data = total_data_ranking2chosen[1000:1200]\n",
    "\n",
    "print(len(train_data))\n",
    "print(len(eval_data))\n",
    "\n",
    "train_dataset = RewardDataset(train_data, tokenizer, 512)\n",
    "eval_dataset = RewardDataset(eval_data, tokenizer, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ffb2524e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = RewardModelTrainer(model=model,\n",
    "                             strategy=NaiveStrategy(),\n",
    "                             optim=Adam(model.parameters(), lr=5e-5),\n",
    "                             train_dataset=train_dataset,\n",
    "                             eval_dataset=eval_dataset,\n",
    "                             batch_size=4,\n",
    "                             max_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fa4ea55f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Train step of epoch 0:   0%|          | 0/250 [00:00<?, ?it/s]\u001b[A\n",
      "Train step of epoch 0:   0%|          | 1/250 [00:01<04:39,  1.12s/it]\u001b[A\n",
      "Train step of epoch 0:   0%|          | 1/250 [00:01<04:39,  1.12s/it, loss=0.721]\u001b[A\n",
      "Train step of epoch 0:   1%|          | 2/250 [00:02<04:14,  1.03s/it, loss=0.721]\u001b[A\n",
      "Train step of epoch 0:   1%|          | 2/250 [00:02<04:14,  1.03s/it, loss=0.403]\u001b[A\n",
      "Train step of epoch 0:   1%|          | 3/250 [00:03<04:07,  1.00s/it, loss=0.403]\u001b[A\n",
      "Train step of epoch 0:   1%|          | 3/250 [00:03<04:07,  1.00s/it, loss=0.464]\u001b[A\n",
      "Train step of epoch 0:   2%|â–         | 4/250 [00:04<04:03,  1.01it/s, loss=0.464]\u001b[A\n",
      "Train step of epoch 0:   2%|â–         | 4/250 [00:04<04:03,  1.01it/s, loss=0.55] \u001b[A\n",
      "Train step of epoch 0:   2%|â–         | 5/250 [00:04<04:00,  1.02it/s, loss=0.55]\u001b[A\n",
      "Train step of epoch 0:   2%|â–         | 5/250 [00:05<04:00,  1.02it/s, loss=0.318]\u001b[A\n",
      "Train step of epoch 0:   2%|â–         | 6/250 [00:05<03:59,  1.02it/s, loss=0.318]\u001b[A\n",
      "Train step of epoch 0:   2%|â–         | 6/250 [00:06<03:59,  1.02it/s, loss=1.06] \u001b[A\n",
      "Train step of epoch 0:   3%|â–         | 7/250 [00:06<03:58,  1.02it/s, loss=1.06]\u001b[A\n",
      "Train step of epoch 0:   3%|â–         | 7/250 [00:06<03:58,  1.02it/s, loss=0.534]\u001b[A\n",
      "Train step of epoch 0:   3%|â–         | 8/250 [00:07<03:58,  1.02it/s, loss=0.534]\u001b[A\n",
      "Train step of epoch 0:   3%|â–         | 8/250 [00:07<03:58,  1.02it/s, loss=2.29] \u001b[A\n",
      "Train step of epoch 0:   4%|â–         | 9/250 [00:08<03:57,  1.01it/s, loss=2.29]\u001b[A\n",
      "Train step of epoch 0:   4%|â–         | 9/250 [00:08<03:57,  1.01it/s, loss=0.296]\u001b[A\n",
      "Train step of epoch 0:   4%|â–         | 10/250 [00:09<03:57,  1.01it/s, loss=0.296]\u001b[A\n",
      "Train step of epoch 0:   4%|â–         | 10/250 [00:09<03:57,  1.01it/s, loss=0.612]\u001b[A\n",
      "Train step of epoch 0:   4%|â–         | 11/250 [00:10<03:57,  1.01it/s, loss=0.612]\u001b[A\n",
      "Train step of epoch 0:   4%|â–         | 11/250 [00:10<03:57,  1.01it/s, loss=0.406]\u001b[A\n",
      "Train step of epoch 0:   5%|â–         | 12/250 [00:11<03:57,  1.00it/s, loss=0.406]\u001b[A\n",
      "Train step of epoch 0:   5%|â–         | 12/250 [00:11<03:57,  1.00it/s, loss=0.553]\u001b[A\n",
      "Train step of epoch 0:   5%|â–Œ         | 13/250 [00:12<03:57,  1.00s/it, loss=0.553]\u001b[A\n",
      "Train step of epoch 0:   5%|â–Œ         | 13/250 [00:12<03:57,  1.00s/it, loss=0.245]\u001b[A\n",
      "Train step of epoch 0:   6%|â–Œ         | 14/250 [00:13<03:56,  1.00s/it, loss=0.245]\u001b[A\n",
      "Train step of epoch 0:   6%|â–Œ         | 14/250 [00:13<03:56,  1.00s/it, loss=0.827]\u001b[A\n",
      "Train step of epoch 0:   6%|â–Œ         | 15/250 [00:14<03:56,  1.01s/it, loss=0.827]\u001b[A\n",
      "Train step of epoch 0:   6%|â–Œ         | 15/250 [00:15<03:56,  1.01s/it, loss=0.623]\u001b[A\n",
      "Train step of epoch 0:   6%|â–‹         | 16/250 [00:15<03:56,  1.01s/it, loss=0.623]\u001b[A\n",
      "Train step of epoch 0:   6%|â–‹         | 16/250 [00:16<03:56,  1.01s/it, loss=1.23] \u001b[A\n",
      "Train step of epoch 0:   7%|â–‹         | 17/250 [00:17<03:55,  1.01s/it, loss=1.23]\u001b[A\n",
      "Train step of epoch 0:   7%|â–‹         | 17/250 [00:17<03:55,  1.01s/it, loss=0.673]\u001b[A\n",
      "Train step of epoch 0:   7%|â–‹         | 18/250 [00:18<03:53,  1.01s/it, loss=0.673]\u001b[A\n",
      "Train step of epoch 0:   7%|â–‹         | 18/250 [00:18<03:53,  1.01s/it, loss=0.488]\u001b[A\n",
      "Train step of epoch 0:   8%|â–Š         | 19/250 [00:19<03:52,  1.01s/it, loss=0.488]\u001b[A\n",
      "Train step of epoch 0:   8%|â–Š         | 19/250 [00:19<03:52,  1.01s/it, loss=0.726]\u001b[A\n",
      "Train step of epoch 0:   8%|â–Š         | 20/250 [00:20<03:50,  1.00s/it, loss=0.726]\u001b[A\n",
      "Train step of epoch 0:   8%|â–Š         | 20/250 [00:20<03:50,  1.00s/it, loss=0.609]\u001b[A\n",
      "Train step of epoch 0:   8%|â–Š         | 21/250 [00:20<03:48,  1.00it/s, loss=0.609]\u001b[A\n",
      "Train step of epoch 0:   8%|â–Š         | 21/250 [00:21<03:48,  1.00it/s, loss=0.584]\u001b[A\n",
      "Train step of epoch 0:   9%|â–‰         | 22/250 [00:21<03:46,  1.01it/s, loss=0.584]\u001b[A\n",
      "Train step of epoch 0:   9%|â–‰         | 22/250 [00:22<03:46,  1.01it/s, loss=0.695]\u001b[A\n",
      "Train step of epoch 0:   9%|â–‰         | 23/250 [00:22<03:44,  1.01it/s, loss=0.695]\u001b[A\n",
      "Train step of epoch 0:   9%|â–‰         | 23/250 [00:22<03:44,  1.01it/s, loss=0.879]\u001b[A\n",
      "Train step of epoch 0:  10%|â–‰         | 24/250 [00:23<03:42,  1.02it/s, loss=0.879]\u001b[A\n",
      "Train step of epoch 0:  10%|â–‰         | 24/250 [00:23<03:42,  1.02it/s, loss=0.656]\u001b[A\n",
      "Train step of epoch 0:  10%|â–ˆ         | 25/250 [00:24<03:40,  1.02it/s, loss=0.656]\u001b[A\n",
      "Train step of epoch 0:  10%|â–ˆ         | 25/250 [00:24<03:40,  1.02it/s, loss=0.477]\u001b[A\n",
      "Train step of epoch 0:  10%|â–ˆ         | 26/250 [00:25<03:38,  1.03it/s, loss=0.477]\u001b[A\n",
      "Train step of epoch 0:  10%|â–ˆ         | 26/250 [00:25<03:38,  1.03it/s, loss=0.654]\u001b[A\n",
      "Train step of epoch 0:  11%|â–ˆ         | 27/250 [00:26<03:36,  1.03it/s, loss=0.654]\u001b[A\n",
      "Train step of epoch 0:  11%|â–ˆ         | 27/250 [00:26<03:36,  1.03it/s, loss=0.802]\u001b[A\n",
      "Train step of epoch 0:  11%|â–ˆ         | 28/250 [00:27<03:34,  1.04it/s, loss=0.802]\u001b[A\n",
      "Train step of epoch 0:  11%|â–ˆ         | 28/250 [00:27<03:34,  1.04it/s, loss=0.656]\u001b[A\n",
      "Train step of epoch 0:  12%|â–ˆâ–        | 29/250 [00:28<03:32,  1.04it/s, loss=0.656]\u001b[A\n",
      "Train step of epoch 0:  12%|â–ˆâ–        | 29/250 [00:28<03:32,  1.04it/s, loss=0.573]\u001b[A\n",
      "Train step of epoch 0:  12%|â–ˆâ–        | 30/250 [00:29<03:30,  1.04it/s, loss=0.573]\u001b[A\n",
      "Train step of epoch 0:  12%|â–ˆâ–        | 30/250 [00:29<03:30,  1.04it/s, loss=0.552]\u001b[A\n",
      "Train step of epoch 0:  12%|â–ˆâ–        | 31/250 [00:30<03:28,  1.05it/s, loss=0.552]\u001b[A\n",
      "Train step of epoch 0:  12%|â–ˆâ–        | 31/250 [00:30<03:28,  1.05it/s, loss=0.491]\u001b[A\n",
      "Train step of epoch 0:  13%|â–ˆâ–        | 32/250 [00:31<03:27,  1.05it/s, loss=0.491]\u001b[A\n",
      "Train step of epoch 0:  13%|â–ˆâ–        | 32/250 [00:31<03:27,  1.05it/s, loss=0.753]\u001b[A\n",
      "Train step of epoch 0:  13%|â–ˆâ–        | 33/250 [00:32<03:25,  1.06it/s, loss=0.753]\u001b[A\n",
      "Train step of epoch 0:  13%|â–ˆâ–        | 33/250 [00:32<03:25,  1.06it/s, loss=0.456]\u001b[A\n",
      "Train step of epoch 0:  14%|â–ˆâ–        | 34/250 [00:33<03:23,  1.06it/s, loss=0.456]\u001b[A\n",
      "Train step of epoch 0:  14%|â–ˆâ–        | 34/250 [00:33<03:23,  1.06it/s, loss=0.456]\u001b[A\n",
      "Train step of epoch 0:  14%|â–ˆâ–        | 35/250 [00:34<03:22,  1.06it/s, loss=0.456]\u001b[A\n",
      "Train step of epoch 0:  14%|â–ˆâ–        | 35/250 [00:34<03:22,  1.06it/s, loss=0.885]\u001b[A\n",
      "Train step of epoch 0:  14%|â–ˆâ–        | 36/250 [00:35<03:20,  1.07it/s, loss=0.885]\u001b[A\n",
      "Train step of epoch 0:  14%|â–ˆâ–        | 36/250 [00:35<03:20,  1.07it/s, loss=1.03] \u001b[A\n",
      "Train step of epoch 0:  15%|â–ˆâ–        | 37/250 [00:36<03:19,  1.07it/s, loss=1.03]\u001b[A\n",
      "Train step of epoch 0:  15%|â–ˆâ–        | 37/250 [00:36<03:19,  1.07it/s, loss=0.799]\u001b[A\n",
      "Train step of epoch 0:  15%|â–ˆâ–Œ        | 38/250 [00:37<03:18,  1.07it/s, loss=0.799]\u001b[A\n",
      "Train step of epoch 0:  15%|â–ˆâ–Œ        | 38/250 [00:37<03:18,  1.07it/s, loss=0.752]\u001b[A\n",
      "Train step of epoch 0:  16%|â–ˆâ–Œ        | 39/250 [00:38<03:16,  1.07it/s, loss=0.752]\u001b[A\n",
      "Train step of epoch 0:  16%|â–ˆâ–Œ        | 39/250 [00:38<03:16,  1.07it/s, loss=0.978]\u001b[A\n",
      "Train step of epoch 0:  16%|â–ˆâ–Œ        | 40/250 [00:39<03:15,  1.07it/s, loss=0.978]\u001b[A\n",
      "Train step of epoch 0:  16%|â–ˆâ–Œ        | 40/250 [00:39<03:15,  1.07it/s, loss=0.532]\u001b[A\n",
      "Train step of epoch 0:  16%|â–ˆâ–‹        | 41/250 [00:39<03:14,  1.08it/s, loss=0.532]\u001b[A\n",
      "Train step of epoch 0:  16%|â–ˆâ–‹        | 41/250 [00:39<03:14,  1.08it/s, loss=0.467]\u001b[A\n",
      "Train step of epoch 0:  17%|â–ˆâ–‹        | 42/250 [00:40<03:13,  1.08it/s, loss=0.467]\u001b[A\n",
      "Train step of epoch 0:  17%|â–ˆâ–‹        | 42/250 [00:40<03:13,  1.08it/s, loss=0.637]\u001b[A\n",
      "Train step of epoch 0:  17%|â–ˆâ–‹        | 43/250 [00:41<03:11,  1.08it/s, loss=0.637]\u001b[A\n",
      "Train step of epoch 0:  17%|â–ˆâ–‹        | 43/250 [00:41<03:11,  1.08it/s, loss=0.727]\u001b[A\n",
      "Train step of epoch 0:  18%|â–ˆâ–Š        | 44/250 [00:42<03:10,  1.08it/s, loss=0.727]\u001b[A\n",
      "Train step of epoch 0:  18%|â–ˆâ–Š        | 44/250 [00:42<03:10,  1.08it/s, loss=0.808]\u001b[A\n",
      "Train step of epoch 0:  18%|â–ˆâ–Š        | 45/250 [00:43<03:09,  1.08it/s, loss=0.808]\u001b[A\n",
      "Train step of epoch 0:  18%|â–ˆâ–Š        | 45/250 [00:43<03:09,  1.08it/s, loss=0.58] \u001b[A\n",
      "Train step of epoch 0:  18%|â–ˆâ–Š        | 46/250 [00:44<03:08,  1.08it/s, loss=0.58]\u001b[A\n",
      "Train step of epoch 0:  18%|â–ˆâ–Š        | 46/250 [00:44<03:08,  1.08it/s, loss=0.52]\u001b[A\n",
      "Train step of epoch 0:  19%|â–ˆâ–‰        | 47/250 [00:45<03:07,  1.08it/s, loss=0.52]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train step of epoch 0:  19%|â–ˆâ–‰        | 47/250 [00:45<03:07,  1.08it/s, loss=0.696]\u001b[A\n",
      "Train step of epoch 0:  19%|â–ˆâ–‰        | 48/250 [00:46<03:06,  1.08it/s, loss=0.696]\u001b[A\n",
      "Train step of epoch 0:  19%|â–ˆâ–‰        | 48/250 [00:46<03:06,  1.08it/s, loss=0.726]\u001b[A\n",
      "Train step of epoch 0:  20%|â–ˆâ–‰        | 49/250 [00:47<03:05,  1.09it/s, loss=0.726]\u001b[A\n",
      "Train step of epoch 0:  20%|â–ˆâ–‰        | 49/250 [00:47<03:05,  1.09it/s, loss=0.627]\u001b[A\n",
      "Train step of epoch 0:  20%|â–ˆâ–ˆ        | 50/250 [00:48<03:03,  1.09it/s, loss=0.627]\u001b[A\n",
      "Train step of epoch 0:  20%|â–ˆâ–ˆ        | 50/250 [00:48<03:03,  1.09it/s, loss=0.757]\u001b[A\n",
      "Train step of epoch 0:  20%|â–ˆâ–ˆ        | 51/250 [00:49<03:02,  1.09it/s, loss=0.757]\u001b[A\n",
      "Train step of epoch 0:  20%|â–ˆâ–ˆ        | 51/250 [00:49<03:02,  1.09it/s, loss=0.447]\u001b[A\n",
      "Train step of epoch 0:  21%|â–ˆâ–ˆ        | 52/250 [00:50<03:01,  1.09it/s, loss=0.447]\u001b[A\n",
      "Train step of epoch 0:  21%|â–ˆâ–ˆ        | 52/250 [00:50<03:01,  1.09it/s, loss=0.638]\u001b[A\n",
      "Train step of epoch 0:  21%|â–ˆâ–ˆ        | 53/250 [00:50<03:00,  1.09it/s, loss=0.638]\u001b[A\n",
      "Train step of epoch 0:  21%|â–ˆâ–ˆ        | 53/250 [00:51<03:00,  1.09it/s, loss=0.442]\u001b[A\n",
      "Train step of epoch 0:  22%|â–ˆâ–ˆâ–       | 54/250 [00:51<03:00,  1.09it/s, loss=0.442]\u001b[A\n",
      "Train step of epoch 0:  22%|â–ˆâ–ˆâ–       | 54/250 [00:51<03:00,  1.09it/s, loss=0.401]\u001b[A\n",
      "Train step of epoch 0:  22%|â–ˆâ–ˆâ–       | 55/250 [00:52<02:58,  1.09it/s, loss=0.401]\u001b[A\n",
      "Train step of epoch 0:  22%|â–ˆâ–ˆâ–       | 55/250 [00:52<02:58,  1.09it/s, loss=0.483]\u001b[A\n",
      "Train step of epoch 0:  22%|â–ˆâ–ˆâ–       | 56/250 [00:53<02:57,  1.09it/s, loss=0.483]\u001b[A\n",
      "Train step of epoch 0:  22%|â–ˆâ–ˆâ–       | 56/250 [00:53<02:57,  1.09it/s, loss=0.531]\u001b[A\n",
      "Train step of epoch 0:  23%|â–ˆâ–ˆâ–       | 57/250 [00:54<02:56,  1.09it/s, loss=0.531]\u001b[A\n",
      "Train step of epoch 0:  23%|â–ˆâ–ˆâ–       | 57/250 [00:54<02:56,  1.09it/s, loss=0.526]\u001b[A\n",
      "Train step of epoch 0:  23%|â–ˆâ–ˆâ–       | 58/250 [00:55<02:55,  1.09it/s, loss=0.526]\u001b[A\n",
      "Train step of epoch 0:  23%|â–ˆâ–ˆâ–       | 58/250 [00:55<02:55,  1.09it/s, loss=0.436]\u001b[A\n",
      "Train step of epoch 0:  24%|â–ˆâ–ˆâ–       | 59/250 [00:56<02:54,  1.09it/s, loss=0.436]\u001b[A\n",
      "Train step of epoch 0:  24%|â–ˆâ–ˆâ–       | 59/250 [00:56<02:54,  1.09it/s, loss=0.62] \u001b[A\n",
      "Train step of epoch 0:  24%|â–ˆâ–ˆâ–       | 60/250 [00:57<02:53,  1.09it/s, loss=0.62]\u001b[A\n",
      "Train step of epoch 0:  24%|â–ˆâ–ˆâ–       | 60/250 [00:57<02:53,  1.09it/s, loss=0.269]\u001b[A\n",
      "Train step of epoch 0:  24%|â–ˆâ–ˆâ–       | 61/250 [00:58<02:52,  1.09it/s, loss=0.269]\u001b[A\n",
      "Train step of epoch 0:  24%|â–ˆâ–ˆâ–       | 61/250 [00:58<02:52,  1.09it/s, loss=1.15] \u001b[A\n",
      "Train step of epoch 0:  25%|â–ˆâ–ˆâ–       | 62/250 [00:59<02:51,  1.09it/s, loss=1.15]\u001b[A\n",
      "Train step of epoch 0:  25%|â–ˆâ–ˆâ–       | 62/250 [00:59<02:51,  1.09it/s, loss=0.148]\u001b[A\n",
      "Train step of epoch 0:  25%|â–ˆâ–ˆâ–Œ       | 63/250 [01:00<02:50,  1.09it/s, loss=0.148]\u001b[A\n",
      "Train step of epoch 0:  25%|â–ˆâ–ˆâ–Œ       | 63/250 [01:00<02:50,  1.09it/s, loss=0.98] \u001b[A\n",
      "Train step of epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 64/250 [01:01<02:50,  1.09it/s, loss=0.98]\u001b[A\n",
      "Train step of epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 64/250 [01:01<02:50,  1.09it/s, loss=0.274]\u001b[A\n",
      "Train step of epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 65/250 [01:01<02:49,  1.09it/s, loss=0.274]\u001b[A\n",
      "Train step of epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 65/250 [01:01<02:49,  1.09it/s, loss=0.746]\u001b[A\n",
      "Train step of epoch 0:  26%|â–ˆâ–ˆâ–‹       | 66/250 [01:02<02:48,  1.09it/s, loss=0.746]\u001b[A\n",
      "Train step of epoch 0:  26%|â–ˆâ–ˆâ–‹       | 66/250 [01:02<02:48,  1.09it/s, loss=0.177]\u001b[A\n",
      "Train step of epoch 0:  27%|â–ˆâ–ˆâ–‹       | 67/250 [01:03<02:47,  1.09it/s, loss=0.177]\u001b[A\n",
      "Train step of epoch 0:  27%|â–ˆâ–ˆâ–‹       | 67/250 [01:03<02:47,  1.09it/s, loss=0.867]\u001b[A\n",
      "Train step of epoch 0:  27%|â–ˆâ–ˆâ–‹       | 68/250 [01:04<02:47,  1.09it/s, loss=0.867]\u001b[A\n",
      "Train step of epoch 0:  27%|â–ˆâ–ˆâ–‹       | 68/250 [01:04<02:47,  1.09it/s, loss=0.398]\u001b[A\n",
      "Train step of epoch 0:  28%|â–ˆâ–ˆâ–Š       | 69/250 [01:05<02:46,  1.09it/s, loss=0.398]\u001b[A\n",
      "Train step of epoch 0:  28%|â–ˆâ–ˆâ–Š       | 69/250 [01:05<02:46,  1.09it/s, loss=0.806]\u001b[A\n",
      "Train step of epoch 0:  28%|â–ˆâ–ˆâ–Š       | 70/250 [01:06<02:45,  1.09it/s, loss=0.806]\u001b[A\n",
      "Train step of epoch 0:  28%|â–ˆâ–ˆâ–Š       | 70/250 [01:06<02:45,  1.09it/s, loss=0.798]\u001b[A\n",
      "Train step of epoch 0:  28%|â–ˆâ–ˆâ–Š       | 71/250 [01:07<02:44,  1.09it/s, loss=0.798]\u001b[A\n",
      "Train step of epoch 0:  28%|â–ˆâ–ˆâ–Š       | 71/250 [01:07<02:44,  1.09it/s, loss=0.587]\u001b[A\n",
      "Train step of epoch 0:  29%|â–ˆâ–ˆâ–‰       | 72/250 [01:08<02:43,  1.09it/s, loss=0.587]\u001b[A\n",
      "Train step of epoch 0:  29%|â–ˆâ–ˆâ–‰       | 72/250 [01:08<02:43,  1.09it/s, loss=0.849]\u001b[A\n",
      "Train step of epoch 0:  29%|â–ˆâ–ˆâ–‰       | 73/250 [01:09<02:42,  1.09it/s, loss=0.849]\u001b[A\n",
      "Train step of epoch 0:  29%|â–ˆâ–ˆâ–‰       | 73/250 [01:09<02:42,  1.09it/s, loss=0.829]\u001b[A\n",
      "Train step of epoch 0:  30%|â–ˆâ–ˆâ–‰       | 74/250 [01:10<02:42,  1.09it/s, loss=0.829]\u001b[A\n",
      "Train step of epoch 0:  30%|â–ˆâ–ˆâ–‰       | 74/250 [01:10<02:42,  1.09it/s, loss=0.583]\u001b[A\n",
      "Train step of epoch 0:  30%|â–ˆâ–ˆâ–ˆ       | 75/250 [01:11<02:41,  1.08it/s, loss=0.583]\u001b[A\n",
      "Train step of epoch 0:  30%|â–ˆâ–ˆâ–ˆ       | 75/250 [01:11<02:41,  1.08it/s, loss=0.713]\u001b[A\n",
      "Train step of epoch 0:  30%|â–ˆâ–ˆâ–ˆ       | 76/250 [01:12<02:40,  1.08it/s, loss=0.713]\u001b[A\n",
      "Train step of epoch 0:  30%|â–ˆâ–ˆâ–ˆ       | 76/250 [01:12<02:40,  1.08it/s, loss=0.72] \u001b[A\n",
      "Train step of epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 77/250 [01:13<02:39,  1.08it/s, loss=0.72]\u001b[A\n",
      "Train step of epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 77/250 [01:13<02:39,  1.08it/s, loss=0.456]\u001b[A\n",
      "Train step of epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 78/250 [01:13<02:38,  1.08it/s, loss=0.456]\u001b[A\n",
      "Train step of epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 78/250 [01:13<02:38,  1.08it/s, loss=0.602]\u001b[A\n",
      "Train step of epoch 0:  32%|â–ˆâ–ˆâ–ˆâ–      | 79/250 [01:14<02:38,  1.08it/s, loss=0.602]\u001b[A\n",
      "Train step of epoch 0:  32%|â–ˆâ–ˆâ–ˆâ–      | 79/250 [01:14<02:38,  1.08it/s, loss=0.582]\u001b[A\n",
      "Train step of epoch 0:  32%|â–ˆâ–ˆâ–ˆâ–      | 80/250 [01:15<02:37,  1.08it/s, loss=0.582]\u001b[A\n",
      "Train step of epoch 0:  32%|â–ˆâ–ˆâ–ˆâ–      | 80/250 [01:15<02:37,  1.08it/s, loss=0.718]\u001b[A\n",
      "Train step of epoch 0:  32%|â–ˆâ–ˆâ–ˆâ–      | 81/250 [01:16<02:36,  1.08it/s, loss=0.718]\u001b[A\n",
      "Train step of epoch 0:  32%|â–ˆâ–ˆâ–ˆâ–      | 81/250 [01:16<02:36,  1.08it/s, loss=0.349]\u001b[A\n",
      "Train step of epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–      | 82/250 [01:17<02:35,  1.08it/s, loss=0.349]\u001b[A\n",
      "Train step of epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–      | 82/250 [01:17<02:35,  1.08it/s, loss=0.783]\u001b[A\n",
      "Train step of epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–      | 83/250 [01:18<02:34,  1.08it/s, loss=0.783]\u001b[A\n",
      "Train step of epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–      | 83/250 [01:18<02:34,  1.08it/s, loss=0.533]\u001b[A\n",
      "Train step of epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 84/250 [01:19<02:34,  1.08it/s, loss=0.533]\u001b[A\n",
      "Train step of epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 84/250 [01:19<02:34,  1.08it/s, loss=0.654]\u001b[A\n",
      "Train step of epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 85/250 [01:20<02:33,  1.08it/s, loss=0.654]\u001b[A\n",
      "Train step of epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 85/250 [01:20<02:33,  1.08it/s, loss=0.778]\u001b[A\n",
      "Train step of epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 86/250 [01:21<02:32,  1.07it/s, loss=0.778]\u001b[A\n",
      "Train step of epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 86/250 [01:21<02:32,  1.07it/s, loss=0.568]\u001b[A\n",
      "Train step of epoch 0:  35%|â–ˆâ–ˆâ–ˆâ–      | 87/250 [01:22<02:32,  1.07it/s, loss=0.568]\u001b[A\n",
      "Train step of epoch 0:  35%|â–ˆâ–ˆâ–ˆâ–      | 87/250 [01:22<02:32,  1.07it/s, loss=0.468]\u001b[A\n",
      "Train step of epoch 0:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 88/250 [01:23<02:30,  1.07it/s, loss=0.468]\u001b[A\n",
      "Train step of epoch 0:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 88/250 [01:23<02:30,  1.07it/s, loss=1.06] \u001b[A\n",
      "Train step of epoch 0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 89/250 [01:24<02:30,  1.07it/s, loss=1.06]\u001b[A\n",
      "Train step of epoch 0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 89/250 [01:24<02:30,  1.07it/s, loss=0.857]\u001b[A\n",
      "Train step of epoch 0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 90/250 [01:25<02:29,  1.07it/s, loss=0.857]\u001b[A\n",
      "Train step of epoch 0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 90/250 [01:25<02:29,  1.07it/s, loss=0.726]\u001b[A\n",
      "Train step of epoch 0:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 91/250 [01:26<02:28,  1.07it/s, loss=0.726]\u001b[A\n",
      "Train step of epoch 0:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 91/250 [01:26<02:28,  1.07it/s, loss=0.664]\u001b[A\n",
      "Train step of epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 92/250 [01:27<02:27,  1.07it/s, loss=0.664]\u001b[A\n",
      "Train step of epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 92/250 [01:27<02:27,  1.07it/s, loss=0.627]\u001b[A\n",
      "Train step of epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 93/250 [01:27<02:26,  1.07it/s, loss=0.627]\u001b[A\n",
      "Train step of epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 93/250 [01:27<02:26,  1.07it/s, loss=0.821]\u001b[A\n",
      "Train step of epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 94/250 [01:28<02:26,  1.07it/s, loss=0.821]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train step of epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 94/250 [01:28<02:26,  1.07it/s, loss=0.661]\u001b[A\n",
      "Train step of epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 95/250 [01:29<02:25,  1.07it/s, loss=0.661]\u001b[A\n",
      "Train step of epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 95/250 [01:29<02:25,  1.07it/s, loss=0.581]\u001b[A\n",
      "Train step of epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 96/250 [01:30<02:24,  1.06it/s, loss=0.581]\u001b[A\n",
      "Train step of epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 96/250 [01:30<02:24,  1.06it/s, loss=0.729]\u001b[A\n",
      "Train step of epoch 0:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 97/250 [01:31<02:24,  1.06it/s, loss=0.729]\u001b[A\n",
      "Train step of epoch 0:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 97/250 [01:31<02:24,  1.06it/s, loss=0.753]\u001b[A\n",
      "Train step of epoch 0:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 98/250 [01:32<02:23,  1.06it/s, loss=0.753]\u001b[A\n",
      "Train step of epoch 0:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 98/250 [01:32<02:23,  1.06it/s, loss=0.711]\u001b[A\n",
      "Train step of epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 99/250 [01:33<02:22,  1.06it/s, loss=0.711]\u001b[A\n",
      "Train step of epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 99/250 [01:33<02:22,  1.06it/s, loss=0.617]\u001b[A\n",
      "Train step of epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 100/250 [01:34<02:21,  1.06it/s, loss=0.617]\u001b[A\n",
      "Train step of epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 100/250 [01:34<02:21,  1.06it/s, loss=0.656]\u001b[A\n",
      "Train step of epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 101/250 [01:35<02:20,  1.06it/s, loss=0.656]\u001b[A\n",
      "Train step of epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 101/250 [01:35<02:20,  1.06it/s, loss=0.695]\u001b[A\n",
      "Train step of epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 102/250 [01:36<02:19,  1.06it/s, loss=0.695]\u001b[A\n",
      "Train step of epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 102/250 [01:36<02:19,  1.06it/s, loss=0.719]\u001b[A\n",
      "Train step of epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 103/250 [01:37<02:19,  1.06it/s, loss=0.719]\u001b[A\n",
      "Train step of epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 103/250 [01:37<02:19,  1.06it/s, loss=0.734]\u001b[A\n",
      "Train step of epoch 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 104/250 [01:38<02:18,  1.06it/s, loss=0.734]\u001b[A\n",
      "Train step of epoch 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 104/250 [01:38<02:18,  1.06it/s, loss=0.753]\u001b[A\n",
      "Train step of epoch 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 105/250 [01:39<02:17,  1.06it/s, loss=0.753]\u001b[A\n",
      "Train step of epoch 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 105/250 [01:39<02:17,  1.06it/s, loss=0.64] \u001b[A\n",
      "Train step of epoch 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 106/250 [01:40<02:16,  1.06it/s, loss=0.64]\u001b[A\n",
      "Train step of epoch 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 106/250 [01:40<02:16,  1.06it/s, loss=0.675]\u001b[A\n",
      "Train step of epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 107/250 [01:41<02:15,  1.06it/s, loss=0.675]\u001b[A\n",
      "Train step of epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 107/250 [01:41<02:15,  1.06it/s, loss=0.701]\u001b[A\n",
      "Train step of epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 108/250 [01:42<02:14,  1.06it/s, loss=0.701]\u001b[A\n",
      "Train step of epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 108/250 [01:42<02:14,  1.06it/s, loss=0.635]\u001b[A\n",
      "Train step of epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 109/250 [01:43<02:13,  1.06it/s, loss=0.635]\u001b[A\n",
      "Train step of epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 109/250 [01:43<02:13,  1.06it/s, loss=0.685]\u001b[A\n",
      "Train step of epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 110/250 [01:44<02:12,  1.06it/s, loss=0.685]\u001b[A\n",
      "Train step of epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 110/250 [01:44<02:12,  1.06it/s, loss=0.713]\u001b[A\n",
      "Train step of epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 111/250 [01:44<02:11,  1.06it/s, loss=0.713]\u001b[A\n",
      "Train step of epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 111/250 [01:44<02:11,  1.06it/s, loss=0.643]\u001b[A\n",
      "Train step of epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 112/250 [01:45<02:10,  1.06it/s, loss=0.643]\u001b[A\n",
      "Train step of epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 112/250 [01:45<02:10,  1.06it/s, loss=0.672]\u001b[A\n",
      "Train step of epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 113/250 [01:46<02:09,  1.06it/s, loss=0.672]\u001b[A\n",
      "Train step of epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 113/250 [01:46<02:09,  1.06it/s, loss=0.663]\u001b[A\n",
      "Train step of epoch 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 114/250 [01:47<02:08,  1.06it/s, loss=0.663]\u001b[A\n",
      "Train step of epoch 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 114/250 [01:47<02:08,  1.06it/s, loss=0.749]\u001b[A\n",
      "Train step of epoch 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 115/250 [01:48<02:07,  1.06it/s, loss=0.749]\u001b[A\n",
      "Train step of epoch 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 115/250 [01:48<02:07,  1.06it/s, loss=0.684]\u001b[A\n",
      "Train step of epoch 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 116/250 [01:49<02:06,  1.06it/s, loss=0.684]\u001b[A\n",
      "Train step of epoch 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 116/250 [01:49<02:06,  1.06it/s, loss=0.643]\u001b[A\n",
      "Train step of epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 117/250 [01:50<02:05,  1.06it/s, loss=0.643]\u001b[A\n",
      "Train step of epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 117/250 [01:50<02:05,  1.06it/s, loss=0.616]\u001b[A\n",
      "Train step of epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 118/250 [01:51<02:04,  1.06it/s, loss=0.616]\u001b[A\n",
      "Train step of epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 118/250 [01:51<02:04,  1.06it/s, loss=0.578]\u001b[A\n",
      "Train step of epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 119/250 [01:52<02:03,  1.06it/s, loss=0.578]\u001b[A\n",
      "Train step of epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 119/250 [01:52<02:03,  1.06it/s, loss=0.552]\u001b[A\n",
      "Train step of epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 120/250 [01:53<02:02,  1.06it/s, loss=0.552]\u001b[A\n",
      "Train step of epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 120/250 [01:53<02:02,  1.06it/s, loss=0.905]\u001b[A\n",
      "Train step of epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 121/250 [01:54<02:01,  1.06it/s, loss=0.905]\u001b[A\n",
      "Train step of epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 121/250 [01:54<02:01,  1.06it/s, loss=0.773]\u001b[A\n",
      "Train step of epoch 0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 122/250 [01:55<02:00,  1.06it/s, loss=0.773]\u001b[A\n",
      "Train step of epoch 0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 122/250 [01:55<02:00,  1.06it/s, loss=0.648]\u001b[A\n",
      "Train step of epoch 0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 123/250 [01:56<01:59,  1.06it/s, loss=0.648]\u001b[A\n",
      "Train step of epoch 0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 123/250 [01:56<01:59,  1.06it/s, loss=0.595]\u001b[A\n",
      "Train step of epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 124/250 [01:57<01:58,  1.06it/s, loss=0.595]\u001b[A\n",
      "Train step of epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 124/250 [01:57<01:58,  1.06it/s, loss=0.668]\u001b[A\n",
      "Train step of epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 125/250 [01:58<01:57,  1.06it/s, loss=0.668]\u001b[A\n",
      "Train step of epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 125/250 [01:58<01:57,  1.06it/s, loss=0.625]\u001b[A\n",
      "Train step of epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 126/250 [01:59<01:56,  1.06it/s, loss=0.625]\u001b[A\n",
      "Train step of epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 126/250 [01:59<01:56,  1.06it/s, loss=0.674]\u001b[A\n",
      "Train step of epoch 0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 127/250 [02:00<01:55,  1.06it/s, loss=0.674]\u001b[A\n",
      "Train step of epoch 0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 127/250 [02:00<01:55,  1.06it/s, loss=0.601]\u001b[A\n",
      "Train step of epoch 0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 128/250 [02:00<01:54,  1.07it/s, loss=0.601]\u001b[A\n",
      "Train step of epoch 0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 128/250 [02:01<01:54,  1.07it/s, loss=0.588]\u001b[A\n",
      "Train step of epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 129/250 [02:01<01:53,  1.07it/s, loss=0.588]\u001b[A\n",
      "Train step of epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 129/250 [02:01<01:53,  1.07it/s, loss=0.654]\u001b[A\n",
      "Train step of epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 130/250 [02:02<01:52,  1.07it/s, loss=0.654]\u001b[A\n",
      "Train step of epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 130/250 [02:02<01:52,  1.07it/s, loss=0.659]\u001b[A\n",
      "Train step of epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 131/250 [02:03<01:51,  1.07it/s, loss=0.659]\u001b[A\n",
      "Train step of epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 131/250 [02:03<01:51,  1.07it/s, loss=0.845]\u001b[A\n",
      "Train step of epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 132/250 [02:04<01:50,  1.07it/s, loss=0.845]\u001b[A\n",
      "Train step of epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 132/250 [02:04<01:50,  1.07it/s, loss=0.605]\u001b[A\n",
      "Train step of epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 133/250 [02:05<01:49,  1.07it/s, loss=0.605]\u001b[A\n",
      "Train step of epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 133/250 [02:05<01:49,  1.07it/s, loss=0.773]\u001b[A\n",
      "Train step of epoch 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 134/250 [02:06<01:48,  1.07it/s, loss=0.773]\u001b[A\n",
      "Train step of epoch 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 134/250 [02:06<01:48,  1.07it/s, loss=0.752]\u001b[A\n",
      "Train step of epoch 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 135/250 [02:07<01:47,  1.07it/s, loss=0.752]\u001b[A\n",
      "Train step of epoch 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 135/250 [02:07<01:47,  1.07it/s, loss=0.741]\u001b[A\n",
      "Train step of epoch 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 136/250 [02:08<01:46,  1.07it/s, loss=0.741]\u001b[A\n",
      "Train step of epoch 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 136/250 [02:08<01:46,  1.07it/s, loss=0.672]\u001b[A\n",
      "Train step of epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 137/250 [02:09<01:45,  1.07it/s, loss=0.672]\u001b[A\n",
      "Train step of epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 137/250 [02:09<01:45,  1.07it/s, loss=0.529]\u001b[A\n",
      "Train step of epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 138/250 [02:10<01:44,  1.07it/s, loss=0.529]\u001b[A\n",
      "Train step of epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 138/250 [02:10<01:44,  1.07it/s, loss=0.604]\u001b[A\n",
      "Train step of epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 139/250 [02:11<01:43,  1.07it/s, loss=0.604]\u001b[A\n",
      "Train step of epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 139/250 [02:11<01:43,  1.07it/s, loss=0.538]\u001b[A\n",
      "Train step of epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 140/250 [02:12<01:42,  1.08it/s, loss=0.538]\u001b[A\n",
      "Train step of epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 140/250 [02:12<01:42,  1.08it/s, loss=0.501]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train step of epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 141/250 [02:13<01:41,  1.08it/s, loss=0.501]\u001b[A\n",
      "Train step of epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 141/250 [02:13<01:41,  1.08it/s, loss=0.606]\u001b[A\n",
      "Train step of epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 142/250 [02:14<01:40,  1.08it/s, loss=0.606]\u001b[A\n",
      "Train step of epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 142/250 [02:14<01:40,  1.08it/s, loss=0.779]\u001b[A\n",
      "Train step of epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 143/250 [02:14<01:39,  1.08it/s, loss=0.779]\u001b[A\n",
      "Train step of epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 143/250 [02:14<01:39,  1.08it/s, loss=0.518]\u001b[A\n",
      "Train step of epoch 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 144/250 [02:15<01:38,  1.08it/s, loss=0.518]\u001b[A\n",
      "Train step of epoch 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 144/250 [02:15<01:38,  1.08it/s, loss=0.552]\u001b[A\n",
      "Train step of epoch 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 145/250 [02:16<01:37,  1.08it/s, loss=0.552]\u001b[A\n",
      "Train step of epoch 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 145/250 [02:16<01:37,  1.08it/s, loss=0.617]\u001b[A\n",
      "Train step of epoch 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 146/250 [02:17<01:36,  1.08it/s, loss=0.617]\u001b[A\n",
      "Train step of epoch 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 146/250 [02:17<01:36,  1.08it/s, loss=0.631]\u001b[A\n",
      "Train step of epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 147/250 [02:18<01:35,  1.08it/s, loss=0.631]\u001b[A\n",
      "Train step of epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 147/250 [02:18<01:35,  1.08it/s, loss=0.323]\u001b[A\n",
      "Train step of epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 148/250 [02:19<01:34,  1.08it/s, loss=0.323]\u001b[A\n",
      "Train step of epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 148/250 [02:19<01:34,  1.08it/s, loss=0.688]\u001b[A\n",
      "Train step of epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 149/250 [02:20<01:33,  1.08it/s, loss=0.688]\u001b[A\n",
      "Train step of epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 149/250 [02:20<01:33,  1.08it/s, loss=0.657]\u001b[A\n",
      "Train step of epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 150/250 [02:21<01:32,  1.08it/s, loss=0.657]\u001b[A\n",
      "Train step of epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 150/250 [02:21<01:32,  1.08it/s, loss=0.552]\u001b[A\n",
      "Train step of epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 151/250 [02:22<01:31,  1.08it/s, loss=0.552]\u001b[A\n",
      "Train step of epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 151/250 [02:22<01:31,  1.08it/s, loss=0.24] \u001b[A\n",
      "Train step of epoch 0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 152/250 [02:23<01:30,  1.08it/s, loss=0.24]\u001b[A\n",
      "Train step of epoch 0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 152/250 [02:23<01:30,  1.08it/s, loss=0.523]\u001b[A\n",
      "Train step of epoch 0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 153/250 [02:24<01:29,  1.08it/s, loss=0.523]\u001b[A\n",
      "Train step of epoch 0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 153/250 [02:24<01:29,  1.08it/s, loss=0.675]\u001b[A\n",
      "Train step of epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 154/250 [02:25<01:29,  1.08it/s, loss=0.675]\u001b[A\n",
      "Train step of epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 154/250 [02:25<01:29,  1.08it/s, loss=1.33] \u001b[A\n",
      "Train step of epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 155/250 [02:26<01:28,  1.08it/s, loss=1.33]\u001b[A\n",
      "Train step of epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 155/250 [02:26<01:28,  1.08it/s, loss=0.808]\u001b[A\n",
      "Train step of epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 156/250 [02:27<01:27,  1.08it/s, loss=0.808]\u001b[A\n",
      "Train step of epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 156/250 [02:27<01:27,  1.08it/s, loss=0.404]\u001b[A\n",
      "Train step of epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 157/250 [02:27<01:26,  1.08it/s, loss=0.404]\u001b[A\n",
      "Train step of epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 157/250 [02:27<01:26,  1.08it/s, loss=0.492]\u001b[A\n",
      "Train step of epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 158/250 [02:28<01:25,  1.08it/s, loss=0.492]\u001b[A\n",
      "Train step of epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 158/250 [02:28<01:25,  1.08it/s, loss=0.696]\u001b[A\n",
      "Train step of epoch 0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 159/250 [02:29<01:24,  1.08it/s, loss=0.696]\u001b[A\n",
      "Train step of epoch 0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 159/250 [02:29<01:24,  1.08it/s, loss=0.741]\u001b[A\n",
      "Train step of epoch 0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 160/250 [02:30<01:23,  1.08it/s, loss=0.741]\u001b[A\n",
      "Train step of epoch 0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 160/250 [02:30<01:23,  1.08it/s, loss=0.456]\u001b[A\n",
      "Train step of epoch 0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 161/250 [02:31<01:22,  1.08it/s, loss=0.456]\u001b[A\n",
      "Train step of epoch 0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 161/250 [02:31<01:22,  1.08it/s, loss=0.675]\u001b[A\n",
      "Train step of epoch 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 162/250 [02:32<01:21,  1.08it/s, loss=0.675]\u001b[A\n",
      "Train step of epoch 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 162/250 [02:32<01:21,  1.08it/s, loss=0.611]\u001b[A\n",
      "Train step of epoch 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 163/250 [02:33<01:20,  1.08it/s, loss=0.611]\u001b[A\n",
      "Train step of epoch 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 163/250 [02:33<01:20,  1.08it/s, loss=0.434]\u001b[A\n",
      "Train step of epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 164/250 [02:34<01:19,  1.08it/s, loss=0.434]\u001b[A\n",
      "Train step of epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 164/250 [02:34<01:19,  1.08it/s, loss=0.718]\u001b[A\n",
      "Train step of epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 165/250 [02:35<01:18,  1.08it/s, loss=0.718]\u001b[A\n",
      "Train step of epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 165/250 [02:35<01:18,  1.08it/s, loss=0.929]\u001b[A\n",
      "Train step of epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 166/250 [02:36<01:17,  1.08it/s, loss=0.929]\u001b[A\n",
      "Train step of epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 166/250 [02:36<01:17,  1.08it/s, loss=0.427]\u001b[A\n",
      "Train step of epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 167/250 [02:37<01:16,  1.08it/s, loss=0.427]\u001b[A\n",
      "Train step of epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 167/250 [02:37<01:16,  1.08it/s, loss=0.387]\u001b[A\n",
      "Train step of epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 168/250 [02:38<01:16,  1.08it/s, loss=0.387]\u001b[A\n",
      "Train step of epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 168/250 [02:38<01:16,  1.08it/s, loss=0.536]\u001b[A\n",
      "Train step of epoch 0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 169/250 [02:39<01:15,  1.08it/s, loss=0.536]\u001b[A\n",
      "Train step of epoch 0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 169/250 [02:39<01:15,  1.08it/s, loss=0.416]\u001b[A\n",
      "Train step of epoch 0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 170/250 [02:39<01:14,  1.08it/s, loss=0.416]\u001b[A\n",
      "Train step of epoch 0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 170/250 [02:40<01:14,  1.08it/s, loss=0.867]\u001b[A\n",
      "Train step of epoch 0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 171/250 [02:40<01:13,  1.08it/s, loss=0.867]\u001b[A\n",
      "Train step of epoch 0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 171/250 [02:40<01:13,  1.08it/s, loss=0.633]\u001b[A\n",
      "Train step of epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 172/250 [02:41<01:12,  1.08it/s, loss=0.633]\u001b[A\n",
      "Train step of epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 172/250 [02:41<01:12,  1.08it/s, loss=0.754]\u001b[A\n",
      "Train step of epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 173/250 [02:42<01:11,  1.08it/s, loss=0.754]\u001b[A\n",
      "Train step of epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 173/250 [02:42<01:11,  1.08it/s, loss=0.444]\u001b[A\n",
      "Train step of epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 174/250 [02:43<01:10,  1.08it/s, loss=0.444]\u001b[A\n",
      "Train step of epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 174/250 [02:43<01:10,  1.08it/s, loss=0.385]\u001b[A\n",
      "Train step of epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 175/250 [02:44<01:09,  1.07it/s, loss=0.385]\u001b[A\n",
      "Train step of epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 175/250 [02:44<01:09,  1.07it/s, loss=0.701]\u001b[A\n",
      "Train step of epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 176/250 [02:45<01:08,  1.08it/s, loss=0.701]\u001b[A\n",
      "Train step of epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 176/250 [02:45<01:08,  1.08it/s, loss=0.435]\u001b[A\n",
      "Train step of epoch 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 177/250 [02:46<01:07,  1.07it/s, loss=0.435]\u001b[A\n",
      "Train step of epoch 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 177/250 [02:46<01:07,  1.07it/s, loss=0.37] \u001b[A\n",
      "Train step of epoch 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 178/250 [02:47<01:06,  1.08it/s, loss=0.37]\u001b[A\n",
      "Train step of epoch 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 178/250 [02:47<01:06,  1.08it/s, loss=0.339]\u001b[A\n",
      "Train step of epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 179/250 [02:48<01:06,  1.07it/s, loss=0.339]\u001b[A\n",
      "Train step of epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 179/250 [02:48<01:06,  1.07it/s, loss=0.555]\u001b[A\n",
      "Train step of epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 180/250 [02:49<01:05,  1.07it/s, loss=0.555]\u001b[A\n",
      "Train step of epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 180/250 [02:49<01:05,  1.07it/s, loss=0.745]\u001b[A\n",
      "Train step of epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 181/250 [02:50<01:04,  1.07it/s, loss=0.745]\u001b[A\n",
      "Train step of epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 181/250 [02:50<01:04,  1.07it/s, loss=0.236]\u001b[A\n",
      "Train step of epoch 0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 182/250 [02:51<01:03,  1.07it/s, loss=0.236]\u001b[A\n",
      "Train step of epoch 0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 182/250 [02:51<01:03,  1.07it/s, loss=1]    \u001b[A\n",
      "Train step of epoch 0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 183/250 [02:52<01:02,  1.08it/s, loss=1]\u001b[A\n",
      "Train step of epoch 0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 183/250 [02:52<01:02,  1.08it/s, loss=1.45]\u001b[A\n",
      "Train step of epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 184/250 [02:53<01:01,  1.07it/s, loss=1.45]\u001b[A\n",
      "Train step of epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 184/250 [02:53<01:01,  1.07it/s, loss=0.905]\u001b[A\n",
      "Train step of epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 185/250 [02:53<01:00,  1.08it/s, loss=0.905]\u001b[A\n",
      "Train step of epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 185/250 [02:53<01:00,  1.08it/s, loss=0.525]\u001b[A\n",
      "Train step of epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 186/250 [02:54<00:59,  1.07it/s, loss=0.525]\u001b[A\n",
      "Train step of epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 186/250 [02:54<00:59,  1.07it/s, loss=0.418]\u001b[A\n",
      "Train step of epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 187/250 [02:55<00:58,  1.07it/s, loss=0.418]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train step of epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 187/250 [02:55<00:58,  1.07it/s, loss=0.386]\u001b[A\n",
      "Train step of epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 188/250 [02:56<00:57,  1.07it/s, loss=0.386]\u001b[A\n",
      "Train step of epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 188/250 [02:56<00:57,  1.07it/s, loss=0.668]\u001b[A\n",
      "Train step of epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 189/250 [02:57<00:56,  1.07it/s, loss=0.668]\u001b[A\n",
      "Train step of epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 189/250 [02:57<00:56,  1.07it/s, loss=0.756]\u001b[A\n",
      "Train step of epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 190/250 [02:58<00:55,  1.07it/s, loss=0.756]\u001b[A\n",
      "Train step of epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 190/250 [02:58<00:55,  1.07it/s, loss=0.466]\u001b[A\n",
      "Train step of epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 191/250 [02:59<00:54,  1.07it/s, loss=0.466]\u001b[A\n",
      "Train step of epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 191/250 [02:59<00:54,  1.07it/s, loss=0.362]\u001b[A\n",
      "Train step of epoch 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 192/250 [03:00<00:53,  1.07it/s, loss=0.362]\u001b[A\n",
      "Train step of epoch 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 192/250 [03:00<00:53,  1.07it/s, loss=0.502]\u001b[A\n",
      "Train step of epoch 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 193/250 [03:01<00:53,  1.07it/s, loss=0.502]\u001b[A\n",
      "Train step of epoch 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 193/250 [03:01<00:53,  1.07it/s, loss=0.762]\u001b[A\n",
      "Train step of epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 194/250 [03:02<00:52,  1.07it/s, loss=0.762]\u001b[A\n",
      "Train step of epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 194/250 [03:02<00:52,  1.07it/s, loss=0.625]\u001b[A\n",
      "Train step of epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 195/250 [03:03<00:51,  1.07it/s, loss=0.625]\u001b[A\n",
      "Train step of epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 195/250 [03:03<00:51,  1.07it/s, loss=0.455]\u001b[A\n",
      "Train step of epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 196/250 [03:04<00:50,  1.07it/s, loss=0.455]\u001b[A\n",
      "Train step of epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 196/250 [03:04<00:50,  1.07it/s, loss=0.937]\u001b[A\n",
      "Train step of epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 197/250 [03:05<00:49,  1.07it/s, loss=0.937]\u001b[A\n",
      "Train step of epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 197/250 [03:05<00:49,  1.07it/s, loss=0.668]\u001b[A\n",
      "Train step of epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 198/250 [03:06<00:48,  1.07it/s, loss=0.668]\u001b[A\n",
      "Train step of epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 198/250 [03:06<00:48,  1.07it/s, loss=0.608]\u001b[A\n",
      "Train step of epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 199/250 [03:06<00:47,  1.07it/s, loss=0.608]\u001b[A\n",
      "Train step of epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 199/250 [03:07<00:47,  1.07it/s, loss=0.491]\u001b[A\n",
      "Train step of epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 200/250 [03:07<00:46,  1.07it/s, loss=0.491]\u001b[A\n",
      "Train step of epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 200/250 [03:07<00:46,  1.07it/s, loss=0.457]\u001b[A\n",
      "Train step of epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 201/250 [03:08<00:45,  1.07it/s, loss=0.457]\u001b[A\n",
      "Train step of epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 201/250 [03:08<00:45,  1.07it/s, loss=0.474]\u001b[A\n",
      "Train step of epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 202/250 [03:09<00:44,  1.07it/s, loss=0.474]\u001b[A\n",
      "Train step of epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 202/250 [03:09<00:44,  1.07it/s, loss=0.791]\u001b[A\n",
      "Train step of epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 203/250 [03:10<00:43,  1.07it/s, loss=0.791]\u001b[A\n",
      "Train step of epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 203/250 [03:10<00:43,  1.07it/s, loss=0.589]\u001b[A\n",
      "Train step of epoch 0:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 204/250 [03:11<00:42,  1.07it/s, loss=0.589]\u001b[A\n",
      "Train step of epoch 0:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 204/250 [03:11<00:42,  1.07it/s, loss=0.521]\u001b[A\n",
      "Train step of epoch 0:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 205/250 [03:12<00:41,  1.07it/s, loss=0.521]\u001b[A\n",
      "Train step of epoch 0:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 205/250 [03:12<00:41,  1.07it/s, loss=0.555]\u001b[A\n",
      "Train step of epoch 0:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 206/250 [03:13<00:41,  1.07it/s, loss=0.555]\u001b[A\n",
      "Train step of epoch 0:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 206/250 [03:13<00:41,  1.07it/s, loss=0.766]\u001b[A\n",
      "Train step of epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 207/250 [03:14<00:40,  1.07it/s, loss=0.766]\u001b[A\n",
      "Train step of epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 207/250 [03:14<00:40,  1.07it/s, loss=0.503]\u001b[A\n",
      "Train step of epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 208/250 [03:15<00:39,  1.07it/s, loss=0.503]\u001b[A\n",
      "Train step of epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 208/250 [03:15<00:39,  1.07it/s, loss=0.667]\u001b[A\n",
      "Train step of epoch 0:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 209/250 [03:16<00:38,  1.07it/s, loss=0.667]\u001b[A\n",
      "Train step of epoch 0:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 209/250 [03:16<00:38,  1.07it/s, loss=0.667]\u001b[A\n",
      "Train step of epoch 0:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 210/250 [03:17<00:37,  1.07it/s, loss=0.667]\u001b[A\n",
      "Train step of epoch 0:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 210/250 [03:17<00:37,  1.07it/s, loss=0.681]\u001b[A\n",
      "Train step of epoch 0:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 211/250 [03:18<00:36,  1.07it/s, loss=0.681]\u001b[A\n",
      "Train step of epoch 0:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 211/250 [03:18<00:36,  1.07it/s, loss=0.449]\u001b[A\n",
      "Train step of epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 212/250 [03:19<00:35,  1.07it/s, loss=0.449]\u001b[A\n",
      "Train step of epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 212/250 [03:19<00:35,  1.07it/s, loss=0.565]\u001b[A\n",
      "Train step of epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 213/250 [03:20<00:34,  1.07it/s, loss=0.565]\u001b[A\n",
      "Train step of epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 213/250 [03:20<00:34,  1.07it/s, loss=0.352]\u001b[A\n",
      "Train step of epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 214/250 [03:20<00:33,  1.07it/s, loss=0.352]\u001b[A\n",
      "Train step of epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 214/250 [03:20<00:33,  1.07it/s, loss=1.2]  \u001b[A\n",
      "Train step of epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 215/250 [03:21<00:32,  1.07it/s, loss=1.2]\u001b[A\n",
      "Train step of epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 215/250 [03:21<00:32,  1.07it/s, loss=0.547]\u001b[A\n",
      "Train step of epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 216/250 [03:22<00:31,  1.07it/s, loss=0.547]\u001b[A\n",
      "Train step of epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 216/250 [03:22<00:31,  1.07it/s, loss=0.845]\u001b[A\n",
      "Train step of epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 217/250 [03:23<00:30,  1.07it/s, loss=0.845]\u001b[A\n",
      "Train step of epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 217/250 [03:23<00:30,  1.07it/s, loss=0.82] \u001b[A\n",
      "Train step of epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 218/250 [03:24<00:29,  1.07it/s, loss=0.82]\u001b[A\n",
      "Train step of epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 218/250 [03:24<00:29,  1.07it/s, loss=0.582]\u001b[A\n",
      "Train step of epoch 0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 219/250 [03:25<00:28,  1.07it/s, loss=0.582]\u001b[A\n",
      "Train step of epoch 0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 219/250 [03:25<00:28,  1.07it/s, loss=0.37] \u001b[A\n",
      "Train step of epoch 0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 220/250 [03:26<00:28,  1.07it/s, loss=0.37]\u001b[A\n",
      "Train step of epoch 0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 220/250 [03:26<00:28,  1.07it/s, loss=0.648]\u001b[A\n",
      "Train step of epoch 0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 221/250 [03:27<00:27,  1.07it/s, loss=0.648]\u001b[A\n",
      "Train step of epoch 0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 221/250 [03:27<00:27,  1.07it/s, loss=0.695]\u001b[A\n",
      "Train step of epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 222/250 [03:28<00:26,  1.07it/s, loss=0.695]\u001b[A\n",
      "Train step of epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 222/250 [03:28<00:26,  1.07it/s, loss=0.364]\u001b[A\n",
      "Train step of epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 223/250 [03:29<00:25,  1.07it/s, loss=0.364]\u001b[A\n",
      "Train step of epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 223/250 [03:29<00:25,  1.07it/s, loss=0.688]\u001b[A\n",
      "Train step of epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 224/250 [03:30<00:24,  1.07it/s, loss=0.688]\u001b[A\n",
      "Train step of epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 224/250 [03:30<00:24,  1.07it/s, loss=0.761]\u001b[A\n",
      "Train step of epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 225/250 [03:31<00:23,  1.07it/s, loss=0.761]\u001b[A\n",
      "Train step of epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 225/250 [03:31<00:23,  1.07it/s, loss=0.614]\u001b[A\n",
      "Train step of epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 226/250 [03:32<00:22,  1.07it/s, loss=0.614]\u001b[A\n",
      "Train step of epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 226/250 [03:32<00:22,  1.07it/s, loss=0.642]\u001b[A\n",
      "Train step of epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 227/250 [03:33<00:21,  1.07it/s, loss=0.642]\u001b[A\n",
      "Train step of epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 227/250 [03:33<00:21,  1.07it/s, loss=0.722]\u001b[A\n",
      "Train step of epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 228/250 [03:34<00:20,  1.07it/s, loss=0.722]\u001b[A\n",
      "Train step of epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 228/250 [03:34<00:20,  1.07it/s, loss=0.527]\u001b[A\n",
      "Train step of epoch 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 229/250 [03:34<00:19,  1.07it/s, loss=0.527]\u001b[A\n",
      "Train step of epoch 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 229/250 [03:35<00:19,  1.07it/s, loss=0.68] \u001b[A\n",
      "Train step of epoch 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 230/250 [03:35<00:18,  1.07it/s, loss=0.68]\u001b[A\n",
      "Train step of epoch 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 230/250 [03:35<00:18,  1.07it/s, loss=0.723]\u001b[A\n",
      "Train step of epoch 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 231/250 [03:36<00:17,  1.07it/s, loss=0.723]\u001b[A\n",
      "Train step of epoch 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 231/250 [03:36<00:17,  1.07it/s, loss=0.427]\u001b[A\n",
      "Train step of epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 232/250 [03:37<00:16,  1.07it/s, loss=0.427]\u001b[A\n",
      "Train step of epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 232/250 [03:37<00:16,  1.07it/s, loss=0.843]\u001b[A\n",
      "Train step of epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 233/250 [03:38<00:15,  1.07it/s, loss=0.843]\u001b[A\n",
      "Train step of epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 233/250 [03:38<00:15,  1.07it/s, loss=0.567]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train step of epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 234/250 [03:39<00:14,  1.07it/s, loss=0.567]\u001b[A\n",
      "Train step of epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 234/250 [03:39<00:14,  1.07it/s, loss=0.557]\u001b[A\n",
      "Train step of epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 235/250 [03:40<00:14,  1.07it/s, loss=0.557]\u001b[A\n",
      "Train step of epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 235/250 [03:40<00:14,  1.07it/s, loss=0.84] \u001b[A\n",
      "Train step of epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 236/250 [03:41<00:13,  1.07it/s, loss=0.84]\u001b[A\n",
      "Train step of epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 236/250 [03:41<00:13,  1.07it/s, loss=0.534]\u001b[A\n",
      "Train step of epoch 0:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 237/250 [03:42<00:12,  1.07it/s, loss=0.534]\u001b[A\n",
      "Train step of epoch 0:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 237/250 [03:42<00:12,  1.07it/s, loss=0.538]\u001b[A\n",
      "Train step of epoch 0:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 238/250 [03:43<00:11,  1.07it/s, loss=0.538]\u001b[A\n",
      "Train step of epoch 0:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 238/250 [03:43<00:11,  1.07it/s, loss=0.758]\u001b[A\n",
      "Train step of epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 239/250 [03:44<00:10,  1.07it/s, loss=0.758]\u001b[A\n",
      "Train step of epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 239/250 [03:44<00:10,  1.07it/s, loss=0.705]\u001b[A\n",
      "Train step of epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 240/250 [03:45<00:09,  1.07it/s, loss=0.705]\u001b[A\n",
      "Train step of epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 240/250 [03:45<00:09,  1.07it/s, loss=0.617]\u001b[A\n",
      "Train step of epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 241/250 [03:46<00:08,  1.07it/s, loss=0.617]\u001b[A\n",
      "Train step of epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 241/250 [03:46<00:08,  1.07it/s, loss=0.503]\u001b[A\n",
      "Train step of epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 242/250 [03:47<00:07,  1.07it/s, loss=0.503]\u001b[A\n",
      "Train step of epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 242/250 [03:47<00:07,  1.07it/s, loss=0.798]\u001b[A\n",
      "Train step of epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 243/250 [03:48<00:06,  1.07it/s, loss=0.798]\u001b[A\n",
      "Train step of epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 243/250 [03:48<00:06,  1.07it/s, loss=0.543]\u001b[A\n",
      "Train step of epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 244/250 [03:49<00:05,  1.07it/s, loss=0.543]\u001b[A\n",
      "Train step of epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 244/250 [03:49<00:05,  1.07it/s, loss=0.625]\u001b[A\n",
      "Train step of epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 245/250 [03:49<00:04,  1.07it/s, loss=0.625]\u001b[A\n",
      "Train step of epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 245/250 [03:49<00:04,  1.07it/s, loss=0.806]\u001b[A\n",
      "Train step of epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 246/250 [03:50<00:03,  1.07it/s, loss=0.806]\u001b[A\n",
      "Train step of epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 246/250 [03:50<00:03,  1.07it/s, loss=0.785]\u001b[A\n",
      "Train step of epoch 0:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 247/250 [03:51<00:02,  1.07it/s, loss=0.785]\u001b[A\n",
      "Train step of epoch 0:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 247/250 [03:51<00:02,  1.07it/s, loss=0.462]\u001b[A\n",
      "Train step of epoch 0:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 248/250 [03:52<00:01,  1.07it/s, loss=0.462]\u001b[A\n",
      "Train step of epoch 0:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 248/250 [03:52<00:01,  1.07it/s, loss=1.24] \u001b[A\n",
      "Train step of epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 249/250 [03:53<00:00,  1.07it/s, loss=1.24]\u001b[A\n",
      "Train step of epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 249/250 [03:53<00:00,  1.07it/s, loss=0.585]\u001b[A\n",
      "Train step of epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [03:54<00:00,  1.07it/s, loss=0.585]\u001b[A\n",
      "Train epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:10<00:00, 250.02s/it]0,  1.07it/s, loss=0.495]\u001b[A\n",
      "Train step of epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [04:10<00:00,  1.00s/it, loss=0.62, dist_mean=0.258]\u001b[A\n",
      "Train epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:10<00:00, 250.02s/it]\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(use_lora=0)\n",
    "\n",
    "model.save_pretrained('/aiffel/KoChatGPT/output_2_RM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0ebed5c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: ëŒ€í•œë¯¼êµ­ì—ì„œ ê°€ì¥ ë†’ì€ ì‚°ì˜ ì´ë¦„ê³¼ ë†’ì´ëŠ” ë¬´ì—‡ì¸ê°€ìš”?\n",
      "reward score: -1.4\n"
     ]
    }
   ],
   "source": [
    "def inference_RM(input_text):\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(\n",
    "        torch.cuda.current_device())\n",
    "    output = model(input_ids)\n",
    "    output_reward = output.cpu().detach().numpy()[0]\n",
    "\n",
    "    print('input: %s\\nreward score: %.1f'%(input_text, output_reward))\n",
    "\n",
    "    return output_reward\n",
    "\n",
    "input_text = 'ëŒ€í•œë¯¼êµ­ì—ì„œ ê°€ì¥ ë†’ì€ ì‚°ì˜ ì´ë¦„ê³¼ ë†’ì´ëŠ” ë¬´ì—‡ì¸ê°€ìš”?'\n",
    "output_reward = inference_RM(input_text=input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "70cecc51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: ì¸ê³µì§€ëŠ¥(AI)ì€ ì»´í“¨í„°ì—ì„œ ìŒì„± ë° ì‘ì„±ëœ ì–¸ì–´ë¥¼ ë³´ê³  ì´í•´í•˜ê³  ë²ˆì—­í•˜ê³  ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê³  ì¶”ì²œí•˜ëŠ” ê¸°ëŠ¥ì„ í¬í•¨í•˜ì—¬ ë‹¤ì–‘í•œ ê³ ê¸‰ ê¸°ëŠ¥ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆëŠ” ì¼ë ¨ì˜ ê¸°ìˆ ì…ë‹ˆë‹¤.\n",
      "reward score: -1.1\n"
     ]
    }
   ],
   "source": [
    "input_text = 'ì¸ê³µì§€ëŠ¥(AI)ì€ ì»´í“¨í„°ì—ì„œ ìŒì„± ë° ì‘ì„±ëœ ì–¸ì–´ë¥¼ ë³´ê³  ì´í•´í•˜ê³  ë²ˆì—­í•˜ê³  ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê³  ì¶”ì²œí•˜ëŠ” ê¸°ëŠ¥ì„ í¬í•¨í•˜ì—¬ ë‹¤ì–‘í•œ ê³ ê¸‰ ê¸°ëŠ¥ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆëŠ” ì¼ë ¨ì˜ ê¸°ìˆ ì…ë‹ˆë‹¤.'\n",
    "\n",
    "output_reward = inference_RM(input_text=input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2b1da617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: ì¸ê³µì§€ëŠ¥(AI)ì€ ì»´í“¨í„°ì—ì„œ ìŒì„± ë° ì‘ì„±ëœ ì–¸ì–´ë¥¼ ë³´ê³  ì´í•´í•˜ê³  ë²ˆì—­í•˜ê³  ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê³  ì¶”ì²œí•˜ëŠ” ê¸°ëŠ¥ì„ í¬í•¨í•˜ì—¬ ë‹¤ì–‘í•œ ê³ ê¸‰ ê¸°ëŠ¥ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆëŠ” ì¼ë ¨ì˜ ê¸°ìˆ ì…ë‹ˆë‹¤. AIëŠ” í˜„ëŒ€ì ì¸ ì»´í“¨íŒ… í˜ì‹ ì—ì„œ ì¤‘ì¶”ì ì¸ ì—­í• ì„ í•˜ë©° ê°œì¸ê³¼ ë¹„ì¦ˆë‹ˆìŠ¤ì˜ ê°€ì¹˜ë¥¼ ì°½ì¶œí•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ê´‘í•™ ë¬¸ì ì¸ì‹(OCR)ì€ AIë¥¼ ì‚¬ìš©í•´ ì´ë¯¸ì§€ ë° ë¬¸ì„œì—ì„œ í…ìŠ¤íŠ¸ ë° ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ê³ , êµ¬ì¡°í™”ë˜ì§€ ì•Šì€ ì½˜í…ì¸ ë¥¼ ë¹„ì¦ˆë‹ˆìŠ¤ì— ë°”ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ ë§Œë“¤ê³ , ìœ ìš©í•œ ì •ë³´ë¥¼ ì°½ì¶œí•©ë‹ˆë‹¤.\n",
      "reward score: -1.0\n"
     ]
    }
   ],
   "source": [
    "input_text = \"ì¸ê³µì§€ëŠ¥(AI)ì€ ì»´í“¨í„°ì—ì„œ ìŒì„± ë° ì‘ì„±ëœ ì–¸ì–´ë¥¼ ë³´ê³  ì´í•´í•˜ê³  ë²ˆì—­í•˜ê³  ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê³  ì¶”ì²œí•˜ëŠ” ê¸°ëŠ¥ì„ í¬í•¨í•˜ì—¬ ë‹¤ì–‘í•œ ê³ ê¸‰ ê¸°ëŠ¥ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆëŠ” ì¼ë ¨ì˜ ê¸°ìˆ ì…ë‹ˆë‹¤. AIëŠ” í˜„ëŒ€ì ì¸ ì»´í“¨íŒ… í˜ì‹ ì—ì„œ ì¤‘ì¶”ì ì¸ ì—­í• ì„ í•˜ë©° ê°œì¸ê³¼ ë¹„ì¦ˆë‹ˆìŠ¤ì˜ ê°€ì¹˜ë¥¼ ì°½ì¶œí•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ê´‘í•™ ë¬¸ì ì¸ì‹(OCR)ì€ AIë¥¼ ì‚¬ìš©í•´ ì´ë¯¸ì§€ ë° ë¬¸ì„œì—ì„œ í…ìŠ¤íŠ¸ ë° ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ê³ , êµ¬ì¡°í™”ë˜ì§€ ì•Šì€ ì½˜í…ì¸ ë¥¼ ë¹„ì¦ˆë‹ˆìŠ¤ì— ë°”ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ ë§Œë“¤ê³ , ìœ ìš©í•œ ì •ë³´ë¥¼ ì°½ì¶œí•©ë‹ˆë‹¤.\"\n",
    "\n",
    "output_reward = inference_RM(input_text=input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "75d791fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: ì¸ê³µì§€ëŠ¥ì€ ì¼ë°˜ì ìœ¼ë¡œ ì¸ê°„ì˜ ì§€ëŠ¥ì´ í•„ìš”í•˜ê±°ë‚˜ ì¸ê°„ì´ ë¶„ì„í•  ìˆ˜ ìˆëŠ” ê²ƒë³´ë‹¤ ê·œëª¨ê°€ í° ë°ì´í„°ë¥¼ í¬í•¨í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ì¶”ë¡ , í•™ìŠµ ë° í–‰ë™í•  ìˆ˜ ìˆëŠ” ì»´í“¨í„° ë° ê¸°ê³„ë¥¼ êµ¬ì¶•í•˜ëŠ” ê²ƒê³¼ ê´€ë ¨ëœ ê³¼í•™ ë¶„ì•¼ì…ë‹ˆë‹¤. AIëŠ” ì»´í“¨í„° ê³µí•™, ë°ì´í„° ë¶„ì„ ë° í†µê³„, í•˜ë“œì›¨ì–´ ë° ì†Œí”„íŠ¸ì›¨ì–´ ì—”ì§€ë‹ˆì–´ë§, ì–¸ì–´í•™, ì‹ ê²½ ê³¼í•™ì€ ë¬¼ë¡  ì² í•™ê³¼ ì‹¬ë¦¬í•™ì„ í¬í•¨í•˜ì—¬ ì—¬ëŸ¬ í•™ë¬¸ì„ í¬ê´„í•˜ëŠ” ê´‘ë²”ìœ„í•œ ë¶„ì•¼ì…ë‹ˆë‹¤. ë¹„ì¦ˆë‹ˆìŠ¤ì˜ ìš´ì˜ ìˆ˜ì¤€ì—ì„œ AIëŠ” ì£¼ë¡œ ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ ëŸ¬ë‹ì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ëŠ” ê¸°ìˆ  ëª¨ìŒìœ¼ë¡œ, ë°ì´í„° ë¶„ì„, ì˜ˆìƒ ë° ì˜ˆì¸¡, ê°ì²´ ë¶„ë¥˜, ìì—°ì–´ ì²˜ë¦¬, ì¶”ì²œ, ì§€ëŠ¥í˜• ë°ì´í„° ê°€ì ¸ì˜¤ê¸° ë“±ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "reward score: -1.0\n"
     ]
    }
   ],
   "source": [
    "input_text = \"ì¸ê³µì§€ëŠ¥ì€ ì¼ë°˜ì ìœ¼ë¡œ ì¸ê°„ì˜ ì§€ëŠ¥ì´ í•„ìš”í•˜ê±°ë‚˜ ì¸ê°„ì´ ë¶„ì„í•  ìˆ˜ ìˆëŠ” ê²ƒë³´ë‹¤ ê·œëª¨ê°€ í° ë°ì´í„°ë¥¼ í¬í•¨í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ì¶”ë¡ , í•™ìŠµ ë° í–‰ë™í•  ìˆ˜ ìˆëŠ” ì»´í“¨í„° ë° ê¸°ê³„ë¥¼ êµ¬ì¶•í•˜ëŠ” ê²ƒê³¼ ê´€ë ¨ëœ ê³¼í•™ ë¶„ì•¼ì…ë‹ˆë‹¤. AIëŠ” ì»´í“¨í„° ê³µí•™, ë°ì´í„° ë¶„ì„ ë° í†µê³„, í•˜ë“œì›¨ì–´ ë° ì†Œí”„íŠ¸ì›¨ì–´ ì—”ì§€ë‹ˆì–´ë§, ì–¸ì–´í•™, ì‹ ê²½ ê³¼í•™ì€ ë¬¼ë¡  ì² í•™ê³¼ ì‹¬ë¦¬í•™ì„ í¬í•¨í•˜ì—¬ ì—¬ëŸ¬ í•™ë¬¸ì„ í¬ê´„í•˜ëŠ” ê´‘ë²”ìœ„í•œ ë¶„ì•¼ì…ë‹ˆë‹¤. ë¹„ì¦ˆë‹ˆìŠ¤ì˜ ìš´ì˜ ìˆ˜ì¤€ì—ì„œ AIëŠ” ì£¼ë¡œ ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ ëŸ¬ë‹ì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ëŠ” ê¸°ìˆ  ëª¨ìŒìœ¼ë¡œ, ë°ì´í„° ë¶„ì„, ì˜ˆìƒ ë° ì˜ˆì¸¡, ê°ì²´ ë¶„ë¥˜, ìì—°ì–´ ì²˜ë¦¬, ì¶”ì²œ, ì§€ëŠ¥í˜• ë°ì´í„° ê°€ì ¸ì˜¤ê¸° ë“±ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\"\n",
    "\n",
    "output_reward = inference_RM(input_text=input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a5b9b7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from chatgpt.models.base import RewardModel\n",
    "from chatgpt.models.gpt import GPTActor, GPTCritic\n",
    "from chatgpt.trainer import PPOTrainer\n",
    "from chatgpt.trainer.strategies import NaiveStrategy\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "05b0f6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with NaiveStrategy().model_init_context():\n",
    "    actor = GPTActor(pretrained='/aiffel/KoChatGPT/output_1_SFT', lora_rank=0).to(torch.cuda.current_device())\n",
    "    critic = GPTCritic(pretrained='/aiffel/KoChatGPT/output_2_RM', lora_rank=0).to(torch.cuda.current_device())\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        'skt/kogpt2-base-v2', bos_token='</s>', eos_token='</s>', unk_token='</s>', pad_token='</s>',\n",
    "        padding_side=\"right\", \n",
    "        model_max_length=512\n",
    "    )\n",
    "\n",
    "    initial_model = deepcopy(actor)\n",
    "    reward_model = RewardModel(deepcopy(critic.model), deepcopy(critic.value_head)).to(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4781a338",
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_optim = Adam(actor.parameters(), lr=5e-6)\n",
    "critic_optim = Adam(critic.parameters(), lr=5e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7d8e3a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "(actor, actor_optim), (critic, critic_optim), reward_model, initial_model = NaiveStrategy().prepare(\n",
    "    (actor, actor_optim), (critic, critic_optim), reward_model, initial_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9eeb0eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/aiffel/KoChatGPT/data_kochatgpt/kochatgpt_3_PPO.jsonl', \"r\", encoding='utf-8-sig') as json_file:\n",
    "    list_data_dict = json.load(json_file)\n",
    "    list_prompt = [tmp['prompt'] for tmp in list_data_dict]\n",
    "\n",
    "def tokenize_fn(texts):\n",
    "    batch = tokenizer(texts, return_tensors='pt', max_length=96, padding=True, truncation=True)\n",
    "    return {k: v.cuda() for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "43f8ea13",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = PPOTrainer(NaiveStrategy(),\n",
    "                     actor,\n",
    "                     critic,\n",
    "                     reward_model,\n",
    "                     initial_model,\n",
    "                     actor_optim,\n",
    "                     critic_optim,\n",
    "                     max_epochs=1,  \n",
    "                     train_batch_size=8, \n",
    "                     tokenizer=tokenize_fn,\n",
    "                     max_length=128,\n",
    "                     do_sample=True,\n",
    "                     temperature=1.0,\n",
    "                     top_k=50,\n",
    "                     pad_token_id=tokenizer.pad_token_id,\n",
    "                     eos_token_id=tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e38d8ea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode [1/10]:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:13<00:06,  6.63s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=0, critic_loss=0.000205]\u001b[A\n",
      "Train epoch [1/1]:  33%|â–ˆâ–ˆâ–ˆâ–      | 1/3 [00:00<00:01,  1.59it/s, actor_loss=0, critic_loss=0.000205]\u001b[A\n",
      "Train epoch [1/1]:  33%|â–ˆâ–ˆâ–ˆâ–      | 1/3 [00:01<00:01,  1.59it/s, actor_loss=0, critic_loss=0.118]   \u001b[A\n",
      "Train epoch [1/1]:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:01<00:00,  1.69it/s, actor_loss=0, critic_loss=0.118]\u001b[A\n",
      "Train epoch [1/1]:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:01<00:00,  1.69it/s, actor_loss=0, critic_loss=0.00889]\u001b[A\n",
      "Train epoch [1/1]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  1.70it/s, actor_loss=0, critic_loss=0.00889]\u001b[A\n",
      "Episode [1/10]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:21<00:00,  7.24s/it]\n",
      "Episode [2/10]:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:13<00:06,  6.77s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=-.129, critic_loss=0.0242]\u001b[A\n",
      "Train epoch [1/1]:  33%|â–ˆâ–ˆâ–ˆâ–      | 1/3 [00:00<00:01,  1.75it/s, actor_loss=-.129, critic_loss=0.0242]\u001b[A\n",
      "Train epoch [1/1]:  33%|â–ˆâ–ˆâ–ˆâ–      | 1/3 [00:01<00:01,  1.75it/s, actor_loss=-.129, critic_loss=0.0712]\u001b[A\n",
      "Train epoch [1/1]:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:01<00:00,  1.76it/s, actor_loss=-.129, critic_loss=0.0712]\u001b[A\n",
      "Train epoch [1/1]:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:01<00:00,  1.76it/s, actor_loss=-.14, critic_loss=0.0461] \u001b[A\n",
      "Train epoch [1/1]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  1.75it/s, actor_loss=-.14, critic_loss=0.0461]\u001b[A\n",
      "Episode [2/10]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:21<00:00,  7.25s/it]\n",
      "Episode [3/10]:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:12<00:06,  6.39s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=-.101, critic_loss=0.0135]\u001b[A\n",
      "Train epoch [1/1]:  33%|â–ˆâ–ˆâ–ˆâ–      | 1/3 [00:00<00:01,  1.79it/s, actor_loss=-.101, critic_loss=0.0135]\u001b[A\n",
      "Train epoch [1/1]:  33%|â–ˆâ–ˆâ–ˆâ–      | 1/3 [00:01<00:01,  1.79it/s, actor_loss=-.106, critic_loss=0.000762]\u001b[A\n",
      "Train epoch [1/1]:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:01<00:00,  1.79it/s, actor_loss=-.106, critic_loss=0.000762]\u001b[A\n",
      "Train epoch [1/1]:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:01<00:00,  1.79it/s, actor_loss=-.106, critic_loss=0.0103]  \u001b[A\n",
      "Train epoch [1/1]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  1.79it/s, actor_loss=-.106, critic_loss=0.0103]\u001b[A\n",
      "Episode [3/10]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:20<00:00,  6.93s/it]\n",
      "Episode [4/10]:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:12<00:06,  6.32s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=0.204, critic_loss=0.0634]\u001b[A\n",
      "Train epoch [1/1]:  33%|â–ˆâ–ˆâ–ˆâ–      | 1/3 [00:00<00:01,  1.78it/s, actor_loss=0.204, critic_loss=0.0634]\u001b[A\n",
      "Train epoch [1/1]:  33%|â–ˆâ–ˆâ–ˆâ–      | 1/3 [00:01<00:01,  1.78it/s, actor_loss=0.182, critic_loss=0.0258]\u001b[A\n",
      "Train epoch [1/1]:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:01<00:00,  1.79it/s, actor_loss=0.182, critic_loss=0.0258]\u001b[A\n",
      "Train epoch [1/1]:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:01<00:00,  1.79it/s, actor_loss=0.17, critic_loss=0.0106] \u001b[A\n",
      "Train epoch [1/1]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  1.78it/s, actor_loss=0.17, critic_loss=0.0106]\u001b[A\n",
      "Episode [4/10]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:20<00:00,  6.86s/it]\n",
      "Episode [5/10]:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:12<00:06,  6.34s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=0.0372, critic_loss=0.00101]\u001b[A\n",
      "Train epoch [1/1]:  33%|â–ˆâ–ˆâ–ˆâ–      | 1/3 [00:00<00:01,  1.74it/s, actor_loss=0.0372, critic_loss=0.00101]\u001b[A\n",
      "Train epoch [1/1]:  33%|â–ˆâ–ˆâ–ˆâ–      | 1/3 [00:01<00:01,  1.74it/s, actor_loss=0.0356, critic_loss=0.00361]\u001b[A\n",
      "Train epoch [1/1]:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:01<00:00,  1.76it/s, actor_loss=0.0356, critic_loss=0.00361]\u001b[A\n",
      "Train epoch [1/1]:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:01<00:00,  1.76it/s, actor_loss=0.0426, critic_loss=0.00663]\u001b[A\n",
      "Train epoch [1/1]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  1.76it/s, actor_loss=0.0426, critic_loss=0.00663]\u001b[A\n",
      "Episode [5/10]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:20<00:00,  6.98s/it]\n",
      "Episode [6/10]:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:12<00:06,  6.43s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=-.115, critic_loss=0.0176]\u001b[A\n",
      "Train epoch [1/1]:  33%|â–ˆâ–ˆâ–ˆâ–      | 1/3 [00:00<00:01,  1.76it/s, actor_loss=-.115, critic_loss=0.0176]\u001b[A\n",
      "Train epoch [1/1]:  33%|â–ˆâ–ˆâ–ˆâ–      | 1/3 [00:01<00:01,  1.76it/s, actor_loss=-.103, critic_loss=0.0135]\u001b[A\n",
      "Train epoch [1/1]:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:01<00:00,  1.77it/s, actor_loss=-.103, critic_loss=0.0135]\u001b[A\n",
      "Train epoch [1/1]:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:01<00:00,  1.77it/s, actor_loss=-.115, critic_loss=0.00612]\u001b[A\n",
      "Train epoch [1/1]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  1.77it/s, actor_loss=-.115, critic_loss=0.00612]\u001b[A\n",
      "Episode [6/10]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:21<00:00,  7.02s/it]\n",
      "Episode [7/10]:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:12<00:06,  6.37s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=0.00637, critic_loss=0.00312]\u001b[A\n",
      "Train epoch [1/1]:  33%|â–ˆâ–ˆâ–ˆâ–      | 1/3 [00:00<00:01,  1.77it/s, actor_loss=0.00637, critic_loss=0.00312]\u001b[A\n",
      "Train epoch [1/1]:  33%|â–ˆâ–ˆâ–ˆâ–      | 1/3 [00:01<00:01,  1.77it/s, actor_loss=-.0237, critic_loss=0.00181] \u001b[A\n",
      "Train epoch [1/1]:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:01<00:00,  1.78it/s, actor_loss=-.0237, critic_loss=0.00181]\u001b[A\n",
      "Train epoch [1/1]:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:01<00:00,  1.78it/s, actor_loss=0.000133, critic_loss=0.0074]\u001b[A\n",
      "Train epoch [1/1]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  1.78it/s, actor_loss=0.000133, critic_loss=0.0074]\u001b[A\n",
      "Episode [7/10]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:20<00:00,  6.95s/it]\n",
      "Episode [8/10]:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:12<00:06,  6.27s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=0.127, critic_loss=0.012]\u001b[A\n",
      "Train epoch [1/1]:  33%|â–ˆâ–ˆâ–ˆâ–      | 1/3 [00:00<00:01,  1.77it/s, actor_loss=0.127, critic_loss=0.012]\u001b[A\n",
      "Train epoch [1/1]:  33%|â–ˆâ–ˆâ–ˆâ–      | 1/3 [00:01<00:01,  1.77it/s, actor_loss=0.0701, critic_loss=0.00327]\u001b[A\n",
      "Train epoch [1/1]:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:01<00:00,  1.78it/s, actor_loss=0.0701, critic_loss=0.00327]\u001b[A\n",
      "Train epoch [1/1]:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:01<00:00,  1.78it/s, actor_loss=0.115, critic_loss=0.0146]  \u001b[A\n",
      "Train epoch [1/1]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  1.78it/s, actor_loss=0.115, critic_loss=0.0146]\u001b[A\n",
      "Episode [8/10]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:20<00:00,  6.85s/it]\n",
      "Episode [9/10]:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:11<00:05,  5.62s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=-.0152, critic_loss=0.000686]\u001b[A\n",
      "Train epoch [1/1]:  33%|â–ˆâ–ˆâ–ˆâ–      | 1/3 [00:00<00:01,  1.77it/s, actor_loss=-.0152, critic_loss=0.000686]\u001b[A\n",
      "Train epoch [1/1]:  33%|â–ˆâ–ˆâ–ˆâ–      | 1/3 [00:01<00:01,  1.77it/s, actor_loss=-.0118, critic_loss=0.0029]  \u001b[A\n",
      "Train epoch [1/1]:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:01<00:00,  1.77it/s, actor_loss=-.0118, critic_loss=0.0029]\u001b[A\n",
      "Train epoch [1/1]:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:01<00:00,  1.77it/s, actor_loss=-.0123, critic_loss=0.00709]\u001b[A\n",
      "Train epoch [1/1]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  1.77it/s, actor_loss=-.0123, critic_loss=0.00709]\u001b[A\n",
      "Episode [9/10]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:19<00:00,  6.48s/it]\n",
      "Episode [10/10]:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:12<00:06,  6.40s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=-.0572, critic_loss=0.00496]\u001b[A\n",
      "Train epoch [1/1]:  33%|â–ˆâ–ˆâ–ˆâ–      | 1/3 [00:00<00:01,  1.77it/s, actor_loss=-.0572, critic_loss=0.00496]\u001b[A\n",
      "Train epoch [1/1]:  33%|â–ˆâ–ˆâ–ˆâ–      | 1/3 [00:01<00:01,  1.77it/s, actor_loss=-.0612, critic_loss=0.00302]\u001b[A\n",
      "Train epoch [1/1]:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:01<00:00,  1.78it/s, actor_loss=-.0612, critic_loss=0.00302]\u001b[A\n",
      "Train epoch [1/1]:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:01<00:00,  1.78it/s, actor_loss=-.0612, critic_loss=0.00106]\u001b[A\n",
      "Train epoch [1/1]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  1.77it/s, actor_loss=-.0612, critic_loss=0.00106]\u001b[A\n",
      "Episode [10/10]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:19<00:00,  6.40s/it]\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(list_prompt, \n",
    "            num_episodes=10,  \n",
    "            max_timesteps=3,\n",
    "            update_timesteps=3)\n",
    "\n",
    "model.save_pretrained('/KoChatGPT/output_3_PPO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2bba514f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Instruction(ëª…ë ¹ì–´):\n",
      "ë¶ˆê³ ê¸°ìš© ê³ ê¸° í•œìš°ì—ìš”?\n",
      "\n",
      "### Response(ì‘ë‹µ):'ì œê°€ AI ì–´ì‹œìŠ¤í„´íŠ¸ì´ê¸° ë•Œë¬¸ì— ì‹¤ì œë¡œ ë¬´ì—‡ì„ ë¨¹ëŠ”ì§€ì— ëŒ€í•œ ì •ë³´ë¥¼ ê°€ì§€ê³  ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì¼ë°˜ì ìœ¼ë¡œ í•œìš°ê³ ê¸°ëŠ” ì‡ ê³ ê¸°ì˜ ë¶€ë“œëŸ¬ì›€ê³¼ ì‹ê°ì´ ì¢‹ì•„ ë¶ˆê³ ê¸°ì˜ í•œìš°ë¥¼ ì„ í˜¸í•˜ëŠ” ê²ƒìœ¼ë¡œ ì•Œê³  ìˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì¼ë¶€ ê³ ê¸°ë“¤ì€ ë‹¤ë¥¸ ë¶€ìœ„ë³´ë‹¤ ë” í¬ê³  ì´‰ì´‰í•œ ë§›ì„ ëŠë‚„ ìˆ˜ ìˆëŠ” ê²½ìš°ë„ ë§ìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ë³´í†µ ê³ ê¸°ì˜ ë§›ì„ ì¢‹ê²Œ ìœ ì§€í•˜ê¸° ìœ„í•´ì„œëŠ” ê³ ê¸°ì˜ ë§›ì„ ìœ ì§€í•˜ë©´ì„œë„ ì ë‹¹íˆ ì„­ì·¨í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.\n",
      "\n",
      "### Instruction(ëª…ë ¹ì–´):\n",
      "ë¦¬ì²˜ë“œ ë‹‰ìŠ¨ì´ 43ëŒ€ ë¶€í†µë ¹ì§ì„ ìˆ˜í–‰í•œ ë…„ë„ëŠ”?\n",
      "\n",
      "### Response(ì‘ë‹µ):'ë¦¬ì²˜ë“œ ë‹‰ìŠ¨ì€ 63ëŒ€ ë¶€í†µë ¹ì§ì„ ìˆ˜í–‰í•˜ëŠ” ë…„ë„ì— ëŒ€í•œ ì¶”ê°€ ì •ë³´ê°€ ì œê³µë˜ì§€ ì•Šì•˜ê¸° ë•Œë¬¸ì—, ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n",
      "\n",
      "### Instruction(ëª…ë ¹ì–´):\n",
      "ì‹œì¹´ê³  ì˜¤í—¤ì–´ êµ­ì œê³µí•­ì€ ì–´ë””ì— ìˆì–´\n",
      "\n",
      "### Response(ì‘ë‹µ):'ì‹œì¹´ê³  ì˜¤í—¤ì–´ êµ­ì œê³µí•­ì€ ìºë‚˜ë‹¤ì˜ ì˜ìºì£¼ ë° ì˜êµ­ ì§€ì—­ì— ìœ„ì¹˜í•˜ê³  ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "### Instruction(ëª…ë ¹ì–´):\n",
      "ì˜¤ëŠ˜ ë¯¸ì„¸ë¨¼ì§€ ì–´ë•Œ?\n",
      "\n",
      "### Response(ì‘ë‹µ):'ë¯¸ì„¸ë¨¼ì§€ ë¹„ìƒí–‰ë™ìœ¼ë¡œëŠ” í˜¸í¡ ì¡°ì ˆì´ í•„ìš”í•©ë‹ˆë‹¤. í˜¸í¡ ì¡°ì ˆì— ë„ì›€ì„ ì£¼ê¸° ìœ„í•´ í˜¸í¡ê¸° ìš´ë™ì„ ì¶”ì²œë“œë¦½ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "def generation(input_text):\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(\n",
    "        torch.cuda.current_device())\n",
    "    outputs = actor.generate(input_ids,\n",
    "                             max_length=250,\n",
    "                             do_sample=True,\n",
    "                             top_k=50,\n",
    "                             top_p=0.95,\n",
    "                             num_return_sequences=1,\n",
    "                             eos_token_id=eos_token_id,\n",
    "                             pad_token_id=eos_token_id, \n",
    "                             early_stopping=True)\n",
    "    output = tokenizer.batch_decode(outputs[0], skip_special_tokens=True)[0]\n",
    "    print()\n",
    "    print(output)\n",
    "    return output\n",
    "\n",
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"### Instruction(ëª…ë ¹ì–´):\\n{prompt}\\n\\n### Response(ì‘ë‹µ):\"\n",
    "    )\n",
    "}\n",
    "\n",
    "list_prompt = [\n",
    "    'ë¶ˆê³ ê¸°ìš© ê³ ê¸° í•œìš°ì—ìš”?', \n",
    "    'ë¦¬ì²˜ë“œ ë‹‰ìŠ¨ì´ 43ëŒ€ ë¶€í†µë ¹ì§ì„ ìˆ˜í–‰í•œ ë…„ë„ëŠ”?', \n",
    "    'ì‹œì¹´ê³  ì˜¤í—¤ì–´ êµ­ì œê³µí•­ì€ ì–´ë””ì— ìˆì–´',\n",
    "    'ì˜¤ëŠ˜ ë¯¸ì„¸ë¨¼ì§€ ì–´ë•Œ?']\n",
    "\n",
    "list_prompt = [PROMPT_DICT['prompt_input'].format_map({'prompt': tmp}) for tmp in list_prompt]\n",
    "\n",
    "list_result = []\n",
    "\n",
    "for input_text in list_prompt:\n",
    "    output = generation(input_text)\n",
    "    list_result.append(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ad18f6",
   "metadata": {},
   "source": [
    "## PPO - ì •ì„±í‰ê°€\n",
    "ëª¨ë‘ ê¸°ì¡´ì˜ ê²°ê³¼ë³´ë‹¤ ë‚˜ìœ ì‘ë‹µì´ ë‚˜ì˜¨ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.\n",
    "\n",
    "ì´ë¥¼ í†µí•´ RMê³¼ PPOë¥¼ ì‚¬ìš©í•  ë•ŒëŠ” ë§ì€ ë°ì´í„°ë¡œ ì˜¤ë˜ í•™ìŠµì‹œì¼œì•¼ ë” ì¢‹ì€ ê²°ê³¼ë¥¼ ê¸°ëŒ€í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì„ ì•Œê²Œ ë˜ì—ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e6edc474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting generated responses...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b8d3b68808143b0910abb55ec4ef4ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a76f6e4e9d64ad5939133146098238b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.12 seconds, 32.45 sentences/sec\n",
      "\n",
      "ğŸ”¹ ì˜ˆì œ 1\n",
      "  ğŸ“ ì§ˆë¬¸: ë¶ˆê³ ê¸°ìš© ê³ ê¸° í•œìš°ì—ìš”?\n",
      "  ğŸ¤– ì‘ë‹µ: 'ì œê°€ AI ì–´ì‹œìŠ¤í„´íŠ¸ì´ê¸° ë•Œë¬¸ì— ì‹¤ì œë¡œ ë¬´ì—‡ì„ ë¨¹ëŠ”ì§€ì— ëŒ€í•œ ì •ë³´ë¥¼ ê°€ì§€ê³  ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì¼ë°˜ì ìœ¼ë¡œ í•œìš°ê³ ê¸°ëŠ” ì‡ ê³ ê¸°ì˜ ë¶€ë“œëŸ¬ì›€ê³¼ ì‹ê°ì´ ì¢‹ì•„ ë¶ˆê³ ê¸°ì˜ í•œìš°ë¥¼ ì„ í˜¸í•˜ëŠ” ê²ƒìœ¼ë¡œ ì•Œê³  ìˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì¼ë¶€ ê³ ê¸°ë“¤ì€ ë‹¤ë¥¸ ë¶€ìœ„ë³´ë‹¤ ë” í¬ê³  ì´‰ì´‰í•œ ë§›ì„ ëŠë‚„ ìˆ˜ ìˆëŠ” ê²½ìš°ë„ ë§ìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ë³´í†µ ê³ ê¸°ì˜ ë§›ì„ ì¢‹ê²Œ ìœ ì§€í•˜ê¸° ìœ„í•´ì„œëŠ” ê³ ê¸°ì˜ ë§›ì„ ìœ ì§€í•˜ë©´ì„œë„ ì ë‹¹íˆ ì„­ì·¨í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.\n",
      "  âœ… ì •ë‹µ: ì •í™•íˆ ì•Œ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. íŒë§¤ìì—ê²Œ ê³ ê¸°ì˜ ì›ì‚°ì§€ ë° í’ˆì¢…(ì˜ˆ: í•œìš°, ìˆ˜ì…ìœ¡ ë“±)ì„ í™•ì¸í•˜ì„¸ìš”. ëŒ€ë¶€ë¶„ í¬ì¥ì§€ì— í‘œì‹œë¼ ìˆìŠµë‹ˆë‹¤.\n",
      "  ğŸ“Œ BERTScore - Precision: 0.5790, Recall: 0.6249, F1: 0.6011\n",
      "\n",
      "ğŸ”¹ ì˜ˆì œ 2\n",
      "  ğŸ“ ì§ˆë¬¸: ë¦¬ì²˜ë“œ ë‹‰ìŠ¨ì´ 43ëŒ€ ë¶€í†µë ¹ì§ì„ ìˆ˜í–‰í•œ ë…„ë„ëŠ”?\n",
      "  ğŸ¤– ì‘ë‹µ: 'ë¦¬ì²˜ë“œ ë‹‰ìŠ¨ì€ 63ëŒ€ ë¶€í†µë ¹ì§ì„ ìˆ˜í–‰í•˜ëŠ” ë…„ë„ì— ëŒ€í•œ ì¶”ê°€ ì •ë³´ê°€ ì œê³µë˜ì§€ ì•Šì•˜ê¸° ë•Œë¬¸ì—, ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n",
      "  âœ… ì •ë‹µ: ë¦¬ì²˜ë“œ ë‹‰ìŠ¨ì€ 1953ë…„ë¶€í„° 1961ë…„ê¹Œì§€ ì œ43ëŒ€ ë¯¸êµ­ ë¶€í†µë ¹ì´ì—ˆìŠµë‹ˆë‹¤.\n",
      "  ğŸ“Œ BERTScore - Precision: 0.5383, Recall: 0.6378, F1: 0.5838\n",
      "\n",
      "ğŸ”¹ ì˜ˆì œ 3\n",
      "  ğŸ“ ì§ˆë¬¸: ì‹œì¹´ê³  ì˜¤í—¤ì–´ êµ­ì œê³µí•­ì€ ì–´ë””ì— ìˆì–´?\n",
      "  ğŸ¤– ì‘ë‹µ: 'ì‹œì¹´ê³  ì˜¤í—¤ì–´ êµ­ì œê³µí•­ì€ ìºë‚˜ë‹¤ì˜ ì˜ìºì£¼ ë° ì˜êµ­ ì§€ì—­ì— ìœ„ì¹˜í•˜ê³  ìˆìŠµë‹ˆë‹¤.\n",
      "  âœ… ì •ë‹µ: ë¯¸êµ­ ì¼ë¦¬ë…¸ì´ì£¼ ì‹œì¹´ê³  ì‹œ ë¶ì„œìª½, ì‹œ ì™¸ê³½ì˜ ì¿¡ ì¹´ìš´í‹°ì— ìœ„ì¹˜í•´ ìˆìŠµë‹ˆë‹¤.\n",
      "  ğŸ“Œ BERTScore - Precision: 0.6383, Recall: 0.6385, F1: 0.6384\n",
      "\n",
      "ğŸ”¹ ì˜ˆì œ 4\n",
      "  ğŸ“ ì§ˆë¬¸: ì˜¤ëŠ˜ ë¯¸ì„¸ë¨¼ì§€ ì–´ë•Œ?\n",
      "  ğŸ¤– ì‘ë‹µ: 'ë¯¸ì„¸ë¨¼ì§€ ë¹„ìƒí–‰ë™ìœ¼ë¡œëŠ” í˜¸í¡ ì¡°ì ˆì´ í•„ìš”í•©ë‹ˆë‹¤. í˜¸í¡ ì¡°ì ˆì— ë„ì›€ì„ ì£¼ê¸° ìœ„í•´ í˜¸í¡ê¸° ìš´ë™ì„ ì¶”ì²œë“œë¦½ë‹ˆë‹¤.\n",
      "  âœ… ì •ë‹µ: ì‚¬ìš©ìì˜ ì§€ì—­ ì •ë³´ê°€ ì—†ìœ¼ë¯€ë¡œ ì •í™•í•œ ìˆ˜ì¹˜ë¥¼ ì•Œ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í¬í„¸ ì‚¬ì´íŠ¸ë‚˜ í™˜ê²½ë¶€ â€˜ì—ì–´ì½”ë¦¬ì•„â€™ì—ì„œ ì‹¤ì‹œê°„ í™•ì¸ ê°€ëŠ¥í•©ë‹ˆë‹¤.\n",
      "  ğŸ“Œ BERTScore - Precision: 0.5134, Recall: 0.5131, F1: 0.5132\n",
      "\n",
      "ğŸ“Š BERTScore F1 í‰ê· : 0.5841\n"
     ]
    }
   ],
   "source": [
    "# 1. ëª¨ë¸ì´ ìƒì„±í•œ \"ì‘ë‹µ\" ë¶€ë¶„ë§Œ ì¶”ì¶œí•˜ê¸°\n",
    "list_generated_responses = []\n",
    "response_marker = PROMPT_DICT[\"prompt_input\"].split(\"{prompt}\")[1].split(\"### Response(ì‘ë‹µ):\")[0] + \"### Response(ì‘ë‹µ):\" # \"### Response(ì‘ë‹µ):\" ë§ˆì»¤\n",
    "\n",
    "# ì›ë³¸ ì§ˆë¬¸ (ì°¸ì¡° í…ìŠ¤íŠ¸ì™€ ë§¤ì¹­í•˜ê¸° ìœ„í•¨)\n",
    "list_prompt_original = ['ë¶ˆê³ ê¸°ìš© ê³ ê¸° í•œìš°ì—ìš”?',\n",
    "                       'ë¦¬ì²˜ë“œ ë‹‰ìŠ¨ì´ 43ëŒ€ ë¶€í†µë ¹ì§ì„ ìˆ˜í–‰í•œ ë…„ë„ëŠ”?',\n",
    "                       'ì‹œì¹´ê³  ì˜¤í—¤ì–´ êµ­ì œê³µí•­ì€ ì–´ë””ì— ìˆì–´?',\n",
    "                       'ì˜¤ëŠ˜ ë¯¸ì„¸ë¨¼ì§€ ì–´ë•Œ?']\n",
    "\n",
    "print(\"\\nExtracting generated responses...\")\n",
    "for i, full_generated_text in enumerate(list_result):\n",
    "    \n",
    "    # \"### Response(ì‘ë‹µ):\" ë§ˆì»¤ ì´í›„ì˜ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œ\n",
    "    # rfindë¥¼ ì‚¬ìš©í•˜ì—¬ í”„ë¡¬í”„íŠ¸ì™€ ì‘ë‹µ ì‚¬ì´ì— ë§ˆì»¤ê°€ ì—¬ëŸ¬ ë²ˆ ë‚˜ì˜¤ëŠ” ê²½ìš°, ë§ˆì§€ë§‰ ë§ˆì»¤ë¥¼ ê¸°ì¤€ìœ¼ë¡œ í•¨\n",
    "    marker_position = full_generated_text.rfind(response_marker)\n",
    "    \n",
    "    if marker_position != -1:\n",
    "        # ë§ˆì»¤ ë‹¤ìŒë¶€í„° í…ìŠ¤íŠ¸ ì¶”ì¶œ\n",
    "        response_only = full_generated_text[marker_position + len(response_marker):].strip()\n",
    "    else:\n",
    "        # ë§ˆì»¤ë¥¼ ì°¾ì§€ ëª»í•œ ê²½ìš°, í¬ë§·íŒ…ëœ í”„ë¡¬í”„íŠ¸(list_prompt[i]) ì´í›„ì˜ í…ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì˜¤ë ¤ëŠ” ì‹œë„\n",
    "        # ì´ ë°©ë²•ì€ ëª¨ë¸ì´ í”„ë¡¬í”„íŠ¸ë¥¼ ê·¸ëŒ€ë¡œ ë°˜ë³µí•˜ì§€ ì•Šìœ¼ë©´ ì •í™•í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "        try:\n",
    "            response_only = full_generated_text.split(list_prompt[i])[1].strip()\n",
    "        except IndexError:\n",
    "            print(f\"âš ï¸ Warning: ì‘ë‹µ ë§ˆì»¤ ë° í”„ë¡¬í”„íŠ¸ ë¶„ë¦¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ìƒ˜í”Œ {i}ëŠ” ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\")\n",
    "            response_only = full_generated_text # ìµœí›„ì˜ ìˆ˜ë‹¨ (ê°œì„  í•„ìš”)\n",
    "\n",
    "    # eos_token (ì˜ˆ: '\\n' ë˜ëŠ” tokenizer.eos_token) ì •ë¦¬\n",
    "    # generation_argsì˜ eos_token_idê°€ 375 ('\\n')ë¡œ ë˜ì–´ ìˆìœ¼ë¯€ë¡œ, ì´ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì²˜ë¦¬\n",
    "    # tokenizer.eos_token (ì˜ˆ: '</s>')ë„ í•¨ê»˜ ê³ ë ¤í•˜ë©´ ì¢‹ìŠµë‹ˆë‹¤.\n",
    "    if tokenizer.eos_token: # tokenizer ë³€ìˆ˜ê°€ ì‚¬ìš© ê°€ëŠ¥í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "         response_only = response_only.replace(tokenizer.eos_token, \"\")\n",
    "    \n",
    "    # eos_token_id=375ê°€ '\\n'ì´ë¼ê³  ê°€ì •í•˜ê³ , ì²« ì¤„ë°”ê¿ˆ ì´ì „ í…ìŠ¤íŠ¸ë§Œ ì‚¬ìš©í•˜ê±°ë‚˜ ë¶ˆí•„ìš”í•œ ì¤„ë°”ê¿ˆ ì œê±°\n",
    "    response_only = response_only.split('\\n')[0].strip()\n",
    "    \n",
    "    list_generated_responses.append(response_only)\n",
    "    \n",
    "list_references = [\n",
    "    \"ì •í™•íˆ ì•Œ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. íŒë§¤ìì—ê²Œ ê³ ê¸°ì˜ ì›ì‚°ì§€ ë° í’ˆì¢…(ì˜ˆ: í•œìš°, ìˆ˜ì…ìœ¡ ë“±)ì„ í™•ì¸í•˜ì„¸ìš”. ëŒ€ë¶€ë¶„ í¬ì¥ì§€ì— í‘œì‹œë¼ ìˆìŠµë‹ˆë‹¤.\",\n",
    "    \"ë¦¬ì²˜ë“œ ë‹‰ìŠ¨ì€ 1953ë…„ë¶€í„° 1961ë…„ê¹Œì§€ ì œ43ëŒ€ ë¯¸êµ­ ë¶€í†µë ¹ì´ì—ˆìŠµë‹ˆë‹¤.\",\n",
    "    \"ë¯¸êµ­ ì¼ë¦¬ë…¸ì´ì£¼ ì‹œì¹´ê³  ì‹œ ë¶ì„œìª½, ì‹œ ì™¸ê³½ì˜ ì¿¡ ì¹´ìš´í‹°ì— ìœ„ì¹˜í•´ ìˆìŠµë‹ˆë‹¤.\",\n",
    "    \"ì‚¬ìš©ìì˜ ì§€ì—­ ì •ë³´ê°€ ì—†ìœ¼ë¯€ë¡œ ì •í™•í•œ ìˆ˜ì¹˜ë¥¼ ì•Œ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í¬í„¸ ì‚¬ì´íŠ¸ë‚˜ í™˜ê²½ë¶€ â€˜ì—ì–´ì½”ë¦¬ì•„â€™ì—ì„œ ì‹¤ì‹œê°„ í™•ì¸ ê°€ëŠ¥í•©ë‹ˆë‹¤.\"\n",
    "]\n",
    "\n",
    "# BERTScore ê³„ì‚° (ê¸°ë³¸ ëª¨ë¸: roberta-large, í•œêµ­ì–´ëŠ” ì•„ë˜ ì°¸ê³ )\n",
    "P, R, F1 = score(list_generated_responses, list_references, model_type=\"klue/roberta-base\", num_layers=12, verbose=True)\n",
    "\n",
    "# ê° ë¬¸ì¥ë³„ ì ìˆ˜ ì¶œë ¥\n",
    "for i, (p, r, f1) in enumerate(zip(P, R, F1)):\n",
    "    print(f\"\\nğŸ”¹ ì˜ˆì œ {i+1}\")\n",
    "    print(f\"  ğŸ“ ì§ˆë¬¸: {list_prompt_original[i]}\")\n",
    "    print(f\"  ğŸ¤– ì‘ë‹µ: {list_generated_responses[i]}\")\n",
    "    print(f\"  âœ… ì •ë‹µ: {list_references[i]}\")\n",
    "    print(f\"  ğŸ“Œ BERTScore - Precision: {p:.4f}, Recall: {r:.4f}, F1: {f1:.4f}\")\n",
    "    \n",
    "# í‰ê·  F1 ì¶œë ¥\n",
    "print(f\"\\nğŸ“Š BERTScore F1 í‰ê· : {F1.mean().item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e913cd6",
   "metadata": {},
   "source": [
    "## PPO - BERTScore í‰ê°€\n",
    "\n",
    "PPOëª¨ë¸ë³´ë‹¤ SFTëª¨ë¸ì´ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì´ê³  ìˆë‹¤.\n",
    "\n",
    "ì •ì„±í‰ê°€ ì •ëŸ‰í‰ê°€ê°€ ì–´ëŠì •ë„ ì¼ì¹˜í•œë‹¤ëŠ”ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆì—ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914124b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
