{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb23b7cf",
   "metadata": {},
   "source": [
    "# HuggingFace 커스텀 프로젝트 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b25d6e5",
   "metadata": {},
   "source": [
    "## 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4765df1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset nsmc (/aiffel/.cache/huggingface/datasets/nsmc/default/1.1.0/bfd4729bf1a67114e5267e6916b9e4807010aeb238e4a3c2b95fbfa3a014b5f3)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71463811ce474a78b04f0d7bb294a3dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'document', 'label'],\n",
       "        num_rows: 150000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'document', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, TrainingArguments, Trainer, AutoModelForSequenceClassification\n",
    "\n",
    "raw_datasets = load_dataset(\"nsmc\")\n",
    "\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90afb751",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '9976970', 'document': '아 더빙.. 진짜 짜증나네요 목소리', 'label': 0}\n",
      "{'id': '3819312', 'document': '흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나', 'label': 1}\n",
      "{'id': '10265843', 'document': '너무재밓었다그래서보는것을추천한다', 'label': 0}\n",
      "{'id': '9045019', 'document': '교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정', 'label': 0}\n",
      "{'id': '6483659', 'document': '사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 던스트가 너무나도 이뻐보였다', 'label': 1}\n"
     ]
    }
   ],
   "source": [
    "for example in raw_datasets[\"train\"].select(range(5)):\n",
    "    print(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735541a0",
   "metadata": {},
   "source": [
    "## 데이터 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5702c8",
   "metadata": {},
   "source": [
    "### 1. 빈 문장 제거 & 중복 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fa5cff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /aiffel/.cache/huggingface/datasets/nsmc/default/1.1.0/bfd4729bf1a67114e5267e6916b9e4807010aeb238e4a3c2b95fbfa3a014b5f3/cache-c259e9100db70862.arrow\n",
      "Loading cached processed dataset at /aiffel/.cache/huggingface/datasets/nsmc/default/1.1.0/bfd4729bf1a67114e5267e6916b9e4807010aeb238e4a3c2b95fbfa3a014b5f3/cache-b7421c716a6e4f65.arrow\n",
      "Loading cached processed dataset at /aiffel/.cache/huggingface/datasets/nsmc/default/1.1.0/bfd4729bf1a67114e5267e6916b9e4807010aeb238e4a3c2b95fbfa3a014b5f3/cache-22e8c42e3b2ed5b5.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebc20f5efef2447bbc2045495360b6a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1. 빈 문장 제거\n",
    "def is_not_empty(example):\n",
    "    return example[\"document\"] is not None and example[\"document\"].strip() != \"\"\n",
    "\n",
    "raw_datasets = raw_datasets.filter(is_not_empty)\n",
    "\n",
    "# 2. 중복 제거 (document 기준)\n",
    "# Hugging Face Datasets는 내부적으로 pandas 사용 불가하므로 unique 처리 수작업 필요\n",
    "seen = set()\n",
    "def is_unique(example):\n",
    "    doc = example[\"document\"]\n",
    "    if doc in seen:\n",
    "        return False\n",
    "    seen.add(doc)\n",
    "    return True\n",
    "\n",
    "raw_datasets = raw_datasets.filter(is_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9c4d8c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size after filtering: 146182\n",
      "Test size after filtering: 49157\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train size after filtering: {len(raw_datasets['train'])}\")\n",
    "print(f\"Test size after filtering: {len(raw_datasets['test'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac7f079",
   "metadata": {},
   "source": [
    "### 2. 여러 데이터 전처리 기법 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "482d166b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 전처리 함수\n",
    "from datasets import DatasetDict\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.strip().lower()\n",
    "\n",
    "    # 여러 공백 정리\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "\n",
    "    # 특수문자 정리 (한글 자모 포함)\n",
    "    sentence = re.sub(r\"[^가-힣ㄱ-ㅎㅏ-ㅣa-zA-Z0-9?.!,♡♥^;]+\", \" \", sentence)\n",
    "\n",
    "    # 허용된 자모(ㅋ, ㅜ, ㅠ, ㄷ, ㅡ, ㅎ, ㅉ) 외 자모 제거\n",
    "    sentence = re.sub(r'[ㄱ-ㅎㅏ-ㅣ]', lambda m: m.group() if m.group() in ['ㅋ', 'ㅜ', 'ㅠ', 'ㄷ', 'ㅡ', 'ㅎ', 'ㅉ'] else '', sentence)\n",
    "\n",
    "    # 연속된 구두점 2개 이상 → 2개로 축소\n",
    "    sentence = re.sub(r'([?.!,])\\1{2,}', r'\\1\\1', sentence)\n",
    "\n",
    "    # 연속 자음/모음 축소 (허용된 것들만)\n",
    "    sentence = re.sub(r'(ㅋ)\\1{1,}', r'\\1\\1', sentence)\n",
    "    sentence = re.sub(r'(ㅎ)\\1{1,}', r'\\1\\1', sentence)\n",
    "    for ch in ['ㅜ', 'ㅠ', 'ㄷ', 'ㅡ', 'ㅉ']:\n",
    "        sentence = re.sub(fr'{ch}+', ch * 2, sentence)\n",
    "\n",
    "    # ♡ → ♥\n",
    "    sentence = sentence.replace('♡', '♥')\n",
    "\n",
    "    # ♥ 연속 → 하나만\n",
    "    sentence = re.sub(r'(♥)\\1+', r'\\1', sentence)\n",
    "\n",
    "    # ^^는 2개만 유지\n",
    "    sentence = re.sub(r'\\^{3,}', '^^', sentence)\n",
    "    sentence = re.sub(r'\\^{1,2}', lambda m: '^^' if len(m.group()) == 2 else '', sentence)\n",
    "\n",
    "    # ㅅㅂ, ㅈㄴ → OO\n",
    "    sentence = re.sub(r'(ㅅㅂ|ㅈㄴ)', 'OO', sentence)\n",
    "\n",
    "    # O 연속 → 2개만\n",
    "    sentence = re.sub(r'(O)\\1{2,}', r'\\1\\1', sentence)\n",
    "\n",
    "    # ; 연속 → 하나만\n",
    "    sentence = re.sub(r';{2,}', ';', sentence)\n",
    "\n",
    "    return sentence.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b60efec9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "619dfd3c1b614b6bb76680cb6be54450",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/146182 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "601b1f8ae3db47119d151380a4a3037e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/49157 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# map에 사용할 래퍼 함수 정의\n",
    "def preprocess_function(example):\n",
    "    example[\"document\"] = preprocess_sentence(example[\"document\"])\n",
    "    return example\n",
    "\n",
    "# 데이터셋 전체에 적용\n",
    "raw_datasets = raw_datasets.map(preprocess_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "462bfeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# 문장 길이 1 & 숫자 문장만 제거하는 필터 함수\n",
    "def not_single_digit(example):\n",
    "    doc = str(example[\"document\"]).strip()\n",
    "    return not (len(doc) == 1 and doc.isdigit())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4968e8ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0c73b0a1841434d9ba0655a7e99f918",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/147 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "삭제된 숫자 문장 수: 5\n"
     ]
    }
   ],
   "source": [
    "# train split에만 적용\n",
    "original_len = len(raw_datasets[\"train\"])\n",
    "raw_datasets[\"train\"] = raw_datasets[\"train\"].filter(not_single_digit)\n",
    "new_len = len(raw_datasets[\"train\"])\n",
    "\n",
    "print(f\"삭제된 숫자 문장 수: {original_len - new_len}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1194c643",
   "metadata": {},
   "source": [
    "## 토크나이저와 모델 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f24d2962",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72e69a40a8a04c798549ca6deb6d55c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/147 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5fa1885fc364881a60a828c6bf1dd00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name_or_path = \"klue/bert-base\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name_or_path, num_labels=2)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"document\"], truncation=True)\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9954a109",
   "metadata": {},
   "source": [
    "### 데이터 토큰 길이 측정 후 적당한 길이로 패딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30df3277",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZkAAAEWCAYAAAC0Q+rDAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAmRUlEQVR4nO3de5xVdb3/8ddbJRFFQFBTwMBL5YVCJS8nT6mlgnXUPGXaRTSP5E87WcdMKc3SSqujKSfvSWKmZqZpZgmZl0pRgUhRUdA0BlEQRfGa6Of3x/puXQz7NsOs2Xv2vJ+Px37M3t+11nd99toz89nfy1pLEYGZmVkR1mh0AGZm1rqcZMzMrDBOMmZmVhgnGTMzK4yTjJmZFcZJxszMCuMkY9aLSNpdUlsX1/kNST/twvpelLR5en6ppO92Yd0XSDq5q+qz2pxkegFJj0v6aDfs59uSLq+xzm6S7pT0vKRnJf1V0geKjq0ZSApJW/akfUq6TdKrkpZLekHSTEknSlq7tE5EfD8i/qvOumquFxHrRcRjnY05t7/DJP2lXd1HRcRpq1u31c9JxrqNpPWBG4H/AzYAhgLfAV5rZFxW05cioj+wCXAccDBwkyR15U4krdWV9VlzcJLpZUrf7iT9r6TnJP1D0rjc8tsknS7pnvTN9XpJG6Rlq3S1lFpJksYC3wA+nbo7/l5m9+8GiIgrI+KNiHglIqZGxH25+r4g6aEU282S3pVbtpekuakV9BNJt5e+GbdvRUkakb7Fr5VeD5B0iaRFkhZK+q6kNes8JhtI+pmkJ9Py3+SWfVzSbEnLUgvtfZ34TNZO+/6npKdTl846+WMu6ThJi1P8h+e2HSzpt+mzuje9r7+kZXek1f6ePpNP57YrW181EfFSRNwG7AfsCnws1fXWsZfUV9LlkpamY3KvpI0lfQ/4d+AnKZafpPVD0jGS5gHzcmX51tcQSdNSa+r20u9E+884ld0m6b8kbQ1cAOya9rcsLV+p+03SkZLmK2tV3yBp09yykHSUpHnpvZwrdW1i7Q2cZHqnnYGHgSHAD4FL2v3xHAp8geyb6wpgUq0KI+IPwPeBX6bujveXWe0R4A1JUySNkzQov1DS/mSJ6kBgQ+DPwJVp2RDgWuCkFPejwAfrfsdwaXovWwLbA3sD+a6basfk50A/YFtgI+DHKabtgcnAF4HBwIXADcp1JdXpDLIEPDrFNxT4Vm75O4EBqfwI4NzcsTsXeCmtMz49AIiID6Wn70+fyS/rqK+miPgnMIMsabQ3PtU9nOyYHAW8EhHfJPs8v5Ri+VJumwPIjv82FXb5WeA0ss9mNvCLOmJ8KO37rrS/ge3XkbQncDpwENnv+hPAVe1W+zjwAeB9ab19au3bVuYk0zs9EREXR8QbwBSyP7CNc8t/HhFzIuIl4GTgoNK3/tURES8AuwEBXAwsSd8eS/s+Cjg9Ih6KiBVkSWt0+ua6L/BARFwTEa8DZwNP1bPfVP++wFfSt/HFZIni4NxqZY+JpE2AccBREfFcRLweEbenbSYAF0bE3allNoWs62+Xeo9JSmQTgK9GxLMRsTy973xsrwOnpn3fBLwIvCd9Jv8JnBIRL0fEgyn2WsrWV2/MyZNkXZ7l6h4MbJmOycz0uVdzenrvr1RY/ruIuCMiXgO+SdY6Gd7BeMv5LDA5Imaluiemukfk1jkjIpalxHor2RcB6wAnmd7prX/OEfFyerpebvmC3PMngD5k3yJXW0ogh0XEMGA7YFOyhAHwLuCc1DWxDHgWENk37k3zcUV2Zdd8nNW8K72HRbm6LyRrlZRUOibDgWcj4rkK9R5XqjPVOzzFWq8NyVpJM3N1/CGVlyxNSbfk5RTbhsBarHwc6jkmlerriKFkn097PwduBq5K3Ys/lNSnRl21Ys5/7i+m/XbkGFeyKdnvd77upWTvrST/RaYzx6nXc5KxcvLfEjcj+3b6DFm3TL/SgvRNOv/PsEOX9I6IuWTdWNulogXAFyNiYO6xTkTcCSzKx5VaAPk4V4qNrEuoZAFZC2NIrt71I2LbOsJcAGwgaWCFZd9rF2+/iLiyjnpLngFeAbbN1TEgIur5Z7aErAtwWK6sK77hV5VaETuSdX+tJLWOvhMR2wD/RtbddGhpcYUqa/3e5D/39chaUE+SfeZQ+XOvVe+TZF8USnWvS9YKW1hjO+sAJxkr53OStpHUDzgVuCZ1Iz0C9JX0sfTt9CQgP/7wNDBCUtnfK0nvTQPOw9Lr4cAhwPS0ygXAREnbpuUDJH0qLfsdsK2kA9NA75dZ+R/KbOBDkjaTNICs6wOAiFgETAXOlLS+pDUkbSHpw7UORNr298B5kgZJ6iOpNNZxMXCUpJ2VWTcdm/5VqnxHGhzvK6kvWUvtYuDHkjZK73uopJp9/+kzuRb4tqR+kt7L2//QS54GNq9VVz3SPj4MXA/cA9xUZp09JI1KX0BeIPuC8uZqxrKvsqnv7yAbm5keEQsiYglZQvicpDUlfQHYIrfd08CwtF05VwKHSxqdxtG+D9wdEY93IkarwEnGyvk5WQvjKaAv2T90IuJ54Gjgp2R/3C8B+dlmv0o/l0qaVabe5WQDvHdLeoksucwhmxZLRFwH/ICsq+WFtGxcWvYM8CmyQfKlwFbAX0sVR8Q04JfAfcBMsqnSeYcC7wAeBJ4DriEbd6nH58n+Wc4FFgNfSfucARwJ/CTVOR84rEZdD5C1XEqPw4ET0rbT0/v+I/WPkXyJbKD9KbLP7UpWnhL+bWBK6oo7qM462/uJpOVk/7TPBn4NjI2IN8us+06yY/sC8BBwe4oL4Bzgk8pm6NWcTJJzBXAKWTfZjsDncsuOBI4n+53YFrgzt+xPZMf7KUnPtK80Iv5INub4a7KW8hasPBZmXUC+aZnlSboNuDwiuuwM7qL0pFi7i6QfAO+MiPE1VzbrBm7JmPVgqQvyfam7bieyKcnXNTousxKfYWvWs/Un6yLblKw760yyMROzpuDuMjMzK4y7y8zMrDC9rrtsyJAhMWLEiEaHYWbWo8ycOfOZiNiw9por63VJZsSIEcyYMaPRYZiZ9SiSnqi91qrcXWZmZoVxkjEzs8I4yZiZWWF63ZiMmVlHvf7667S1tfHqq682OpTC9e3bl2HDhtGnT62LZ9fHScbMrIa2tjb69+/PiBEjaOWbY0YES5cupa2tjZEjR3ZJne4uMzOr4dVXX2Xw4MEtnWAAJDF48OAubbE5yZiZ1aHVE0xJV79PJxkzMyuMx2TMzDpo4rX3d2l9px84quryZcuWccUVV3D00Ud3qN59992XK664goEDB65GdKvHSaYBKv2C1vpFM7PeadmyZZx33nmrJJkVK1aw1lqV/43fdNMqNy/tdk4yZmZN7sQTT+TRRx9l9OjR9OnTh759+zJo0CDmzp3LI488wgEHHMCCBQt49dVXOfbYY5kwYQLw9mW0XnzxRcaNG8duu+3GnXfeydChQ7n++utZZ511Co/dYzJmZk3ujDPOYIsttmD27Nn86Ec/YtasWZxzzjk88sgjAEyePJmZM2cyY8YMJk2axNKlS1epY968eRxzzDE88MADDBw4kF//+tfdErtbMmZmPcxOO+200nkskyZN4rrrshuiLliwgHnz5jF48OCVthk5ciSjR48GYMcdd+Txxx/vllidZMzMeph11133ree33XYbf/zjH7nrrrvo168fu+++e9nzXNZee+23nq+55pq88sor3RKru8vMzJpc//79Wb58edllzz//PIMGDaJfv37MnTuX6dOnd3N01bklY2bWQd09E3Tw4MF88IMfZLvttmOdddZh4403fmvZ2LFjueCCC9h66615z3vewy677NKtsdXiJGNm1gNcccUVZcvXXnttfv/735ddVhp3GTJkCHPmzHmr/Gtf+1qXx1eJu8vMzKwwTjJmZlYYJxkzMyuMk4yZmRXGScbMzApTWJKRNFzSrZIelPSApGNT+QaSpkmal34OSuWSNEnSfEn3SdohV9f4tP48SeNz5TtKuj9tM0m95YYPZmY9RJFTmFcAx0XELEn9gZmSpgGHAbdExBmSTgROBE4AxgFbpcfOwPnAzpI2AE4BxgCR6rkhIp5L6xwJ3A3cBIwFys/lMzPrKr89tmvr+49zqi7u7KX+Ac4++2wmTJhAv379OhvdaimsJRMRiyJiVnq+HHgIGArsD0xJq00BDkjP9wcui8x0YKCkTYB9gGkR8WxKLNOAsWnZ+hExPSICuCxXl5lZyyhd6r8zzj77bF5++eUujqh+3XIypqQRwPZkLY6NI2JRWvQUUDp1dSiwILdZWyqrVt5Wprzc/icAEwA222yz1XgnZmbdL3+p/7322ouNNtqIq6++mtdee41PfOITfOc73+Gll17ioIMOoq2tjTfeeIOTTz6Zp59+mieffJI99tiDIUOGcOutt3Z77IUnGUnrAb8GvhIRL+SHTSIiJEXRMUTERcBFAGPGjCl8f2ZmXemMM85gzpw5zJ49m6lTp3LNNddwzz33EBHst99+3HHHHSxZsoRNN92U3/3ud0B2TbMBAwZw1llnceuttzJkyJCGxF7o7DJJfcgSzC8i4tpU/HTq6iL9XJzKFwLDc5sPS2XVyoeVKTcza1lTp05l6tSpbL/99uywww7MnTuXefPmMWrUKKZNm8YJJ5zAn//8ZwYMGNDoUIFiZ5cJuAR4KCLOyi26ASjNEBsPXJ8rPzTNMtsFeD51q90M7C1pUJqJtjdwc1r2gqRd0r4OzdVlZtaSIoKJEycye/ZsZs+ezfz58zniiCN497vfzaxZsxg1ahQnnXQSp556aqNDBYptyXwQ+Dywp6TZ6bEvcAawl6R5wEfTa8hmhz0GzAcuBo4GiIhngdOAe9Pj1FRGWuenaZtH8cwyM2tB+Uv977PPPkyePJkXX3wRgIULF7J48WKefPJJ+vXrx+c+9zmOP/54Zs2atcq2jVDYmExE/AWodN7KR8qsH8AxFeqaDEwuUz4D2G41wjQz67gaU467Wv5S/+PGjeMzn/kMu+66KwDrrbcel19+OfPnz+f4449njTXWoE+fPpx//vkATJgwgbFjx7Lppps2ZOBf2f/23mPMmDExY8aMhsYw8dr7y5Z39z0qzKw+Dz30EFtvvXWjw+g25d6vpJkRMaajdfmyMmZmVhgnGTMzK4yTjJlZHXrL0EJXv08nGTOzGvr27cvSpUtbPtFEBEuXLqVv375dVme3XFbGzKwnGzZsGG1tbSxZsqTRoRSub9++DBs2rPaKdXKSMTOroU+fPowcObLRYfRITjJNxFObzazVeEzGzMwK4yRjZmaFcZIxM7PCOMmYmVlhnGTMzKwwTjJmZlYYJxkzMyuMk4yZmRXGScbMzArjJGNmZoVxkjEzs8I4yZiZWWGcZMzMrDBOMmZmVhgnGTMzK4yTjJmZFcZJxszMCuMkY2ZmhXGSMTOzwjjJmJlZYZxkzMysMDWTjKRjJa2vzCWSZknauzuCMzOznq2elswXIuIFYG9gEPB54IxCozIzs5ZQT5JR+rkv8POIeCBXZmZmVlE9SWampKlkSeZmSf2BN4sNy8zMWsFadaxzBDAaeCwiXpY0GDi80KjMzKwl1NOSCWAb4Mvp9bpA38IiMjOzllFPkjkP2BU4JL1eDpxbWERmZtYy6uku2zkidpD0N4CIeE7SOwqOy8zMWkA9LZnXJa1J1m2GpA3xwL+ZmdWhniQzCbgO2EjS94C/AN8vNCozM2sJNZNMRPwC+DpwOrAIOCAiflVrO0mTJS2WNCdX9m1JCyXNTo99c8smSpov6WFJ++TKx6ay+ZJOzJWPlHR3Kv+lu/DMzJpPxSQjaYPSA1gMXAlcATydymq5FBhbpvzHETE6PW5K+9oGOBjYNm1znqQ1UzfducA4shluh6R1AX6Q6toSeI5sqrWZmTWRagP/M8nGYcqd3R/A5tUqjog7JI2oM479gasi4jXgH5LmAzulZfMj4jEASVcB+0t6CNgT+ExaZwrwbeD8OvdnZmbdoGKSiYiRBe3zS5IOBWYAx0XEc8BQYHpunbZUBrCgXfnOwGBgWUSsKLP+KiRNACYAbLbZZl3xHszMrA51Xepf0oGSzpJ0pqQDVmN/5wNbkF1BYBFw5mrUVbeIuCgixkTEmA033LA7dmlmZtRxnoyk84AtycZkAI6StFdEHNPRnUXE07l6LwZuTC8XAsNzqw5LZVQoXwoMlLRWas3k1zczsyZRz8mYewJbR0TpPJkpwAOd2ZmkTSJiUXr5CaA08+wG4ApJZwGbAlsB95CNB20laSRZEjkY+ExEhKRbgU8CVwHjges7E5OZmRWnniQzH9gMeCK9Hp7KqpJ0JbA7MERSG3AKsLuk0WQTBx4HvggQEQ9Iuhp4EFgBHBMRb6R6vgTcDKwJTE63GgA4AbhK0neBvwGX1PFezMysG9WTZPoDD0m6J73+ADBD0g0AEbFfuY0i4pAyxRUTQUR8D/hemfKbgJvKlD/G2zPQeqWJ195fcdnpB47qxkjMzMqrJ8l8q/AozMysJdVMMhFxO4Ck9fPrR8SzBcZlZmYtoJ7ZZROAU4FXyS6MKeo4GdPMzKye7rLjge0i4pmigzEzs9ZSz8mYjwIvFx2ImZm1nnpaMhOBOyXdDbxWKoyIL1fexMzMrL4kcyHwJ+B+fLMyMzPrgHqSTJ+I+J/CIzEzs5ZTz5jM7yVNkLRJu3vMmJmZVVVPS6Z05v7EXJmnMJuZWU31nIxZ1H1lzMysxdXTkkHSdmS3P+5bKouIy4oKyszMWkM9Z/yfQnY15W3ILlQ5DvgL4CRjZmZV1TPw/0ngI8BTEXE48H5gQKFRmZlZS6gnybwSEW8CK9JFMhez8t0qzczMyqpnTGaGpIHAxcBM4EXgriKDMjOz1lDP7LKj09MLJP0BWD8i7is2LDMzawUVk4ykdwHLIuL59HoP4ADgCUlzI+Jf3ROimZn1VNXGZK4G1gWQNBr4FfBPsoH/8wqPzMzMerxq3WXrRMST6fnngMkRcaakNYDZhUdmZmY9XrWWjHLP9wRuAUgzzczMzGqq1pL5k6SrgUXAILLL/SNpE8DjMWZmVlO1JPMV4NPAJsBuEfF6Kn8n8M2C42oJE6+9v9EhmJk1VMUkExEBXFWm/G+FRmRdolKCO/3AUd0ciZn1ZvWc8W9mZtYpTjJmZlaYiklG0i3p5w+6LxwzM2sl1Qb+N5H0b8B+kq5i5SnNRMSsQiMzM7Mer1qS+RZwMjAMOKvdsiA7d8bMzKyiarPLrgGukXRyRJzWjTGZmVmLqOcqzKdJ2g/4UCq6LSJuLDYsMzNrBTVnl0k6HTgWeDA9jpX0/aIDMzOznq+em5Z9DBhdumaZpCnA34BvFBmYmZn1fPWeJzMw93xAAXGYmVkLqqclczrwN0m3kk1j/hBwYqFRmZlZS6hn4P9KSbcBH0hFJ0TEU4VGZSvxhTbNrKeqpyVDRCwCbig4FjMzazG+dpmZmRWmsCQjabKkxZLm5Mo2kDRN0rz0c1Aql6RJkuZLuk/SDrltxqf150kanyvfUdL9aZtJkoSZmTWVqklG0pqS5nay7kuBse3KTgRuiYityG7nXJpAMA7YKj0mAOen/W8AnALsDOwEnFJKTGmdI3Pbtd+XmZk1WNUkExFvAA9L2qyjFUfEHcCz7Yr3B6ak51OAA3Lll0VmOjAw3eZ5H2BaRDwbEc8B04Cxadn6ETE93VztslxdZmbWJOoZ+B8EPCDpHuClUmFE7NeJ/W2cJhEAPAVsnJ4PBRbk1mtLZdXK28qUm5lZE6knyZxcxI4jIiRFEXW3J2kCWTccm23W4UaZmZl1Us2B/4i4HXgc6JOe3wt09l4yT6euLtLPxal8ITA8t96wVFatfFiZ8krv4aKIGBMRYzbccMNOhm5mZh1VzwUyjwSuAS5MRUOB33RyfzcApRli44Hrc+WHpllmuwDPp261m4G9JQ1KA/57AzenZS9I2iXNKjs0V5eZmTWJerrLjiGb2XU3QETMk7RRrY0kXQnsDgyR1EY2S+wM4GpJRwBPAAel1W8C9gXmAy8Dh6d9PSvpNLLWE8CpEVGaTHA02Qy2dYDfp0dD+Ix8M7Py6kkyr0XEv0qnoUhai+zOmFVFxCEVFn2kzLpBlszK1TMZmFymfAawXa04zMysceo5GfN2Sd8A1pG0F/Ar4LfFhmVmZq2gniRzIrAEuB/4IlnX1klFBmVmZq2hnqswv5luVHY3WTfZw6l7y8zMrKqaSUbSx4ALgEfJ7iczUtIXI6JhA+1mZtYz1DPwfyawR0TMB5C0BfA7Gjiby8zMeoZ6xmSWlxJM8hiwvKB4zMyshVRsyUg6MD2dIekm4GqyMZlP8fZ5K2ZmZhVV6y77j9zzp4EPp+dLyE6ANDMzq6pikomIw7szEDMzaz31zC4bCfw3MCK/ficv9W9mZr1IPbPLfgNcQnaW/5uFRmNmZi2lniTzakRMKjwSMzNrOfUkmXMknQJMBV4rFUZEZ+8pY2ZmvUQ9SWYU8HlgT97uLov02szMrKJ6ksyngM0j4l9FB2NmZq2lniQzBxjI27dKthZU6cZrpx84qpsjMbNWUk+SGQjMlXQvK4/JeAqzmZlVVU+SOaXwKMzMrCXVcz+Z27sjEDMzaz31nPG/nGw2GcA7gD7ASxGxfpGBmZlZz1dPS6Z/6bkkAfsDuxQZlJmZtYZ67ifzlsj8BtinmHDMzKyV1NNddmDu5RrAGODVwiIyM7OWUc/ssvx9ZVYAj5N1mZmZmVVVz5iM7ytjZmadUu32y9+qsl1ExGkFxGNmZi2kWkvmpTJl6wJHAIMBJ5keqNLlY8zMilDt9stnlp5L6g8cCxwOXAWcWWk7MzOzkqpjMpI2AP4H+CwwBdghIp7rjsDMzKznqzYm8yPgQOAiYFREvNhtUZmZWUuodjLmccCmwEnAk5JeSI/lkl7onvDMzKwnqzYm06GrAZiZmbXnRGJmZoWp54x/a5AD2n64Stlvhn29AZGYmXWOWzJmZlYYJxkzMyuMk4yZmRXGScbMzArjJGNmZoVpyOwySY8Dy4E3gBURMSZdwuaXwAiye9YcFBHPpVs+nwPsC7wMHBYRs1I948lOFgX4bkRM6c73UU25mWHg2WFm1rs0siWzR0SMjogx6fWJwC0RsRVwS3oNMA7YKj0mAOfDW9dVOwXYGdgJOEXSoG6M38zMamim82T2B3ZPz6cAtwEnpPLLIiKA6ZIGStokrTstIp4FkDQNGAtc2b1hd0xPO/el0q0BTj9wVDdHYmY9UaNaMgFMlTRT0oRUtnFELErPnwI2Ts+HAgty27alskrlq5A0QdIMSTOWLFnSVe/BzMxqaFRLZreIWChpI2CapLn5hRERkqKrdhYRF5FdTZoxY8Z0Wb1mZlZdQ1oyEbEw/VwMXEc2pvJ06gYj/VycVl8IDM9tPiyVVSo3M7Mm0e1JRtK66U6bSFoX2BuYA9wAjE+rjQeuT89vAA5VZhfg+dStdjOwt6RBacB/71RmZmZNohHdZRsD12Uzk1kLuCIi/iDpXuBqSUcATwAHpfVvIpu+PJ9sCvPhABHxrKTTgHvTeqeWJgGYmVlz6PYkExGPAe8vU74U+EiZ8gCOqVDXZGByV8fY3SqdU7O6dTTzrDUz6x18xr+ZmRXGScbMzArjJGNmZoVxkjEzs8I4yZiZWWGcZMzMrDBOMmZmVhgnGTMzK4yTjJmZFaaZ7idjdeiKqwOYmXUXt2TMzKwwTjJmZlYYJxkzMyuMx2SsS0289v6y5acfOKqbIzGzZuCWjJmZFcZJxszMCuMkY2ZmhfGYTAurdE5NuTtmdmRdqDz2YmaW55aMmZkVxknGzMwK4yRjZmaF8ZhML+Trn5lZd3GSsYbyyZtmrc1JpgPK/UM8oO2HHND9oZiZ9QgekzEzs8K4JWNNyd1oZq3BSca6hU/eNOud3F1mZmaFcUvGWoK718yak5OMtbRq3XROQGbFc5Kxqjpy4mali2maWe/lJGPWTrN1vTVbPGYd4SRjPUorz1Jr5fdmvZeTjFmdOtqiaFQLxC0fayZOMh3gC0uamXWMk4x1mXJJuNJkgI6sW5Se3j3V0+O33kER0egYutWYMWNixowZndr27kmf7+JoLK8jCana+tZx7kqzWiTNjIgxHd3OLRnrFZqh5WTWGznJWNNoljEvJySzrtPjk4ykscA5wJrATyPijAaHZN1kdZNSUUmtI917XbFuvdtX09HxHXevWb169JiMpDWBR4C9gDbgXuCQiHiw0jYekzFbVVe11Jx8WldvHZPZCZgfEY8BSLoK2B+omGTMbFVd1aq7e9LKrzubvJysWkdPTzJDgQW5123Azu1XkjQBmJBevijp4TrqHgI8s9oRdr+eGjf03Ngdd0WXd2qrGn3ePt7dqxT3uzqzcU9PMnWJiIuAizqyjaQZnWkaNlpPjRt6buyOu3s57u61unH39JuWLQSG514PS2VmZtYEenqSuRfYStJISe8ADgZuaHBMZmaW9OjusohYIelLwM1kU5gnR8QDXVR9h7rXmkhPjRt6buyOu3s57u61WnH36CnMZmbW3Hp6d5mZmTUxJxkzMyuMk0wZksZKeljSfEknNjqeSiQNl3SrpAclPSDp2FS+gaRpkualn4MaHWs5ktaU9DdJN6bXIyXdnY77L9NkjqYiaaCkayTNlfSQpF17wvGW9NX0OzJH0pWS+jbr8ZY0WdJiSXNyZWWPsTKT0nu4T9IOTRb3j9Lvyn2SrpM0MLdsYor7YUn7NCRoysedW3acpJA0JL3u8PF2kmknXarmXGAcsA1wiKRtGhtVRSuA4yJiG2AX4JgU64nALRGxFXBLet2MjgUeyr3+AfDjiNgSeA44oiFRVXcO8IeIeC/wfrL4m/p4SxoKfBkYExHbkU2SOZjmPd6XAmPblVU6xuOArdJjAnB+N8VYzqWsGvc0YLuIeB/ZJbAmAqS/04OBbdM256X/PY1wKavGjaThwN7AP3PFHT7eTjKreutSNRHxL6B0qZqmExGLImJWer6c7B/eULJ4p6TVpgAHNCTAKiQNAz4G/DS9FrAncE1apeniljQA+BBwCUBE/CsiltEDjjfZTNJ1JK0F9AMW0aTHOyLuAJ5tV1zpGO8PXBaZ6cBASZt0S6DtlIs7IqZGxIr0cjrZuXyQxX1VRLwWEf8A5pP97+l2FY43wI+BrwP52WEdPt5OMqsqd6maoQ2KpW6SRgDbA3cDG0fEorToKWDjRsVVxdlkv8BvpteDgWW5P8hmPO4jgSXAz1I3308lrUuTH++IWAj8L9k30kXA88BMmv9451U6xj3p7/ULwO/T86aOW9L+wMKI+Hu7RR2O20mmBUhaD/g18JWIeCG/LLI56k01T13Sx4HFETGz0bF00FrADsD5EbE98BLtusaa9HgPIvsGOhLYFFiXMt0jPUUzHuNaJH2TrHv7F42OpRZJ/YBvAN/qivqcZFbVoy5VI6kPWYL5RURcm4qfLjVh08/FjYqvgg8C+0l6nKw7ck+ysY6BqTsHmvO4twFtEXF3en0NWdJp9uP9UeAfEbEkIl4HriX7DJr9eOdVOsZN//cq6TDg48Bn4+0TE5s57i3IvpD8Pf2NDgNmSXonnYjbSWZVPeZSNWkc4xLgoYg4K7foBmB8ej4euL67Y6smIiZGxLCIGEF2fP8UEZ8FbgU+mVZrxrifAhZIek8q+gjZbSWa+niTdZPtIqlf+p0pxd3Ux7udSsf4BuDQNOtpF+D5XLdawym7qeLXgf0i4uXcohuAgyWtLWkk2UD6PY2Isb2IuD8iNoqIEelvtA3YIf3+d/x4R4Qf7R7AvmQzQR4FvtnoeKrEuRtZt8F9wOz02JdsfOMWYB7wR2CDRsda5T3sDtyYnm9O9oc2H/gVsHaj4ysT72hgRjrmvwEG9YTjDXwHmAvMAX4OrN2sxxu4kmzs6PX0D+6ISscYENls0EeB+8lm0DVT3PPJxjBKf58X5Nb/Zor7YWBcM8XdbvnjwJDOHm9fVsbMzArj7jIzMyuMk4yZmRXGScbMzArjJGNmZoVxkjEzs8I4yVjLk/TNdAXi+yTNlrRzo2NaHZIulfTJ2mt2uv7dJf1bd+3PWluPvv2yWS2SdiU723qHiHgtXbK8KS5p38R2B14E7mxwHNYC3JKxVrcJ8ExEvAYQEc9ExJMAknaUdLukmZJuzl22ZEdJf0+PH5XusyHpMEk/KVUs6UZJu6fne0u6S9IsSb9K15ND0uOSvpPK75f03lS+nqSfpbL7JP1ntXpqUXZvnh9JujfV98VUvruk2/T2PXB+kc76R9K+qWymsnuE3JgutHoU8NXU6vv3tIsPSbpT0mNu1VhHOMlYq5sKDJf0iKTzJH0Y3rrm2/8Bn4yIHYHJwPfSNj8D/jsi3l/PDlLr6CTgoxGxA9kVAf4nt8ozqfx84Gup7GSyS3KMiuxeI3+qo55qjkj1fQD4AHBkulwJZFfn/grZ/ZE2Bz4oqS9wIdmZ5jsCGwJExOPABWT3mRkdEX9OdWxCdoWJjwNn1BmTmbvLrLVFxIuSdgT+HdgD+KWyu53OALYDpqUv9msCi5TduXBgZPfYgOwSLONq7GYXsn/gf011vQO4K7e8dOHSmcCB6flHya7bVorzuXR16mr1VLM38L5cK2MA2fWw/gXcExFtAJJmAyPIusMei+xeJpBdWmRClfp/ExFvAg9KaqpbGVhzc5KxlhcRbwC3AbdJup/sAoszgQciYtf8usrdHreMFazc+u9b2gyYFhGHVNjutfTzDar/zdWqpxqRtb5uXqkw6857LVdUK4ZK8nWoE9tbL+XuMmtpkt4jaatc0WjgCbKLEm6YJgYgqY+kbSO70+UySbul9T+b2/ZxYLSkNZTdmrZ0J8PpZF1QW6a61pX07hqhTQOOycU5qJP1lNwM/L/UDYikdyu7oVolDwObpzEYgE/nli0H+te5X7OqnGSs1a0HTJH0oKT7yLqjvh3ZrbU/CfxA0t/JrpBbmrZ7OHBu6lrKf2v/K/APssvkTwJKt75eAhwGXJn2cRfw3hpxfRcYJGlO2v8eHaznQklt6XEX2W2sHyS778ccsvGWii2WiHgFOBr4g6SZZInl+bT4t8An2g38m3WKr8JsVkX6pn9jRGzX6Fi6mqT10phV6fLt8yLix42Oy1qLWzJmvdeRqbX2ANlEgQsbG461IrdkzMysMG7JmJlZYZxkzMysME4yZmZWGCcZMzMrjJOMmZkV5v8DB3GBNBQVI1cAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 길이 리스트 추출\n",
    "train_lengths = [len(x[\"input_ids\"]) for x in tokenized_datasets[\"train\"]]\n",
    "test_lengths = [len(x[\"input_ids\"]) for x in tokenized_datasets[\"test\"]]\n",
    "\n",
    "# 히스토그램 출력\n",
    "plt.hist(train_lengths, bins=50, alpha=0.6, label=\"train\")\n",
    "plt.hist(test_lengths, bins=50, alpha=0.6, label=\"test\")\n",
    "plt.xlabel(\"Sequence Length\")\n",
    "plt.ylabel(\"Number of Samples\")\n",
    "plt.title(\"Input Sequence Length Distribution\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b6a8c21",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79fa64a1acb74940b0611f70da76990f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/147 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8edbd442754a4abc964b31149bf4f640",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function_with_padding(example):\n",
    "    return tokenizer(\n",
    "        example[\"document\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",  # 고정 길이로 패딩\n",
    "        max_length=64          # 길이 제한\n",
    "    )\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function_with_padding, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb55796",
   "metadata": {},
   "source": [
    "### validation set 나누기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc1a2dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "# 1. train 데이터를 90:10 비율로 나누기\n",
    "split_ds = tokenized_datasets['train'].train_test_split(test_size=0.1, seed=42)\n",
    "\n",
    "# 2. 원래 DatasetDict에 병합\n",
    "tokenized_datasets = DatasetDict({\n",
    "    'train': split_ds['train'],\n",
    "    'validation': split_ds['test'],  # 10% eval set\n",
    "    'test': tokenized_datasets['test']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d3f955b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['attention_mask', 'document', 'id', 'input_ids', 'label', 'token_type_ids'],\n",
       "        num_rows: 131559\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['attention_mask', 'document', 'id', 'input_ids', 'label', 'token_type_ids'],\n",
       "        num_rows: 14618\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['attention_mask', 'document', 'id', 'input_ids', 'label', 'token_type_ids'],\n",
       "        num_rows: 49157\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e9e82e",
   "metadata": {},
   "source": [
    "### 학습에 사용할 metric 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "351a8e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    logits, labels = pred\n",
    "    preds = logits.argmax(axis=-1)\n",
    "    return {\n",
    "        'accuracy': accuracy_score(labels, preds),\n",
    "        'f1': f1_score(labels, preds)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d174666a",
   "metadata": {},
   "source": [
    "### arguments 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb63e389",
   "metadata": {},
   "source": [
    "validation accuracy와 f1 score를 확인하기 위해 wandb를 사용했다가 접속이 꼬여 학습이 진행되지 않는 상황 발생\n",
    "\n",
    "wandb를 포기하고 콘솔에 에포크 별 accuracy와 f1 score를 확인할 수 있도록 수정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "74649e6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "output_dir = os.getenv('HOME')+'/aiffel/transformers'\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir,                                         # output이 저장될 경로\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    learning_rate = 2e-5,                         #learning_rate\n",
    "    per_device_train_batch_size = 64,   # 각 device 당 batch size\n",
    "    per_device_eval_batch_size = 64,    # evaluation 시에 batch size\n",
    "    num_train_epochs = 3,                     # train 시킬 총 epochs\n",
    "    weight_decay = 0.01,                        # weight decay\n",
    "    fp16=True,\n",
    "    report_to=\"none\",  # ← 이거 추가!\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bc8c0492",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using amp fp16 backend\n",
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: document, id.\n",
      "***** Running training *****\n",
      "  Num examples = 131559\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 6168\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6168' max='6168' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6168/6168 37:21, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.261400</td>\n",
       "      <td>0.245039</td>\n",
       "      <td>0.897318</td>\n",
       "      <td>0.896561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.186700</td>\n",
       "      <td>0.245089</td>\n",
       "      <td>0.905117</td>\n",
       "      <td>0.904378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.132000</td>\n",
       "      <td>0.273519</td>\n",
       "      <td>0.904228</td>\n",
       "      <td>0.904567</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in /aiffel/aiffel/transformers/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in /aiffel/aiffel/transformers/checkpoint-500/special_tokens_map.json\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-1000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-1000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in /aiffel/aiffel/transformers/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in /aiffel/aiffel/transformers/checkpoint-1000/special_tokens_map.json\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-1500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-1500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in /aiffel/aiffel/transformers/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in /aiffel/aiffel/transformers/checkpoint-1500/special_tokens_map.json\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-2000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-2000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in /aiffel/aiffel/transformers/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in /aiffel/aiffel/transformers/checkpoint-2000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: document, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 14618\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step 2056] eval_accuracy=0.8973, f1=0.8966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/transformers/trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-2500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-2500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in /aiffel/aiffel/transformers/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in /aiffel/aiffel/transformers/checkpoint-2500/special_tokens_map.json\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-3000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-3000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in /aiffel/aiffel/transformers/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in /aiffel/aiffel/transformers/checkpoint-3000/special_tokens_map.json\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-3500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-3500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in /aiffel/aiffel/transformers/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in /aiffel/aiffel/transformers/checkpoint-3500/special_tokens_map.json\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-4000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-4000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in /aiffel/aiffel/transformers/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in /aiffel/aiffel/transformers/checkpoint-4000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: document, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 14618\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step 4112] eval_accuracy=0.9051, f1=0.9044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-4500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-4500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-4500/pytorch_model.bin\n",
      "tokenizer config file saved in /aiffel/aiffel/transformers/checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in /aiffel/aiffel/transformers/checkpoint-4500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-5000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-5000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-5000/pytorch_model.bin\n",
      "tokenizer config file saved in /aiffel/aiffel/transformers/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in /aiffel/aiffel/transformers/checkpoint-5000/special_tokens_map.json\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-5500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-5500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-5500/pytorch_model.bin\n",
      "tokenizer config file saved in /aiffel/aiffel/transformers/checkpoint-5500/tokenizer_config.json\n",
      "Special tokens file saved in /aiffel/aiffel/transformers/checkpoint-5500/special_tokens_map.json\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-6000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-6000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-6000/pytorch_model.bin\n",
      "tokenizer config file saved in /aiffel/aiffel/transformers/checkpoint-6000/tokenizer_config.json\n",
      "Special tokens file saved in /aiffel/aiffel/transformers/checkpoint-6000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: document, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 14618\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step 6168] eval_accuracy=0.9042, f1=0.9046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6168, training_loss=0.19336959205558174, metrics={'train_runtime': 2242.112, 'train_samples_per_second': 176.029, 'train_steps_per_second': 2.751, 'total_flos': 1.298048524953984e+16, 'train_loss': 0.19336959205558174, 'epoch': 3.0})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainerCallback\n",
    "\n",
    "class ConsoleLoggerCallback(TrainerCallback):\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs is not None and \"eval_accuracy\" in logs:\n",
    "            print(f\"[step {state.global_step}] eval_accuracy={logs['eval_accuracy']:.4f}, f1={logs.get('eval_f1', 0):.4f}\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,                                                                    # 학습시킬 model\n",
    "    args=training_arguments,                                                # TrainingArguments을 통해 설정한 arguments\n",
    "    train_dataset=tokenized_datasets[\"train\"],         # training dataset\n",
    "    eval_dataset=tokenized_datasets[\"validation\"], # validation dataset\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[ConsoleLoggerCallback()]  # ✅ 추가\n",
    ")\n",
    "\n",
    "# 모델 학습\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c0bc1d",
   "metadata": {},
   "source": [
    "## test set으로 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "25d1c3cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: document, id.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 49157\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='769' max='769' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [769/769 03:17]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_loss': 0.2782084345817566, 'test_accuracy': 0.902902943629595, 'test_f1': 0.9043391121354846, 'test_runtime': 197.58, 'test_samples_per_second': 248.795, 'test_steps_per_second': 3.892}\n",
      "예측 라벨: [1 1 0 0 0 1 0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "# 예측 수행\n",
    "predictions = trainer.predict(tokenized_datasets[\"test\"])\n",
    "\n",
    "# 예측 결과에서 logits, labels, metrics 추출\n",
    "logits = predictions.predictions\n",
    "pred_labels = logits.argmax(axis=-1)  # 예측된 클래스\n",
    "true_labels = predictions.label_ids   # 실제 레이블 (있다면)\n",
    "\n",
    "# 평가 지표 출력\n",
    "print(predictions.metrics)\n",
    "\n",
    "# 예측 라벨 일부 확인\n",
    "print(\"예측 라벨:\", pred_labels[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b0c4b43e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[문장] 굳 ㅋ\n",
      "[실제 라벨] 1\n",
      "[예측 라벨] 1\n",
      "\n",
      "[문장] gdntopclassintheclub\n",
      "[실제 라벨] 0\n",
      "[예측 라벨] 1\n",
      "\n",
      "[문장] 뭐야 이 평점들은.. 나쁘진 않지만 10점 짜리는 더더욱 아니잖아\n",
      "[실제 라벨] 0\n",
      "[예측 라벨] 0\n",
      "\n",
      "[문장] 지루하지는 않은데 완전 막장임.. 돈주고 보기에는..\n",
      "[실제 라벨] 0\n",
      "[예측 라벨] 0\n",
      "\n",
      "[문장] 3d만 아니었어도 별 다섯 개 줬을텐데.. 왜 3d로 나와서 제 심기를 불편하게 하죠??\n",
      "[실제 라벨] 0\n",
      "[예측 라벨] 0\n",
      "\n",
      "[문장] 음악이 주가 된, 최고의 음악영화\n",
      "[실제 라벨] 1\n",
      "[예측 라벨] 1\n",
      "\n",
      "[문장] 진정한 쓰레기\n",
      "[실제 라벨] 0\n",
      "[예측 라벨] 0\n",
      "\n",
      "[문장] 마치 미국애니에서 튀어나온듯한 창의력없는 로봇디자인부터가,고개를 젖게한다\n",
      "[실제 라벨] 0\n",
      "[예측 라벨] 0\n",
      "\n",
      "[문장] 갈수록 개판되가는 중국영화 유치하고 내용없음 폼잡다 끝남 말도안되는 무기에 유치한cg남무 아 그립다 동사서독같은 영화가 이건 3류아류작이다\n",
      "[실제 라벨] 0\n",
      "[예측 라벨] 0\n",
      "\n",
      "[문장] 이별의 아픔뒤에 찾아오는 새로운 인연의 기쁨 but, 모든 사람이 그렇지는 않네..\n",
      "[실제 라벨] 1\n",
      "[예측 라벨] 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test 데이터 원본과 함께 보기\n",
    "for i in range(10):\n",
    "    print(f\"[문장] {raw_datasets['test'][i]['document']}\")\n",
    "    print(f\"[실제 라벨] {raw_datasets['test'][i]['label']}\")\n",
    "    print(f\"[예측 라벨] {pred_labels[i]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b87776",
   "metadata": {},
   "source": [
    "## Bucketing 적용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addd6612",
   "metadata": {},
   "source": [
    "### 패딩 없이 데이터셋 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6a93f094",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9fbbf6caa114cd59c76e4bf9ce78c1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/147 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26ca9572ffe440cbb6f7e5518586cd5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = raw_datasets.map(\n",
    "    lambda x: tokenizer(x[\"document\"], truncation=False, padding=False),\n",
    "    batched=True,\n",
    "    remove_columns=[\"document\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2e5f13c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. train 데이터를 90:10 비율로 나누기\n",
    "split_ds = tokenized_datasets['train'].train_test_split(test_size=0.1, seed=42)\n",
    "\n",
    "# 2. 원래 DatasetDict에 병합\n",
    "tokenized_datasets = DatasetDict({\n",
    "    'train': split_ds['train'],\n",
    "    'validation': split_ds['test'],  # 10% eval set\n",
    "    'test': tokenized_datasets['test']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f06999de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['attention_mask', 'id', 'input_ids', 'label', 'token_type_ids'],\n",
       "        num_rows: 131559\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['attention_mask', 'id', 'input_ids', 'label', 'token_type_ids'],\n",
       "        num_rows: 14618\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['attention_mask', 'id', 'input_ids', 'label', 'token_type_ids'],\n",
       "        num_rows: 49157\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34e8f3d",
   "metadata": {},
   "source": [
    "### 새로운 학습을 위해 모델 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d064da5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/klue/bert-base/resolve/main/config.json from cache at /aiffel/.cache/huggingface/transformers/fbd0b2ef898c4653902683fea8cc0dd99bf43f0e082645b913cda3b92429d1bb.99b3298ed554f2ad731c27cdb11a6215f39b90bc845ff5ce709bb4e74ba45621\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/klue/bert-base/resolve/main/pytorch_model.bin from cache at /aiffel/.cache/huggingface/transformers/05b36ee62545d769939a7746eca739b844a40a7a7553700f110b58b28ed6a949.7cb231256a5dbe886e12b902d05cb1241f330d8c19428508f91b2b28c1cfe0b6\n",
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_name_or_path, num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04896f62",
   "metadata": {},
   "source": [
    "### Data Collator를 사용하여 dynamic padding 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "63b7d32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf5986f",
   "metadata": {},
   "source": [
    "### group_by_length=True로 설정하여 비슷한 길이로 묶어서 학습에 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fa84cf10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    fp16=True,\n",
    "    group_by_length=True,  # ✅ 핵심 설정\n",
    "    report_to=\"none\",\n",
    "    run_name=\"bucketed_dynamic_padding\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "879f90b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using amp fp16 backend\n",
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id.\n",
      "***** Running training *****\n",
      "  Num examples = 131559\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 6168\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6168' max='6168' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6168/6168 29:30, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.279800</td>\n",
       "      <td>0.239027</td>\n",
       "      <td>0.901218</td>\n",
       "      <td>0.900276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.190300</td>\n",
       "      <td>0.240935</td>\n",
       "      <td>0.906212</td>\n",
       "      <td>0.906244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.137500</td>\n",
       "      <td>0.263883</td>\n",
       "      <td>0.905254</td>\n",
       "      <td>0.905467</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/transformers/trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to ./results/checkpoint-500\n",
      "Configuration saved in ./results/checkpoint-500/config.json\n",
      "Model weights saved in ./results/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-1000\n",
      "Configuration saved in ./results/checkpoint-1000/config.json\n",
      "Model weights saved in ./results/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-1000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-1500\n",
      "Configuration saved in ./results/checkpoint-1500/config.json\n",
      "Model weights saved in ./results/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-1500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-2000\n",
      "Configuration saved in ./results/checkpoint-2000/config.json\n",
      "Model weights saved in ./results/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-2000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 14618\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step 2056] eval_accuracy=0.9012, f1=0.9003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results/checkpoint-2500\n",
      "Configuration saved in ./results/checkpoint-2500/config.json\n",
      "Model weights saved in ./results/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-2500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-3000\n",
      "Configuration saved in ./results/checkpoint-3000/config.json\n",
      "Model weights saved in ./results/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-3000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-3500\n",
      "Configuration saved in ./results/checkpoint-3500/config.json\n",
      "Model weights saved in ./results/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-3500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-4000\n",
      "Configuration saved in ./results/checkpoint-4000/config.json\n",
      "Model weights saved in ./results/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-4000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 14618\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step 4112] eval_accuracy=0.9062, f1=0.9062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results/checkpoint-4500\n",
      "Configuration saved in ./results/checkpoint-4500/config.json\n",
      "Model weights saved in ./results/checkpoint-4500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-4500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-5000\n",
      "Configuration saved in ./results/checkpoint-5000/config.json\n",
      "Model weights saved in ./results/checkpoint-5000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-5000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to ./results/checkpoint-5500\n",
      "Configuration saved in ./results/checkpoint-5500/config.json\n",
      "Model weights saved in ./results/checkpoint-5500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-5500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-5500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-6000\n",
      "Configuration saved in ./results/checkpoint-6000/config.json\n",
      "Model weights saved in ./results/checkpoint-6000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-6000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-6000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 14618\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step 6168] eval_accuracy=0.9053, f1=0.9055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6168, training_loss=0.2025555958234537, metrics={'train_runtime': 1770.7468, 'train_samples_per_second': 222.887, 'train_steps_per_second': 3.483, 'total_flos': 4733099635983240.0, 'train_loss': 0.2025555958234537, 'epoch': 3.0})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_bucketing = Trainer(\n",
    "    model,                                                                    # 학습시킬 model\n",
    "    args=training_args,                                                # TrainingArguments을 통해 설정한 arguments\n",
    "    train_dataset=tokenized_datasets[\"train\"],         # training dataset\n",
    "    eval_dataset=tokenized_datasets[\"validation\"], # validation dataset\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[ConsoleLoggerCallback()]  # ✅ 추가\n",
    ")\n",
    "\n",
    "trainer_bucketing.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3457954",
   "metadata": {},
   "source": [
    "### test set으로 예측 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1047b54b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 49157\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='769' max='769' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [769/769 04:08]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_loss': 0.2689038813114166, 'test_accuracy': 0.904916898915719, 'test_f1': 0.9061935535664111, 'test_runtime': 249.4265, 'test_samples_per_second': 197.08, 'test_steps_per_second': 3.083}\n",
      "예측 라벨: [1 1 0 0 0 1 0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "# 예측 수행\n",
    "predictions = trainer_bucketing.predict(tokenized_datasets[\"test\"])\n",
    "\n",
    "# 예측 결과에서 logits, labels, metrics 추출\n",
    "logits = predictions.predictions\n",
    "pred_labels = logits.argmax(axis=-1)  # 예측된 클래스\n",
    "true_labels = predictions.label_ids   # 실제 레이블 (있다면)\n",
    "\n",
    "# 평가 지표 출력\n",
    "print(predictions.metrics)\n",
    "\n",
    "# 예측 라벨 일부 확인\n",
    "print(\"예측 라벨:\", pred_labels[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2f96b99d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[문장] 굳 ㅋ\n",
      "[실제 라벨] 1\n",
      "[예측 라벨] 1\n",
      "\n",
      "[문장] gdntopclassintheclub\n",
      "[실제 라벨] 0\n",
      "[예측 라벨] 1\n",
      "\n",
      "[문장] 뭐야 이 평점들은.. 나쁘진 않지만 10점 짜리는 더더욱 아니잖아\n",
      "[실제 라벨] 0\n",
      "[예측 라벨] 0\n",
      "\n",
      "[문장] 지루하지는 않은데 완전 막장임.. 돈주고 보기에는..\n",
      "[실제 라벨] 0\n",
      "[예측 라벨] 0\n",
      "\n",
      "[문장] 3d만 아니었어도 별 다섯 개 줬을텐데.. 왜 3d로 나와서 제 심기를 불편하게 하죠??\n",
      "[실제 라벨] 0\n",
      "[예측 라벨] 0\n",
      "\n",
      "[문장] 음악이 주가 된, 최고의 음악영화\n",
      "[실제 라벨] 1\n",
      "[예측 라벨] 1\n",
      "\n",
      "[문장] 진정한 쓰레기\n",
      "[실제 라벨] 0\n",
      "[예측 라벨] 0\n",
      "\n",
      "[문장] 마치 미국애니에서 튀어나온듯한 창의력없는 로봇디자인부터가,고개를 젖게한다\n",
      "[실제 라벨] 0\n",
      "[예측 라벨] 0\n",
      "\n",
      "[문장] 갈수록 개판되가는 중국영화 유치하고 내용없음 폼잡다 끝남 말도안되는 무기에 유치한cg남무 아 그립다 동사서독같은 영화가 이건 3류아류작이다\n",
      "[실제 라벨] 0\n",
      "[예측 라벨] 0\n",
      "\n",
      "[문장] 이별의 아픔뒤에 찾아오는 새로운 인연의 기쁨 but, 모든 사람이 그렇지는 않네..\n",
      "[실제 라벨] 1\n",
      "[예측 라벨] 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test 데이터 원본과 함께 보기\n",
    "for i in range(10):\n",
    "    print(f\"[문장] {raw_datasets['test'][i]['document']}\")\n",
    "    print(f\"[실제 라벨] {raw_datasets['test'][i]['label']}\")\n",
    "    print(f\"[예측 라벨] {pred_labels[i]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa621e0f",
   "metadata": {},
   "source": [
    "## 회고\n",
    "- 팀원들에 비해 프로젝트를 늦게 시작하였고, 팀원들이 학습에 3시간 가량 걸린다는 조언을 해 주어 colab의 유로 버전으로 A100 GPU를 사용하려 했으나, 허깅페이스에서 데이터를 불러오는 과정에서 오류가 발생해 Aiffel Cloud로 진행하였다. 데이터셋을 load_data 메소드를 사용하지 않고 직접 다운로드하여 pandas로 데이터를 다뤘다면 문제가 해결되었겠지만, load_data를 사용하여 학습을 진행하는 것부터 프로젝트의 목표라고 생각하여 colab을 포기하고 load_data 메소드를 살리기로 결정했다.\n",
    "- 첫 번째 학습이 끝난 이후 로그 파일에서 validation accuracy와 f1 score를 찾으려 했으나 기록이 남아있지 않다는 것을 확인했다. 기록을 찾아보기 위해 자동 생성된 wandb 결과를 찾아보았으나 결국 학습 중 저장되지 않았다는 것을 깨달았고, 재학습을 통해 다시 구해보려 하였으나 wandb 세션에 충돌이 일어나 학습이 진행되지 않는 상황이 있었다. 노트북 파일에서 wandb.init이 전혀 작동되지 않는 것을 확인한 후 wandb로 학습 과정 분석하기 대신 노트북 아웃풋에 각 epoch이 끝날 때마다 validation 결과를 확인할 수 있도록 코드를 수정했다.\n",
    "- Bucketing을 이용하여 dynamic padding을 적용한 결과 순수 모델 학습 시간이 37분에서 28분으로 단축된 것을 확인했다. 또한 test accuracy가 0.002만큼 증가한 것을 확인했다. 정확도 측면에서 큰 차이가 있진 않지만, 긴 데이터에 대해 뒷 내용을 생략할 필요가 없고, 길이가 비슷한 데이터끼리 모아서 패딩을 진행하여 패딩 양이 학습 속도에 미치는 영향을 최소화하여 더 빠르게 좋은 결과를 낼 수 있었다고 생각한다.\n",
    "- 팀원들은 본인 포함 2명은 Bucketing의 속도가 빠르고 결과가 향상되었고, 2명은 속도와 결과에 큰 변화가 없었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b806980",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
